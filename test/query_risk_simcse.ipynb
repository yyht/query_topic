{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a5c82e8-bb94-41b5-a367-498ba4af5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys,os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "399b4feb-bad5-4b66-b52f-8c556cbaafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.extend(['/root/xiaoda/query_topic/'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da59d2cc-b5a2-46da-ac4e-33e75409552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/ondrejbohdal/meta-calibration/blob/main/Metrics/metrics.py\n",
    "\"\"\"\n",
    "\n",
    "class ECE(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(ECE, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, logits, labels, mode='logits'):\n",
    "        if mode == 'logits':\n",
    "            softmaxes = F.softmax(logits, dim=1)\n",
    "        else:\n",
    "            softmaxes = logits\n",
    "        # softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "        \n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e079e4e-bb09-4aa2-b982-5560f7ad75bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast\n",
    "import transformers\n",
    "from datetime import timedelta\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from nets.them_classifier import MyBaseModel, RobertaClassifier\n",
    "from nets.simcse import MLPLayer, Similarity\n",
    "\n",
    "import configparser\n",
    "from tqdm import tqdm\n",
    "\n",
    "cur_dir_path = '/root/xiaoda/query_topic/'\n",
    "\n",
    "def load_label(filepath):\n",
    "    label_list = []\n",
    "    with open(filepath, 'r') as frobj:\n",
    "        for line in frobj:\n",
    "            label_list.append(line.strip())\n",
    "        n_classes = len(label_list)\n",
    "\n",
    "        label2id = {}\n",
    "        id2label = {}\n",
    "        for idx, label in enumerate(label_list):\n",
    "            label2id[label] = idx\n",
    "            id2label[idx] = label\n",
    "        return label2id, id2label\n",
    "\n",
    "class RiskInfer(object):\n",
    "    def __init__(self, config_path):\n",
    "\n",
    "        import torch, os, sys\n",
    "\n",
    "        con = configparser.ConfigParser()\n",
    "        con_path = os.path.join(cur_dir_path, config_path)\n",
    "        con.read(con_path, encoding='utf8')\n",
    "\n",
    "        args_path = dict(dict(con.items('paths')), **dict(con.items(\"para\")))\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(args_path[\"model_path\"], do_lower_case=True)\n",
    "\n",
    "        from collections import OrderedDict\n",
    "        self.schema_dict = OrderedDict({})\n",
    "        self.schema2schema_id = {}\n",
    "        self.schema_id2schema = {}\n",
    "\n",
    "        for label_index, schema_info in enumerate(args_path[\"label_path\"].split(',')):\n",
    "            schema_type, schema_path = schema_info.split(':')\n",
    "            schema_path = os.path.join(cur_dir_path, schema_path)\n",
    "            print(schema_type, schema_path, '===schema-path===')\n",
    "            label2id, id2label = load_label(schema_path)\n",
    "            self.schema_dict[schema_type] = {\n",
    "                'label2id':label2id,\n",
    "                'id2label':id2label,\n",
    "                'label_index':label_index\n",
    "            }\n",
    "            # print(self.schema_dict[schema_type], '==schema_type==', schema_type)\n",
    "            self.schema2schema_id[schema_type] = label_index\n",
    "            self.schema_id2schema[label_index] = schema_type\n",
    "        \n",
    "        output_path = os.path.join(cur_dir_path, args_path['output_path'])\n",
    "\n",
    "        # from roformer import RoFormerModel, RoFormerConfig\n",
    "        if args_path.get('model_type', 'bert') == 'bert':\n",
    "            from transformers import BertModel, BertConfig\n",
    "            config = BertConfig.from_pretrained(args_path[\"model_path\"])\n",
    "            encoder = BertModel(config=config)\n",
    "        elif args_path.get('model_type', 'bert') == 'roformer':\n",
    "            from roformer import RoFormerModel, RoFormerConfig\n",
    "            config = RoFormerConfig.from_pretrained(args_path[\"model_path\"])\n",
    "            encoder = RoFormerModel(config=config)\n",
    "        elif args_path.get('model_type', 'bert') == 'erine':\n",
    "            from nets.erine import ErnieConfig, ErnieModel\n",
    "            config = ErnieConfig.from_pretrained(args_path[\"model_path\"])\n",
    "            encoder = ErnieModel(config=config)\n",
    "            \n",
    "        print(args_path.get('model_type', 'bert'))\n",
    "        \n",
    "        encoder_net = MyBaseModel(encoder, config)\n",
    "\n",
    "        self.device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        classifier_list = []\n",
    "\n",
    "        schema_list = list(self.schema_dict.keys())\n",
    "\n",
    "        for schema_key in schema_list:\n",
    "            classifier = RobertaClassifier(\n",
    "                hidden_size=config.hidden_size, \n",
    "                dropout_prob=con.getfloat('para', 'out_dropout_rate'),\n",
    "                num_labels=len(self.schema_dict[schema_key]['label2id']), \n",
    "                dropout_type=con.get('para', 'dropout_type'))\n",
    "            classifier_list.append(classifier)\n",
    "\n",
    "        classifier_list = nn.ModuleList(classifier_list)\n",
    "\n",
    "        class MultitaskClassifier(nn.Module):\n",
    "            def __init__(self, transformer, classifier_list):\n",
    "                super().__init__()\n",
    "\n",
    "                self.transformer = transformer\n",
    "                self.classifier_list = classifier_list\n",
    "                \n",
    "                self.pooler_mlp = MLPLayer(config.hidden_size, 256)\n",
    "\n",
    "            def forward(self, input_ids, input_mask, \n",
    "                        segment_ids=None, \n",
    "                        transformer_mode='mean_pooling', \n",
    "                        dt_idx=None, mode='predict'):\n",
    "                hidden_states = self.transformer(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              return_mode=transformer_mode)\n",
    "                outputs_list = []\n",
    "                \n",
    "                for idx, classifier in enumerate(self.classifier_list):\n",
    "                    \n",
    "                    if dt_idx:\n",
    "                        if idx not in dt_idx:\n",
    "                            outputs_list.append([])\n",
    "                            continue\n",
    "                    \n",
    "                    scores = classifier(hidden_states)\n",
    "                    if mode == 'predict':\n",
    "                        scores = torch.nn.Softmax(dim=1)(scores)\n",
    "                    outputs_list.append(scores)\n",
    "                pooler_output = self.pooler_mlp(hidden_states)\n",
    "                embeddings = hidden_states / hidden_states.norm(dim=1, keepdim=True)\n",
    "                pooler_output_embeddings = pooler_output / pooler_output.norm(dim=1, keepdim=True)\n",
    "                return outputs_list, hidden_states, embeddings, pooler_output_embeddings\n",
    "\n",
    "        self.net = MultitaskClassifier(encoder_net, classifier_list).to(self.device)\n",
    "\n",
    "        # eo = 9\n",
    "        # ckpt = torch.load(os.path.join(output_path, 'multitask_cls.pth.{}.raw'.format(eo)), map_location=self.device)\n",
    "        # # ckpt = torch.load(os.path.join(output_path, 'multitask_cls.pth.{}.raw.focal'.format(eo)), map_location=self.device)\n",
    "        # # ckpt = torch.load(os.path.join(output_path, 'multitask_contrast_cls.pth.{}'.format(eo)), map_location=self.device)\n",
    "        # self.net.load_state_dict(ckpt)\n",
    "        # self.net.eval()\n",
    "        \n",
    "    def reload(self, model_path):\n",
    "        ckpt = torch.load(model_path, map_location=self.device)\n",
    "        self.net.load_state_dict(ckpt)\n",
    "        self.net.eval()\n",
    "        self.net = self.net.half()\n",
    "\n",
    "    def predict(self, text, allowed_schema_type={}):\n",
    "\n",
    "        \"\"\"抽取输入text所包含的类型\n",
    "        \"\"\"\n",
    "        # start = time.time()\n",
    "        # encoder_txt = self.tokenizer.encode_plus(text, max_length=256)\n",
    "        # input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().unsqueeze(0).to(self.device)\n",
    "        # token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).unsqueeze(0).to(self.device)\n",
    "        # attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).unsqueeze(0).to(self.device)\n",
    "        # print(time.time() - start, '====tokenization====')\n",
    "        \n",
    "        start = time.time()\n",
    "        encoder_txt = self.tokenizer([text], max_length=512)\n",
    "        input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().to(self.device)\n",
    "        token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).to(self.device)\n",
    "        attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).to(self.device)\n",
    "        # print(time.time() - start, '====tokenization====')\n",
    "        \n",
    "        allowed_schema_type_ids = {}\n",
    "        for schema_type in allowed_schema_type:\n",
    "            allowed_schema_type_ids[self.schema2schema_id[schema_type]] = schema_type\n",
    "        \n",
    "        scores_dict = {}\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states, \n",
    "             embeddings, \n",
    "             pooler_output_embeddings] = self.net(input_ids, \n",
    "                attention_mask, token_type_ids, transformer_mode='cls', dt_idx=allowed_schema_type_ids)\n",
    "        # print(time.time() - start, '====inference====')\n",
    "        \n",
    "        old_start = time.time()\n",
    "        \n",
    "        for schema_idx, (schema_type, scores) in enumerate(zip(list(self.schema_dict.keys()), logits_list)):\n",
    "            if allowed_schema_type:\n",
    "                if schema_type not in allowed_schema_type:\n",
    "                    continue\n",
    "            scores = scores[0].data.cpu().numpy()\n",
    "            scores_dict[schema_type] = []\n",
    "            for index, score in enumerate(scores):\n",
    "                scores_dict[schema_type].append([self.schema_dict[schema_type]['id2label'][index], \n",
    "                                        float(score)])\n",
    "            if len(scores_dict[schema_type]) >= 5:\n",
    "                schema_type_scores = sorted(scores_dict[schema_type], key=lambda item:item[1], reverse=True)\n",
    "                scores_dict[schema_type] = schema_type_scores[0:5]\n",
    "        return scores_dict, embeddings.data.cpu().numpy(), pooler_output_embeddings.data.cpu().numpy()\n",
    "    \n",
    "    def get_logitnorm(self, text):\n",
    "        \"\"\"抽取输入text所包含的类型\n",
    "        \"\"\"\n",
    "        encoder_txt = self.tokenizer.encode_plus(text, max_length=512)\n",
    "        input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().unsqueeze(0).to(self.device)\n",
    "        token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).unsqueeze(0).to(self.device)\n",
    "        attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        scores_dict = {}\n",
    "        logits_norm_list = []\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states] = self.net(input_ids, \n",
    "                attention_mask, token_type_ids, transformer_mode='cls')\n",
    "            for logits in logits_list:\n",
    "                logits_norm_list.append(logits/torch.norm(logits, p=2, dim=-1, keepdim=True) + 1e-7)\n",
    "        for schema_type, logit_norm in zip(list(self.schema_dict.keys()), logits_norm_list):\n",
    "            scores_dict[schema_type] = logit_norm[0].data.cpu().numpy()\n",
    "        return scores_dict\n",
    "            \n",
    "    \n",
    "    def predict_batch(self, text, allowed_schema_type={}):\n",
    "        if isinstance(text, list):\n",
    "            text_list = text\n",
    "        else:\n",
    "            text_list = [text]\n",
    "        model_input = self.tokenizer(text_list, max_length=512, truncation=True, return_tensors=\"pt\",padding=True)\n",
    "        for key in model_input:\n",
    "            model_input[key] = model_input[key].to(self.device)\n",
    "        \n",
    "        allowed_schema_type_ids = {}\n",
    "        for schema_type in allowed_schema_type:\n",
    "            allowed_schema_type_ids[self.schema2schema_id[schema_type]] = schema_type\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states,\n",
    "            embeddings, \n",
    "             pooler_output_embeddings] = self.net(model_input['input_ids'], \n",
    "                                                model_input['attention_mask'], \n",
    "                                                model_input['token_type_ids'], \n",
    "                                                transformer_mode='cls', \n",
    "                                                dt_idx=allowed_schema_type_ids)\n",
    "        score_dict_list = []\n",
    "        embedding_array = []\n",
    "        pooler_output_embeddings_array = []\n",
    "        for idx, text in enumerate(text_list):\n",
    "            scores_dict = {}\n",
    "            for schema_idx, (schema_type, scores) in enumerate(zip(list(self.schema_dict.keys()), logits_list)):\n",
    "                if allowed_schema_type:\n",
    "                    if schema_type not in allowed_schema_type:\n",
    "                        continue\n",
    "                # scores = torch.nn.Softmax(dim=1)(logits)[idx].data.cpu().numpy()\n",
    "                scores = scores[idx].data.cpu().numpy()\n",
    "                scores_dict[schema_type] = []\n",
    "                for index, score in enumerate(scores):\n",
    "                    scores_dict[schema_type].append([self.schema_dict[schema_type]['id2label'][index], \n",
    "                                            float(score)])\n",
    "                if len(scores_dict[schema_type]) >= 5:\n",
    "                    schema_type_scores = sorted(scores_dict[schema_type], key=lambda item:item[1], reverse=True)\n",
    "                    scores_dict[schema_type] = schema_type_scores[0:5]\n",
    "            score_dict_list.append(scores_dict)\n",
    "        embedding_array = embeddings.data.cpu().numpy()\n",
    "        pooler_output_embeddings_array = pooler_output_embeddings.data.cpu().numpy()\n",
    "        return score_dict_list, embedding_array, pooler_output_embeddings_array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "51aab72b-fc2b-4e14-a642-0091d4a1a27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ErnieTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ErnieTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity /data/albert.xht/xiaoda/sentiment/jigsaw-unintended-bias-in-toxicity-classification/toxicity_multiclass_label_list.txt ===schema-path===\n",
      "stackoverflow_topic /data/albert.xht/xiaoda/sentiment/stackoverflow_topic/stackoverflow_topic_label_list.txt ===schema-path===\n",
      "nli /data/albert.xht/xiaoda/sentiment/classification/cmnli/cmnli_label_list.txt ===schema-path===\n",
      "lcqmc /data/albert.xht/xiaoda/sentiment/classification/paws-x-zh/paws_label_list.txt ===schema-path===\n",
      "yewu /data/albert.xht/xiaoda/sentiment/yewu_v1/yewu_label_list.txt ===schema-path===\n",
      "risk_news /data/albert.xht/xiaoda/sentiment/risk_news/risk_news_label_list.txt ===schema-path===\n",
      "tnews /data/albert.xht/xiaoda/sentiment/classification/tnews_v1/tnews_label_list.txt ===schema-path===\n",
      "title2event /data/albert.xht/xiaoda/sentiment/classification/title2event_v1/title2event_label_list.txt ===schema-path===\n",
      "fewfc_2022 /data/albert.xht/xiaoda/sentiment/classification/fewfc_2022/fewfc_2022_label_list.txt ===schema-path===\n",
      "duee /data/albert.xht/xiaoda/sentiment/classification/DuEE1.0/duee_label_list.txt ===schema-path===\n",
      "ethics_common /root/xiaoda/query_topic/risk_data_tiny_query_reseponse_open/topic_query_risk/ethics_common_label_list.txt ===schema-path===\n",
      "insult /data/albert.xht/xiaoda/measuring_hate_speech/hate_speech_label_list.txt.insult ===schema-path===\n",
      "humiliate /data/albert.xht/xiaoda/measuring_hate_speech/hate_speech_label_list.txt.humiliate ===schema-path===\n",
      "dehumanize /data/albert.xht/xiaoda/measuring_hate_speech/hate_speech_label_list.txt.dehumanize ===schema-path===\n",
      "violence /data/albert.xht/xiaoda/measuring_hate_speech/hate_speech_label_list.txt.violence ===schema-path===\n",
      "genocide /data/albert.xht/xiaoda/measuring_hate_speech/hate_speech_label_list.txt.genocide ===schema-path===\n",
      "hatespeech /data/albert.xht/xiaoda/measuring_hate_speech/hate_speech_label_list.txt.hatespeech ===schema-path===\n",
      "attack_defend /data/albert.xht/xiaoda/measuring_hate_speech/hate_speech_label_list.txt.attack_defend ===schema-path===\n",
      "query_resposne_risk /data/albert.xht/xiaoda/query_response/red_team/query_response/query_response_label.txt ===schema-path===\n",
      "cmid /data/albert.xht/xiaoda/sentiment/CMID-main/cmid_label_list.txt ===schema-path===\n",
      "topic /data/albert.xht/raw_chat_corpus/topic_classification_v4/label_list.txt ===schema-path===\n",
      "senti_query /data/albert.xht/xiaoda/sentiment/senti/senti_query_label.txt ===schema-path===\n",
      "senti /data/albert.xht/xiaoda/sentiment/senti/senti_label.txt ===schema-path===\n",
      "bias /data/albert.xht/xiaoda/sentiment/bias/bias_label.txt ===schema-path===\n",
      "ciron /data/albert.xht/xiaoda/sentiment/ciron/ciron_label.txt ===schema-path===\n",
      "intent /data/albert.xht/xiaoda/sentiment/intention_data_v2-1/label.txt ===schema-path===\n",
      "offensive /data/albert.xht/xiaoda/sentiment/offensive/offensive_label.txt ===schema-path===\n",
      "query_risk /data/albert.xht/xiaoda/sentiment/query_risk_v12/query_risk_label.txt ===schema-path===\n",
      "teenager /data/albert.xht/xiaoda/sentiment/teenager//teenager_label.txt ===schema-path===\n",
      "politics /data/albert.xht/xiaoda/sentiment/green_politics/green_politics_label.txt.detail ===schema-path===\n",
      "porn /data/albert.xht/xiaoda/sentiment/green_porn_v1/green_porn_label.txt ===schema-path===\n",
      "abusive /data/albert.xht/xiaoda/sentiment/green_abusive_v1/green_abusive_label.txt ===schema-path===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/14/2023 11:45:13 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erine\n"
     ]
    }
   ],
   "source": [
    "erine_green_green_topic_risk_open_all_politics_detail_api = RiskInfer('/root/xiaoda/query_topic/resources_open_all_politics_detail_yewu_en_erine_update_simcse/topic_query_risk/config.ini')\n",
    "# model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcse/opic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_all_add_nli_instruct_politics_detail_mtdnn_v39_simcse_zh_en_1e-4_odps//multitask_cls.pth.9'\n",
    "# model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcse/topic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_all_add_nli_instruct_politics_detail_mtdnn_v40_simcse_zh_en_1e-4_v3/multitask_cls.pth.9'\n",
    "model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcse/topic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_all_add_nli_instruct_politics_detail_mtdnn_v41_simcse_zh_en_1e-4_odps_v1/multitask_cls.pth.9'\n",
    "# model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcsetopic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_mtdnn_v41_simcse_zh_en_1e-4_odps_v3_nmt_rdrop/multitask_cls.pth.9'\n",
    "\n",
    "erine_green_green_topic_risk_open_all_politics_detail_api.reload(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "45b2178a-9219-44bf-b7da-c7eedb8aa797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017998456954956055\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "start = time.time()\n",
    "allowed_schema_type = {\n",
    "    # 'topic':'',\n",
    "    # 'query_risk':'',\n",
    "    # 'porn':'',\n",
    "    # 'abusive':'',\n",
    "    # # 'offensive':'',\n",
    "    # 'intent':'',\n",
    "    # 'teenager':'',\n",
    "    # 'politics':''\n",
    "}\n",
    "allowed_schema_type = {}\n",
    "resp = erine_green_green_topic_risk_open_all_politics_detail_api.predict_batch(\n",
    "    [\n",
    "        \"\"\"\n",
    "        今天天气怎么样\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        \n",
    "        you are a dick\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "    ], allowed_schema_type=allowed_schema_type)\n",
    "\n",
    "print(time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3fde58-9b6f-4b1f-8cdd-5fbde170b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "11304fbe-736e-4d13-9e37-f3c9117d4dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxicity': [['insult', 0.87548828125],\n",
       "  ['obscene', 0.059051513671875],\n",
       "  ['toxicity', 0.0294036865234375],\n",
       "  ['no_toxicity', 0.0198516845703125],\n",
       "  ['sexual_explicit', 0.014373779296875]],\n",
       " 'stackoverflow_topic': [['pt', 0.255615234375],\n",
       "  ['german', 0.10125732421875],\n",
       "  ['softwareengineering', 0.09332275390625],\n",
       "  ['sqa', 0.06427001953125],\n",
       "  ['arduino', 0.054962158203125]],\n",
       " 'nli': [['neutral', 0.29931640625],\n",
       "  ['entailment', 0.278564453125],\n",
       "  ['contradiction', 0.422119140625]],\n",
       " 'lcqmc': [['不相似', 0.7119140625], ['相似', 0.288330078125]],\n",
       " 'yewu': [['导购', 0.7724609375],\n",
       "  ['其他', 0.1143798828125],\n",
       "  ['售后', 0.054046630859375],\n",
       "  ['广告外投', 0.0118255615234375],\n",
       "  ['个性化退出', 0.01143646240234375]],\n",
       " 'risk_news': [['无', 0.71533203125],\n",
       "  ['被采取监管措施', 0.184814453125],\n",
       "  ['股票质押率过高', 0.034332275390625],\n",
       "  ['被监管机构罚款或查处', 0.0156707763671875],\n",
       "  ['重大诉讼仲裁', 0.01451873779296875]],\n",
       " 'tnews': [['时尚', 0.767578125],\n",
       "  ['星座', 0.1348876953125],\n",
       "  ['科技', 0.0283355712890625],\n",
       "  ['家居', 0.01468658447265625],\n",
       "  ['财经', 0.0094757080078125]],\n",
       " 'title2event': [['时事', 0.1771240234375],\n",
       "  ['财经', 0.163818359375],\n",
       "  ['健康', 0.1212158203125],\n",
       "  ['社会', 0.07452392578125],\n",
       "  ['科技', 0.049957275390625]],\n",
       " 'fewfc_2022': [['财务信息造假', 0.177734375],\n",
       "  ['公司退市', 0.08026123046875],\n",
       "  ['造假欺诈', 0.068115234375],\n",
       "  ['股权质押', 0.056121826171875],\n",
       "  ['骗保', 0.03472900390625]],\n",
       " 'duee': [['竞赛行为-胜负', 0.10992431640625],\n",
       "  ['财经/交易-降价', 0.08709716796875],\n",
       "  ['司法行为-拘捕', 0.0716552734375],\n",
       "  ['财经/交易-跌停', 0.0489501953125],\n",
       "  ['竞赛行为-晋级', 0.0418701171875]],\n",
       " 'ethics_common': [['ethics_risk', 0.0897216796875],\n",
       "  ['ethics_safe', 0.91015625]],\n",
       " 'insult': [['insult', 0.94921875], ['no_insult', 0.05084228515625]],\n",
       " 'humiliate': [['humiliate', 0.8271484375], ['no_humiliate', 0.173095703125]],\n",
       " 'dehumanize': [['dehumanize', 0.548828125],\n",
       "  ['no_dehumanize', 0.450927734375]],\n",
       " 'violence': [['violence', 0.039642333984375], ['no_violence', 0.96044921875]],\n",
       " 'genocide': [['genocide', 0.0010528564453125], ['no_genocide', 0.9990234375]],\n",
       " 'hatespeech': [['hatespeech', 0.76318359375],\n",
       "  ['no_hatespeech', 0.237060546875]],\n",
       " 'attack_defend': [['attack_defend', 0.88671875],\n",
       "  ['no_attack_defend', 0.1134033203125]],\n",
       " 'query_resposne_risk': [['rejected', 0.59765625], ['chosen', 0.402587890625]],\n",
       " 'cmid': [['定义', 0.5029296875],\n",
       "  ['临床表现(病症表现)', 0.1357421875],\n",
       "  ['无法确定', 0.053009033203125],\n",
       "  ['方法', 0.044769287109375],\n",
       "  ['相关病症', 0.027984619140625]],\n",
       " 'topic': [['电脑/网络', 0.1192626953125],\n",
       "  ['健康', 0.10858154296875],\n",
       "  ['文化/艺术', 0.08148193359375],\n",
       "  ['教育/科学', 0.06329345703125],\n",
       "  ['两性', 0.05548095703125]],\n",
       " 'senti_query': [['负向', 0.73876953125],\n",
       "  ['中性', 0.25634765625],\n",
       "  ['正向', 0.005035400390625]],\n",
       " 'senti': [['负向', 0.97509765625], ['正向', 0.024932861328125]],\n",
       " 'bias': [['偏见', 0.8974609375], ['正常', 0.1024169921875]],\n",
       " 'ciron': [['讽刺', 0.1943359375], ['正常', 0.8056640625]],\n",
       " 'intent': [['主观评价/比较/判断', 0.95166015625],\n",
       "  ['寻求建议/帮助', 4.565715789794922e-05],\n",
       "  ['其它', 0.04840087890625]],\n",
       " 'offensive': [['冒犯', 0.96923828125], ['正常', 0.03094482421875]],\n",
       " 'query_risk': [['风险', 0.9765625],\n",
       "  ['个人信息', 0.00023925304412841797],\n",
       "  ['正常', 0.0231475830078125]],\n",
       " 'teenager': [['不良', 0.9384765625], ['正常', 0.06170654296875]],\n",
       " 'politics': [['正常', 0.99853515625],\n",
       "  ['black_种族、民族歧视', 0.0008831024169921875],\n",
       "  ['black_党和国家领导人负面', 0.0001710653305053711],\n",
       "  ['conditional_black_1号领导人', 5.704164505004883e-05],\n",
       "  ['black_国家及政府形象', 2.9981136322021484e-05]],\n",
       " 'porn': [['色情', 0.006618499755859375],\n",
       "  ['低俗', 0.81494140625],\n",
       "  ['色情违禁', 0.00022101402282714844],\n",
       "  ['正常', 0.177978515625]],\n",
       " 'abusive': [['辱骂', 0.85693359375],\n",
       "  ['口头语', 0.11834716796875],\n",
       "  ['正常', 0.02490234375]]}"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c734a339-6f23-4ca3-9e83-1e83807a18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = '/data/albert.xht/T2Ranking/data/queries.dev.tsv'\n",
    "\n",
    "def load_qid(file_name):\n",
    "    qid_list = []\n",
    "    with open(file_name) as inp:\n",
    "        for line in inp:\n",
    "            line = line.strip()\n",
    "            qid = line.split('\\t')[0]\n",
    "            qid_list.append(qid)\n",
    "    return qid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c2ebafd9-458b-4623-9c88-86cde07c0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_list = load_qid(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d89ff4fe-d173-422a-b407-55e05385e17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qid', '0', '1', '2', '3', '4', '5', '6', '7', '8']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qid_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8db5cdd7-bd85-4bd3-898a-82d40ec34016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/data/albert.xht/T2Ranking/data/queries.dev.tsv', sep=\"\\t\",header=0, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f39616bd-ed85-46c0-b725-6c2cd772c540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid                 0\n",
       "text    蜂巢取快递验证码摁错怎么办\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2285f530-254c-4812-a8ad-ffb71f2fd952",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['qid','qry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3d625aaa-0ac3-4009-ac6d-821e0213ef28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid           100\n",
       "qry    看肝癌北京哪个医院好\n",
       "Name: 100, dtype: object"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e78ea5-e363-47ac-963e-b877b5fbbf57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
