{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db655bcd-008a-4059-9014-95cb3a8cec0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2071401it [01:39, 20901.43it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm \n",
    "output_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.all_risk.v9.1'\n",
    "result_list = []\n",
    "with open(output_path) as frobj:\n",
    "    for line in tqdm(frobj):\n",
    "        content = json.loads(line.strip())\n",
    "        result_list.append(content)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cdd0f481-24cc-4a96-a36a-60cff9a2b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/xiaodao/query_risk_v10/query_risk_itag.json', 'w') as fwobj:\n",
    "    # fwobj.write(\"&&&\".join(['content'])+'\\n')\n",
    "    with open('/data/albert.xht/xiaodao/query_risk_v10/query_risk_corpus.json.select', 'r') as frobj:\n",
    "        for idx, line in enumerate(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            inputs = [content['text']]\n",
    "            fwobj.write(\"&&&\".join(inputs)+'\\n')\n",
    "    with open('/data/albert.xht/xiaodao/query_risk_v10/biake_qa_web_text_zh_train.json.offensive.all', 'r') as frobj:\n",
    "        for idx, line in enumerate(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            inputs = [content['text']]\n",
    "            fwobj.write(\"&&&\".join(inputs)+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a51afd9-6b47-48ec-b774-61d6defdf15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2071401it [08:05, 4270.41it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_list = []\n",
    "text_list = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.embed') as frobj:\n",
    "    for line in tqdm(frobj):\n",
    "        content = json.loads(line.strip())\n",
    "        embed_list.append(content['feat'])\n",
    "        text_list.append(content['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "567b524f-3dba-4722-9b4b-3aaf396c03a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LogitClipLoss(nn.Module):\n",
    "    def __init__(self, device, t=1.0):\n",
    "        super(LogitClipLoss, self).__init__()\n",
    "        self.device = device\n",
    "        self.t = t\n",
    "\n",
    "    def forward(self, x, target, reduction):\n",
    "        norms = torch.norm(x, p=2, dim=-1, keepdim=True) + 1e-7\n",
    "        logit_norm = torch.div(x, norms) / self.t\n",
    "        clip = (norms > self.t).expand(-1, x.shape[-1])\n",
    "        logit_clip = torch.where(clip, logit_norm, x)\n",
    "        return F.cross_entropy(logit_clip, target, reduction=reduction)\n",
    "\n",
    "class data_generator_embed(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        \n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def collate(examples):\n",
    "        batch_embed, batch_label = [], []\n",
    "        for item in examples: # batch_size\n",
    "            embed, label = item\n",
    "            batch_label.append(label)\n",
    "            batch_embed.append(embed)\n",
    "\n",
    "        batch_embed = torch.tensor(batch_embed).float()\n",
    "        batch_label = torch.tensor(batch_label).long()#RoBERTa 不需要NSP\n",
    "         \n",
    "        return [batch_embed, batch_label]\n",
    "    \n",
    "import torch\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EmbedLinear(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, dropout_prob, num_labels, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.dense = nn.Linear(input_dim, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        self.out_proj = nn.Linear(self.hidden_size, self.num_labels)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        last_rep = torch.tanh(x)\n",
    "        last_rep = self.dropout(last_rep)\n",
    "        logits = self.out_proj(last_rep)\n",
    "        return logits\n",
    "    \n",
    "    def compile(self, method=\"logistic\", optimizer=\"Adam\", learning_rate=0.001, momentum=0.0):\n",
    "        \"\"\"Wrapper to set the activation, loss and the optimizer.\n",
    "        Parameters:\n",
    "        ----------\n",
    "        method (str) : regression, logistic or multiclass\n",
    "        optimizer (str): SGD, Adam, or RMSprop\n",
    "        \"\"\"\n",
    "        if method == 'regression':\n",
    "            self.activation, self.criterion = None, F.mse_loss\n",
    "        if method == 'logistic':\n",
    "            self.activation, self.criterion = F.sigmoid, F.binary_cross_entropy\n",
    "        if method == 'multiclass':\n",
    "            self.activation, self.criterion = F.softmax, F.cross_entropy\n",
    "        if method == 'multiclass_logitclip':\n",
    "            self.activation, self.criterion = F.softmax, LogitClipLoss(self.device, tau=1.5)\n",
    "\n",
    "        if optimizer == \"Adam\":\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        if optimizer == \"RMSprop\":\n",
    "            self.optimizer = torch.optim.RMSprop(self.parameters(), lr=learning_rate)\n",
    "        if optimizer == \"SGD\":\n",
    "            self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        self.method = method\n",
    "    \n",
    "    def fit(self, data, label, epoch, batch_size):\n",
    "        train_data = data_generator_embed(\n",
    "                    data, label\n",
    "                    )\n",
    "        collate_fn = train_data.collate\n",
    "        print(len(train_data))\n",
    "        train_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                                   batch_size=batch_size, collate_fn=collate_fn,\n",
    "                                                shuffle=True, drop_last=True)\n",
    "        \n",
    "        for epoch in tqdm(range(epoch)):\n",
    "            net = self.train()\n",
    "            for i, (embed, target) in (enumerate(train_loader)):\n",
    "                embed, target = embed.to(self.device), target.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                y_pred =  net(embed)\n",
    "                loss = self.criterion(y_pred, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "    def predict_proba(self, data, batch_size):\n",
    "        \"\"\"Predict predict probability for dataset.\n",
    "        This method will only work with method logistic/multiclass\n",
    "        Parameters:\n",
    "        ----------\n",
    "        dataset (dict): dictionary with the testing dataset -\n",
    "        X_wide_test, X_deep_test, target\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like with the probability for dataset.\n",
    "        \"\"\"\n",
    "        queue = []\n",
    "        pred_list = []\n",
    "        net = self.eval()\n",
    "        for d in data:\n",
    "            queue.append(d)\n",
    "            if np.mod(len(queue), batch_size) == 0:\n",
    "                embed = torch.tensor(queue).float()\n",
    "                embed = embed.to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    pred = net(embed)\n",
    "                    probs = nn.Softmax(dim=-1)(pred).cpu().data.numpy()\n",
    "                pred_list.extend(probs)\n",
    "                queue = []\n",
    "        if queue:\n",
    "            embed = torch.tensor(queue).float()\n",
    "            embed = embed.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                pred = net(embed)\n",
    "                probs = nn.Softmax(dim=-1)(pred).cpu().data.numpy()\n",
    "            pred_list.extend(probs)\n",
    "        return np.array(pred_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3dee75e0-5cbe-47e4-82db-33884bc56137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LogitClipLoss(nn.Module):\n",
    "    def __init__(self, device, t=1.0):\n",
    "        super(LogitClipLoss, self).__init__()\n",
    "        self.device = device\n",
    "        self.t = t\n",
    "\n",
    "    def forward(self, x, target, reduction='mean'):\n",
    "        norms = torch.norm(x, p=2, dim=-1, keepdim=True) + 1e-7\n",
    "        logit_norm = torch.div(x, norms) / self.t\n",
    "        clip = (norms > self.t).expand(-1, x.shape[-1])\n",
    "        logit_clip = torch.where(clip, logit_norm, x)\n",
    "        return F.cross_entropy(logit_clip, target, reduction=reduction)\n",
    "\n",
    "class data_generator_wide_and_deep(Dataset):\n",
    "    def __init__(self, data, feat, label):\n",
    "        \n",
    "        self.data = data\n",
    "        self.feat = feat\n",
    "        self.label = label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.feat[idx], self.label[idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def collate(examples):\n",
    "        batch_embed, batch_feat, batch_label = [], [], []\n",
    "        for item in examples: # batch_size\n",
    "            embed, feat, label = item\n",
    "            batch_label.append(label)\n",
    "            batch_embed.append(embed)\n",
    "            batch_feat.append(feat)\n",
    "            \n",
    "        batch_embed = torch.tensor(batch_embed).float()\n",
    "        batch_label = torch.tensor(batch_label).long()#RoBERTa 不需要NSP\n",
    "        batch_feat = torch.tensor(batch_feat).float()\n",
    "         \n",
    "        return [batch_embed, batch_feat, batch_label]\n",
    "    \n",
    "import torch\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EmbedLinear_wide_and_deep(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, dropout_prob, num_labels, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.dense = nn.Linear(input_dim, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        self.out_proj = nn.Linear(self.hidden_size, self.num_labels)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        last_rep = torch.tanh(x)\n",
    "        last_rep = self.dropout(last_rep)\n",
    "        logits = self.out_proj(last_rep)\n",
    "        return logits\n",
    "    \n",
    "    def compile(self, method=\"logistic\", optimizer=\"Adam\", learning_rate=0.001, momentum=0.0):\n",
    "        \"\"\"Wrapper to set the activation, loss and the optimizer.\n",
    "        Parameters:\n",
    "        ----------\n",
    "        method (str) : regression, logistic or multiclass\n",
    "        optimizer (str): SGD, Adam, or RMSprop\n",
    "        \"\"\"\n",
    "        if method == 'regression':\n",
    "            self.activation, self.criterion = None, F.mse_loss\n",
    "        if method == 'logistic':\n",
    "            self.activation, self.criterion = F.sigmoid, F.binary_cross_entropy\n",
    "        if method == 'multiclass':\n",
    "            self.activation, self.criterion = F.softmax, F.cross_entropy\n",
    "        if method == 'multiclass_logitclip':\n",
    "            self.activation, self.criterion = F.softmax, LogitClipLoss(self.device, tau=1.5)\n",
    "\n",
    "        if optimizer == \"Adam\":\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        if optimizer == \"RMSprop\":\n",
    "            self.optimizer = torch.optim.RMSprop(self.parameters(), lr=learning_rate)\n",
    "        if optimizer == \"SGD\":\n",
    "            self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        self.method = method\n",
    "    \n",
    "    def fit(self, data, feat, label, epoch, batch_size):\n",
    "        train_data = data_generator_wide_and_deep(\n",
    "                    data, feat, label\n",
    "                    )\n",
    "        collate_fn = train_data.collate\n",
    "        print(len(train_data))\n",
    "        train_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                                   batch_size=batch_size, collate_fn=collate_fn,\n",
    "                                                shuffle=True, drop_last=True)\n",
    "        \n",
    "        for epoch in tqdm(range(epoch)):\n",
    "            net = self.train()\n",
    "            for i, (embed, feat, target) in (enumerate(train_loader)):\n",
    "                embed, feat, target = embed.to(self.device), feat.to(self.device), target.to(self.device)\n",
    "                embed = torch.cat([feat, embed], dim=-1)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                y_pred =  net(embed)\n",
    "                loss = self.criterion(y_pred, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "    def predict_proba(self, data, feat, batch_size):\n",
    "        \"\"\"Predict predict probability for dataset.\n",
    "        This method will only work with method logistic/multiclass\n",
    "        Parameters:\n",
    "        ----------\n",
    "        dataset (dict): dictionary with the testing dataset -\n",
    "        X_wide_test, X_deep_test, target\n",
    "        Returns:\n",
    "        --------\n",
    "        array-like with the probability for dataset.\n",
    "        \"\"\"\n",
    "        queue_d = []\n",
    "        queue_f = []\n",
    "        pred_list = []\n",
    "        net = self.eval()\n",
    "        for d, f in zip(data, feat):\n",
    "            queue_d.append(d)\n",
    "            queue_f.append(f)\n",
    "            if np.mod(len(queue_d), batch_size) == 0:\n",
    "                embed = torch.tensor(queue_d).float()\n",
    "                embed = embed.to(self.device)\n",
    "                feat = torch.tensor(queue_f).float()\n",
    "                feat = feat.to(self.device)\n",
    "                embed = torch.cat([feat, embed], dim=-1)\n",
    "                with torch.no_grad():\n",
    "                    pred = net(embed)\n",
    "                    probs = nn.Softmax(dim=-1)(pred).cpu().data.numpy()\n",
    "                pred_list.extend(probs)\n",
    "                queue_d = []\n",
    "                queue_f = []\n",
    "        if queue_d:\n",
    "            embed = torch.tensor(queue_d).float()\n",
    "            embed = embed.to(self.device)\n",
    "            feat = torch.tensor(queue_f).float()\n",
    "            feat = feat.to(self.device)\n",
    "            embed = torch.cat([feat, embed], dim=-1)\n",
    "            with torch.no_grad():\n",
    "                pred = net(embed)\n",
    "                probs = nn.Softmax(dim=-1)(pred).cpu().data.numpy()\n",
    "            pred_list.extend(probs)\n",
    "        return np.array(pred_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03f1b8be-d2e9-4da4-8cf3-0a975695545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "@njit\n",
    "def each_evidence(y_, f, fh, v, s, vh, N, D):\n",
    "    \"\"\"\n",
    "    compute the maximum evidence for each class\n",
    "    \"\"\"\n",
    "    epsilon = 1e-5\n",
    "    alpha = 1.0\n",
    "    beta = 1.0\n",
    "    lam = alpha / beta\n",
    "    tmp = (vh @ (f @ np.ascontiguousarray(y_)))\n",
    "    for _ in range(11):\n",
    "        # should converge after at most 10 steps\n",
    "        # typically converge after two or three steps\n",
    "        gamma = (s / (s + lam)).sum()\n",
    "        # A = v @ np.diag(alpha + beta * s) @ v.transpose() # no need to compute A\n",
    "        # A_inv = v @ np.diag(1.0 / (alpha + beta * s)) @ v.transpose() # no need to compute A_inv\n",
    "        m = v @ (tmp * beta / (alpha + beta * s))\n",
    "        alpha_de = (m * m).sum()\n",
    "        alpha = gamma / (alpha_de + epsilon)\n",
    "        beta_de = ((y_ - fh @ m) ** 2).sum()\n",
    "        beta = (N - gamma) / (beta_de + epsilon)\n",
    "        new_lam = alpha / beta\n",
    "        if np.abs(new_lam - lam) / lam < 0.01:\n",
    "            break\n",
    "        lam = new_lam\n",
    "    evidence = D / 2.0 * np.log(alpha) \\\n",
    "               + N / 2.0 * np.log(beta) \\\n",
    "               - 0.5 * np.sum(np.log(alpha + beta * s)) \\\n",
    "               - beta / 2.0 * (beta_de + epsilon) \\\n",
    "               - alpha / 2.0 * (alpha_de + epsilon) \\\n",
    "               - N / 2.0 * np.log(2 * np.pi)\n",
    "    return evidence / N, alpha, beta, m\n",
    "\n",
    "\n",
    "# use pseudo data to compile the function\n",
    "# D = 20, N = 50\n",
    "f_tmp = np.random.randn(20, 50).astype(np.float64)\n",
    "each_evidence(np.random.randint(0, 2, 50).astype(np.float64), f_tmp, f_tmp.transpose(), np.eye(20, dtype=np.float64), np.ones(20, dtype=np.float64), np.eye(20, dtype=np.float64), 50, 20)\n",
    "\n",
    "\n",
    "@njit\n",
    "def truncated_svd(x):\n",
    "    u, s, vh = np.linalg.svd(x.transpose() @ x)\n",
    "    s = np.sqrt(s)\n",
    "    u_times_sigma = x @ vh.transpose()\n",
    "    k = np.sum((s > 1e-10) * 1)  # rank of f\n",
    "    s = s.reshape(-1, 1)\n",
    "    s = s[:k]\n",
    "    vh = vh[:k]\n",
    "    u = u_times_sigma[:, :k] / s.reshape(1, -1)\n",
    "    return u, s, vh\n",
    "truncated_svd(np.random.randn(20, 10).astype(np.float64))\n",
    "\n",
    "\n",
    "class LogME(object):\n",
    "    def __init__(self, regression=False):\n",
    "        \"\"\"\n",
    "            :param regression: whether regression\n",
    "        \"\"\"\n",
    "        self.regression = regression\n",
    "        self.fitted = False\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.num_dim = 0\n",
    "        self.alphas = []  # alpha for each class / dimension\n",
    "        self.betas = []  # beta for each class / dimension\n",
    "        # self.ms.shape --> [C, D]\n",
    "        self.ms = []  # m for each class / dimension\n",
    "\n",
    "    def _fit_icml(self, f: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        LogME calculation proposed in the ICML 2021 paper\n",
    "        \"LogME: Practical Assessment of Pre-trained Models for Transfer Learning\"\n",
    "        at http://proceedings.mlr.press/v139/you21b.html\n",
    "        \"\"\"\n",
    "        fh = f\n",
    "        f = f.transpose()\n",
    "        D, N = f.shape\n",
    "        v, s, vh = np.linalg.svd(f @ fh, full_matrices=True)\n",
    "\n",
    "        evidences = []\n",
    "        self.num_dim = y.shape[1] if self.regression else int(y.max() + 1)\n",
    "        for i in range(self.num_dim):\n",
    "            y_ = y[:, i] if self.regression else (y == i).astype(np.float64)\n",
    "            evidence, alpha, beta, m = each_evidence(y_, f, fh, v, s, vh, N, D)\n",
    "            evidences.append(evidence)\n",
    "            self.alphas.append(alpha)\n",
    "            self.betas.append(beta)\n",
    "            self.ms.append(m)\n",
    "        self.ms = np.stack(self.ms)\n",
    "        return np.mean(evidences)\n",
    "\n",
    "    def _fit_fixed_point(self, f: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        LogME calculation proposed in the arxiv 2021 paper\n",
    "        \"Ranking and Tuning Pre-trained Models: A New Paradigm of Exploiting Model Hubs\"\n",
    "        at https://arxiv.org/abs/2110.10545\n",
    "        \"\"\"\n",
    "        N, D = f.shape  # k = min(N, D)\n",
    "        if N > D: # direct SVD may be expensive\n",
    "            u, s, vh = truncated_svd(f)\n",
    "        else:\n",
    "            u, s, vh = np.linalg.svd(f, full_matrices=False)\n",
    "        # u.shape = N x k\n",
    "        # s.shape = k\n",
    "        # vh.shape = k x D\n",
    "        s = s.reshape(-1, 1)\n",
    "        sigma = (s ** 2)\n",
    "\n",
    "        evidences = []\n",
    "        self.num_dim = y.shape[1] if self.regression else int(y.max() + 1)\n",
    "        for i in range(self.num_dim):\n",
    "            y_ = y[:, i] if self.regression else (y == i).astype(np.float64)\n",
    "            y_ = y_.reshape(-1, 1)\n",
    "            x = u.T @ y_  # x has shape [k, 1], but actually x should have shape [N, 1]\n",
    "            x2 = x ** 2\n",
    "            res_x2 = (y_ ** 2).sum() - x2.sum()  # if k < N, we compute sum of xi for 0 singular values directly\n",
    "\n",
    "            alpha, beta = 1.0, 1.0\n",
    "            for _ in range(11):\n",
    "                t = alpha / beta\n",
    "                gamma = (sigma / (sigma + t)).sum()\n",
    "                m2 = (sigma * x2 / ((t + sigma) ** 2)).sum()\n",
    "                res2 = (x2 / ((1 + sigma / t) ** 2)).sum() + res_x2\n",
    "                alpha = gamma / (m2 + 1e-5)\n",
    "                beta = (N - gamma) / (res2 + 1e-5)\n",
    "                t_ = alpha / beta\n",
    "                evidence = D / 2.0 * np.log(alpha) \\\n",
    "                           + N / 2.0 * np.log(beta) \\\n",
    "                           - 0.5 * np.sum(np.log(alpha + beta * sigma)) \\\n",
    "                           - beta / 2.0 * res2 \\\n",
    "                           - alpha / 2.0 * m2 \\\n",
    "                           - N / 2.0 * np.log(2 * np.pi)\n",
    "                evidence /= N\n",
    "                if abs(t_ - t) / t <= 1e-3:  # abs(t_ - t) <= 1e-5 or abs(1 / t_ - 1 / t) <= 1e-5:\n",
    "                    break\n",
    "            evidence = D / 2.0 * np.log(alpha) \\\n",
    "                       + N / 2.0 * np.log(beta) \\\n",
    "                       - 0.5 * np.sum(np.log(alpha + beta * sigma)) \\\n",
    "                       - beta / 2.0 * res2 \\\n",
    "                       - alpha / 2.0 * m2 \\\n",
    "                       - N / 2.0 * np.log(2 * np.pi)\n",
    "            evidence /= N\n",
    "            m = 1.0 / (t + sigma) * s * x\n",
    "            m = (vh.T @ m).reshape(-1)\n",
    "            evidences.append(evidence)\n",
    "            self.alphas.append(alpha)\n",
    "            self.betas.append(beta)\n",
    "            self.ms.append(m)\n",
    "        self.ms = np.stack(self.ms)\n",
    "        return np.mean(evidences)\n",
    "\n",
    "    _fit = _fit_fixed_point\n",
    "\n",
    "    def fit(self, f: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param f: [N, F], feature matrix from pre-trained model\n",
    "        :param y: target labels.\n",
    "            For classification, y has shape [N] with element in [0, C_t).\n",
    "            For regression, y has shape [N, C] with C regression-labels\n",
    "        :return: LogME score (how well f can fit y directly)\n",
    "        \"\"\"\n",
    "        if self.fitted:\n",
    "            warnings.warn('re-fitting for new data. old parameters cleared.')\n",
    "            self.reset()\n",
    "        else:\n",
    "            self.fitted = True\n",
    "        f = f.astype(np.float64)\n",
    "        if self.regression:\n",
    "            y = y.astype(np.float64)\n",
    "            if len(y.shape) == 1:\n",
    "                y = y.reshape(-1, 1)\n",
    "        return self._fit(f, y)\n",
    "\n",
    "    def predict(self, f: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param f: [N, F], feature matrix\n",
    "        :return: prediction, return shape [N, X]\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"not fitted, please call fit first\")\n",
    "        f = f.astype(np.float64)\n",
    "        logits = f @ self.ms.T\n",
    "        if self.regression:\n",
    "            return logits\n",
    "        return np.argmax(logits, axis=-1)\n",
    "    \n",
    "    def predict_prob(self, f: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param f: [N, F], feature matrix\n",
    "        :return: prediction, return shape [N, X]\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise RuntimeError(\"not fitted, please call fit first\")\n",
    "        f = f.astype(np.float64)\n",
    "        logits = f @ self.ms.T\n",
    "        if self.regression:\n",
    "            return logits\n",
    "        return softmax(logits, axis=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "092ae202-c39e-462d-a7fe-ea2f857b2479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2071401it [00:07, 280165.95it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "result_matrix = []\n",
    "result_item = []\n",
    "result_embed = []\n",
    "result_text = []\n",
    "for idx, item in tqdm(enumerate(result_list)):\n",
    "    p = []\n",
    "    for key in ['senti', 'bias', 'ciron', 'offensive', 'teenager', 'senti_query']:\n",
    "        if item['score_list'][key][0][1] >= 0.5:\n",
    "            p.append(1)\n",
    "        else:\n",
    "            p.append(0)\n",
    "    if item['score_list']['query_risk'][0][1] >= 0.5:\n",
    "        p.append(1)\n",
    "    else:\n",
    "        p.append(0)\n",
    "    if item['score_list']['query_risk'][1][1] >= 0.5:\n",
    "        p.append(1)\n",
    "    else:\n",
    "        p.append(0)\n",
    "    if sum(p) <= 1:\n",
    "        continue\n",
    "    result_matrix.append(p)\n",
    "    result_item.append(item)\n",
    "    result_embed.append(embed_list[idx])\n",
    "    result_text.append(text_list[idx])\n",
    "    \n",
    "\n",
    "# result_matrix = np.array(result_matrix)\n",
    "# votes = np.sum(result_matrix, axis=-1)\n",
    "# labels = np.array(votes >= 3).astype(np.int)\n",
    "# sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4f5de2-b09f-4a45-9e5e-438660e4985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "class make_model():\n",
    "    def __init__(self,param,num_round=300):\n",
    "        self.param=param\n",
    "        self.num_round=num_round\n",
    "    def fit(self,gen_data):\n",
    "        iteration = 0\n",
    "        \n",
    "        for df in gen_data:\n",
    "            dtrain = xgb.DMatrix(np.array(df[features]), label=df['label'])\n",
    "            if iteration ==0:\n",
    "                model = xgb.Booster(self.param, [dtrain])\n",
    "            model = xgb.train(self.param,dtrain,num_boost_round=1, xgb_model=model)\n",
    "            iteration += 1\n",
    "            \n",
    "        self.model_=model\n",
    "    def predict(self,X):\n",
    "        dtest=xgb.DMatrix(X)\n",
    "        return self.model_.predict(dtest)>0.5 # use argmax in non-binary classification\n",
    "parameters = {'max_depth':5, \"booster\":\"gbtree\"} # parameters to tune, see xgboost doc. Can be used to make boosted trees or Random Forests.\n",
    "model = make_model(parameters) \n",
    "model.fit(gen_data)\n",
    "xgb.plot_importance(model.model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb7f0190-d253-4a29-aa96-b46821f91b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===start==== 0\n",
      "772017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.23s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(772017, 2)\n",
      "0.1335566444780361\n",
      "===start==== 1\n",
      "772017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:14<00:00, 14.90s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(772017, 2)\n",
      "0.026279214058757775\n",
      "===start==== 2\n",
      "772017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.06s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(772017, 2)\n",
      "0.018408921047075387\n",
      "===start==== 3\n",
      "772017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.03s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(772017, 2)\n",
      "0.015109770898827358\n",
      "===start==== 4\n",
      "772017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:14<00:00, 14.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(772017, 2)\n",
      "0.010936287672421722\n"
     ]
    }
   ],
   "source": [
    "\n",
    "votes = np.sum(result_matrix, axis=-1)\n",
    "labels = np.array(votes >= 3).astype(np.int)\n",
    "epoch = 1\n",
    "batch_size = 512\n",
    "\n",
    "model_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    device = \"cuda:1\"\n",
    "    model = EmbedLinear(input_dim=384, hidden_size=128, dropout_prob=0.1, num_labels=2, device=device).to(device)\n",
    "    model.compile(method=\"multiclass\", optimizer=\"Adam\", learning_rate=1e-4, momentum=0.0)    \n",
    "    \n",
    "    print('===start====', i)\n",
    "    result = model.fit(result_embed, labels, epoch=epoch, batch_size=batch_size)\n",
    "    probs = model.predict_proba(result_embed, batch_size=batch_size)\n",
    "    print(probs.shape)\n",
    "    prev_labels = np.argmax(probs, axis=-1)\n",
    "    diff = (prev_labels != np.array(labels)).sum()\n",
    "    print(diff / prev_labels.shape[0])\n",
    "    labels = prev_labels\n",
    "    \n",
    "    model_list.append(model)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a115ca2f-adcf-4376-bd8f-3109829f2348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===start==== 0\n",
      "772017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:17<00:00, 17.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(772017, 2)\n",
      "0.03231664587696903\n",
      "===start==== 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(772017, 2)\n",
      "0.028674239038777645\n",
      "===start==== 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(772017, 2)\n",
      "0.02079746948577557\n",
      "===start==== 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(772017, 2)\n",
      "0.013083908774029587\n",
      "===start==== 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(772017, 2)\n",
      "0.009998484489331194\n"
     ]
    }
   ],
   "source": [
    "\n",
    "votes = np.sum(result_matrix, axis=-1)\n",
    "labels = np.array(votes >= 3).astype(np.int)\n",
    "epoch = 1\n",
    "batch_size = 512\n",
    "\n",
    "model_list_wide_and_deep = []\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    device = \"cuda:1\"\n",
    "    model = EmbedLinear_wide_and_deep(input_dim=384+8, hidden_size=128, dropout_prob=0.1, num_labels=2, device=device).to(device)\n",
    "    model.compile(method=\"multiclass\", optimizer=\"Adam\", learning_rate=1e-4, momentum=0.0)    \n",
    "    \n",
    "    print('===start====', i)\n",
    "    result = model.fit(result_embed, np.array(result_matrix), labels, epoch=epoch, batch_size=batch_size)\n",
    "    probs = model.predict_proba(result_embed, np.array(result_matrix), batch_size=batch_size)\n",
    "    print(probs.shape)\n",
    "    prev_labels = np.argmax(probs, axis=-1)\n",
    "    diff = (prev_labels != np.array(labels)).sum()\n",
    "    print(diff / prev_labels.shape[0])\n",
    "    labels = prev_labels\n",
    "    \n",
    "    model_list_wide_and_deep.append(model)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c21398b0-482e-45ab-adc8-f1cfe9750ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model_list[1].predict_proba(result_embed, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f20c0453-acfc-4d33-ae4a-c3dcb6a5dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_wide_and_deep = model_list_wide_and_deep[0].predict_proba(result_embed, np.array(result_matrix), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "49af7541-553e-482d-b9de-8a5fc06a3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "black = []\n",
    "for prob, prob_wide_and_deep, item in zip(probs, probs_wide_and_deep, result_item):\n",
    "    if prob[1] >= 0.9 and prob_wide_and_deep[1] > 0.9:\n",
    "        black.append((prob, item))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "568bcbfe-b9b3-4903-9d00-617d86ee1cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([0.03497289, 0.9650271 ], dtype=float32),\n",
       "  {'text': '追赶小偷致其跳河溺亡应如何处置？',\n",
       "   'topic': ['法律'],\n",
       "   'score_list': {'senti_query': [['负向', 0.9976511597633362],\n",
       "     ['中性', 0.0022351674269884825],\n",
       "     ['正向', 0.00011369012645445764]],\n",
       "    'senti': [['负向', 0.9974364638328552], ['正向', 0.0025636202190071344]],\n",
       "    'bias': [['偏见', 0.25562426447868347], ['正常', 0.7443757057189941]],\n",
       "    'ciron': [['讽刺', 0.2297625094652176], ['正常', 0.7702375054359436]],\n",
       "    'intent': [['主观评价/比较/判断', 0.0007977944333106279],\n",
       "     ['寻求建议/帮助', 0.9991441965103149],\n",
       "     ['其它', 5.80373452976346e-05]],\n",
       "    'offensive': [['冒犯', 0.9323585629463196], ['正常', 0.0676414966583252]],\n",
       "    'query_risk': [['风险', 0.9970659613609314],\n",
       "     ['个人信息', 0.0005186186754144728],\n",
       "     ['正常', 0.0024155047722160816]],\n",
       "    'teenager': [['不良', 0.36774078011512756], ['正常', 0.6322592496871948]]}}),\n",
       " (array([3.8990277e-04, 9.9961013e-01], dtype=float32),\n",
       "  {'text': '如何看待部分中国人认为欧洲收留叙利亚难民是圣母行为？',\n",
       "   'topic': ['宗教'],\n",
       "   'score_list': {'senti_query': [['负向', 0.9793327450752258],\n",
       "     ['中性', 0.020264051854610443],\n",
       "     ['正向', 0.000403146113967523]],\n",
       "    'senti': [['负向', 0.9969090819358826], ['正向', 0.003090877318754792]],\n",
       "    'bias': [['偏见', 0.06782922148704529], ['正常', 0.9321708083152771]],\n",
       "    'ciron': [['讽刺', 0.43136364221572876], ['正常', 0.5686363577842712]],\n",
       "    'intent': [['主观评价/比较/判断', 0.999966025352478],\n",
       "     ['寻求建议/帮助', 2.9432016162900254e-05],\n",
       "     ['其它', 4.560182787827216e-06]],\n",
       "    'offensive': [['冒犯', 0.8231633901596069], ['正常', 0.17683659493923187]],\n",
       "    'query_risk': [['风险', 0.9924658536911011],\n",
       "     ['个人信息', 0.0001396939333062619],\n",
       "     ['正常', 0.007394387852400541]],\n",
       "    'teenager': [['不良', 0.14727503061294556], ['正常', 0.8527250289916992]]}}),\n",
       " (array([0.05742543, 0.94257456], dtype=float32),\n",
       "  {'text': '这次美团清真外卖事件对美团到底有多大影响？',\n",
       "   'topic': ['社会'],\n",
       "   'score_list': {'senti_query': [['负向', 0.767403781414032],\n",
       "     ['中性', 0.22806036472320557],\n",
       "     ['正向', 0.004535815678536892]],\n",
       "    'senti': [['负向', 0.9825520515441895], ['正向', 0.017447976395487785]],\n",
       "    'bias': [['偏见', 0.07857295870780945], ['正常', 0.9214270114898682]],\n",
       "    'ciron': [['讽刺', 0.13318277895450592], ['正常', 0.8668172359466553]],\n",
       "    'intent': [['主观评价/比较/判断', 0.8935627341270447],\n",
       "     ['寻求建议/帮助', 0.0805356353521347],\n",
       "     ['其它', 0.025901615619659424]],\n",
       "    'offensive': [['冒犯', 0.7724584341049194], ['正常', 0.22754155099391937]],\n",
       "    'query_risk': [['风险', 0.99199378490448],\n",
       "     ['个人信息', 2.4584705897723325e-05],\n",
       "     ['正常', 0.007981626316905022]],\n",
       "    'teenager': [['不良', 0.1429956555366516], ['正常', 0.8570043444633484]]}}),\n",
       " (array([0.0509115, 0.9490885], dtype=float32),\n",
       "  {'text': '读博觉得周围女生太幼稚毫无恋爱欲望怎么办？',\n",
       "   'topic': ['恋爱'],\n",
       "   'score_list': {'senti_query': [['负向', 0.9981037378311157],\n",
       "     ['中性', 0.0017871204763650894],\n",
       "     ['正向', 0.00010912009747698903]],\n",
       "    'senti': [['负向', 0.9981076717376709], ['正向', 0.0018922671442851424]],\n",
       "    'bias': [['偏见', 0.4822826683521271], ['正常', 0.5177173018455505]],\n",
       "    'ciron': [['讽刺', 0.16027210652828217], ['正常', 0.8397278785705566]],\n",
       "    'intent': [['主观评价/比较/判断', 0.00011486637959023938],\n",
       "     ['寻求建议/帮助', 0.9998685121536255],\n",
       "     ['其它', 1.6631100152153522e-05]],\n",
       "    'offensive': [['冒犯', 0.4905865788459778], ['正常', 0.5094133615493774]],\n",
       "    'query_risk': [['风险', 0.9237884879112244],\n",
       "     ['个人信息', 4.882407301920466e-05],\n",
       "     ['正常', 0.07616277784109116]],\n",
       "    'teenager': [['不良', 0.5770788192749023], ['正常', 0.42292115092277527]]}}),\n",
       " (array([0.00150113, 0.9984989 ], dtype=float32),\n",
       "  {'text': '如何看待2016年10月15号川普粉丝反击在川普集会上挑衅的反对者？',\n",
       "   'topic': ['时事政治'],\n",
       "   'score_list': {'senti_query': [['负向', 0.988330602645874],\n",
       "     ['中性', 0.011332795023918152],\n",
       "     ['正向', 0.00033662712667137384]],\n",
       "    'senti': [['负向', 0.9953930377960205], ['正向', 0.004606969188898802]],\n",
       "    'bias': [['偏见', 0.0441293865442276], ['正常', 0.9558706283569336]],\n",
       "    'ciron': [['讽刺', 0.23884901404380798], ['正常', 0.7611509561538696]],\n",
       "    'intent': [['主观评价/比较/判断', 0.9999552965164185],\n",
       "     ['寻求建议/帮助', 2.517923894629348e-05],\n",
       "     ['其它', 1.9608169168350287e-05]],\n",
       "    'offensive': [['冒犯', 0.756280779838562], ['正常', 0.24371926486492157]],\n",
       "    'query_risk': [['风险', 0.9881429076194763],\n",
       "     ['个人信息', 6.971148832235485e-05],\n",
       "     ['正常', 0.011787387542426586]],\n",
       "    'teenager': [['不良', 0.1057944968342781], ['正常', 0.8942055106163025]]}}),\n",
       " (array([0.03119525, 0.9688047 ], dtype=float32),\n",
       "  {'text': '怎样判断自己的年幼女儿是否被性侵了？',\n",
       "   'topic': ['心理健康'],\n",
       "   'score_list': {'senti_query': [['负向', 0.9869676232337952],\n",
       "     ['中性', 0.012800105847418308],\n",
       "     ['正向', 0.00023223452444653958]],\n",
       "    'senti': [['负向', 0.9931926131248474], ['正向', 0.006807425525039434]],\n",
       "    'bias': [['偏见', 0.07318609952926636], ['正常', 0.9268138408660889]],\n",
       "    'ciron': [['讽刺', 0.15292130410671234], ['正常', 0.8470787405967712]],\n",
       "    'intent': [['主观评价/比较/判断', 0.06354385614395142],\n",
       "     ['寻求建议/帮助', 0.9358877539634705],\n",
       "     ['其它', 0.0005683917552232742]],\n",
       "    'offensive': [['冒犯', 0.5355421900749207], ['正常', 0.46445778012275696]],\n",
       "    'query_risk': [['风险', 0.9903505444526672],\n",
       "     ['个人信息', 2.831535311997868e-05],\n",
       "     ['正常', 0.00962104368954897]],\n",
       "    'teenager': [['不良', 0.12176614254713058], ['正常', 0.8782338500022888]]}}),\n",
       " (array([0.01531803, 0.98468196], dtype=float32),\n",
       "  {'text': '网红脸的特征是什么？怎样看待整容成瘾的女生？',\n",
       "   'topic': ['情感'],\n",
       "   'score_list': {'senti_query': [['负向', 0.8423722982406616],\n",
       "     ['中性', 0.14417122304439545],\n",
       "     ['正向', 0.013456513173878193]],\n",
       "    'senti': [['负向', 0.9949896931648254], ['正向', 0.005010298453271389]],\n",
       "    'bias': [['偏见', 0.21872077882289886], ['正常', 0.7812792062759399]],\n",
       "    'ciron': [['讽刺', 0.19445893168449402], ['正常', 0.8055410385131836]],\n",
       "    'intent': [['主观评价/比较/判断', 0.9984641075134277],\n",
       "     ['寻求建议/帮助', 0.0013115371111780405],\n",
       "     ['其它', 0.00022442013141699135]],\n",
       "    'offensive': [['冒犯', 0.6062151193618774], ['正常', 0.39378488063812256]],\n",
       "    'query_risk': [['风险', 0.9864935874938965],\n",
       "     ['个人信息', 1.0251947969663888e-05],\n",
       "     ['正常', 0.013496183790266514]],\n",
       "    'teenager': [['不良', 0.40911704301834106], ['正常', 0.5908829569816589]]}}),\n",
       " (array([0.01082141, 0.98917854], dtype=float32),\n",
       "  {'text': '被爸妈家庭冷暴力我该怎么办?',\n",
       "   'topic': ['家庭关系'],\n",
       "   'score_list': {'senti_query': [['负向', 0.9983389377593994],\n",
       "     ['中性', 0.0015730169834569097],\n",
       "     ['正向', 8.804160461295396e-05]],\n",
       "    'senti': [['负向', 0.9972838163375854], ['正向', 0.00271621230058372]],\n",
       "    'bias': [['偏见', 0.35036057233810425], ['正常', 0.6496394276618958]],\n",
       "    'ciron': [['讽刺', 0.1438562124967575], ['正常', 0.8561437726020813]],\n",
       "    'intent': [['主观评价/比较/判断', 0.00014814466703683138],\n",
       "     ['寻求建议/帮助', 0.9998003840446472],\n",
       "     ['其它', 5.149235585122369e-05]],\n",
       "    'offensive': [['冒犯', 0.8720724582672119], ['正常', 0.12792754173278809]],\n",
       "    'query_risk': [['风险', 0.9616429805755615],\n",
       "     ['个人信息', 9.424964082427323e-05],\n",
       "     ['正常', 0.0382627509534359]],\n",
       "    'teenager': [['不良', 0.5092096924781799], ['正常', 0.49079036712646484]]}}),\n",
       " (array([0.00410839, 0.9958916 ], dtype=float32),\n",
       "  {'text': '萨德反导系统的幕后黑手明明是美国，为啥大家不去抵制沃尔玛，好莱坞而是去抵制韩国的乐天和韩剧？',\n",
       "   'topic': ['国家'],\n",
       "   'score_list': {'senti_query': [['负向', 0.9968612194061279],\n",
       "     ['中性', 0.003037339076399803],\n",
       "     ['正向', 0.0001015142843243666]],\n",
       "    'senti': [['负向', 0.9987077713012695], ['正向', 0.0012921987799927592]],\n",
       "    'bias': [['偏见', 0.2582761347293854], ['正常', 0.7417238354682922]],\n",
       "    'ciron': [['讽刺', 0.315407931804657], ['正常', 0.684592068195343]],\n",
       "    'intent': [['主观评价/比较/判断', 0.7042437791824341],\n",
       "     ['寻求建议/帮助', 0.2262142300605774],\n",
       "     ['其它', 0.0695420652627945]],\n",
       "    'offensive': [['冒犯', 0.9943832159042358], ['正常', 0.005616738460958004]],\n",
       "    'query_risk': [['风险', 0.9983199238777161],\n",
       "     ['个人信息', 0.00031060384935699403],\n",
       "     ['正常', 0.001369415083900094]],\n",
       "    'teenager': [['不良', 0.6544215083122253], ['正常', 0.34557852149009705]]}}),\n",
       " (array([0.00347944, 0.9965205 ], dtype=float32),\n",
       "  {'text': '中国同性恋群体出路何在？',\n",
       "   'topic': ['LGBT'],\n",
       "   'score_list': {'senti_query': [['负向', 0.831189751625061],\n",
       "     ['中性', 0.15795853734016418],\n",
       "     ['正向', 0.01085177343338728]],\n",
       "    'senti': [['负向', 0.9676901698112488], ['正向', 0.03230981528759003]],\n",
       "    'bias': [['偏见', 0.172183096408844], ['正常', 0.827816903591156]],\n",
       "    'ciron': [['讽刺', 0.14186614751815796], ['正常', 0.8581339120864868]],\n",
       "    'intent': [['主观评价/比较/判断', 0.11517012864351273],\n",
       "     ['寻求建议/帮助', 0.8601287007331848],\n",
       "     ['其它', 0.024701213464140892]],\n",
       "    'offensive': [['冒犯', 0.625041663646698], ['正常', 0.3749583661556244]],\n",
       "    'query_risk': [['风险', 0.9982445240020752],\n",
       "     ['个人信息', 4.0006481867749244e-05],\n",
       "     ['正常', 0.0017155257519334555]],\n",
       "    'teenager': [['不良', 0.13448166847229004], ['正常', 0.86551833152771]]}})]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9a1b39b-5804-431b-9fa6-cf2fc21b5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/data/albert.xht/raw_chat_corpus/topic_classification_v4/embed_linear_small.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7396ea32-5c9d-48fa-a7ff-ef72408f0e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "black = []\n",
    "white = []\n",
    "for text, label, prob, resp in zip(result_text, labels, probs, result_item):\n",
    "    if prob[1] > 0.9:\n",
    "        black.append((label, prob, resp, text))\n",
    "    elif prob[0] > 0.9 and resp['score_list']['query_risk'][0][-1] > 0.5 and resp['score_list']['query_risk'][0][-1] < 0.7:\n",
    "        white.append((label, prob, resp, text))\n",
    "\n",
    "# labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4be3feee-9a35-434d-a72c-133922c2c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logme = LogME(regression=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b07624be-3bde-4a92-a65b-9afe4c3347e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 8, 2, 3, 1, 9, 5, 6, 7, 4]),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([0, 8, 2, 3, 1, 9, 5, 6, 7, 4]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.permutation(np.array(range(10)))\n",
    "np.array(range(10))[a], np.array(range(10)), a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1783204b-24f5-4a73-baa0-d9cee82673a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162260 (772017, 384) (772017,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:28<04:18, 28.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1297950692795625 -0.2483103892726054 118898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:08<04:15, 31.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03152650783596734 0.10574989211574631 100795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:52<04:10, 35.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024735206608144638 0.17886811820834642 84069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:35<03:46, 37.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02306296363940172 0.2368189375284372 67544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [03:19<03:18, 39.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02293990935432769 0.30117219002876827 50676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [03:58<02:37, 39.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022052623193530713 0.3944501723686289 34313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [04:35<01:55, 38.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01778069653906585 0.5436984127998115 21108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [05:10<01:15, 37.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01321214429215937 0.7459536845734029 11024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [05:42<00:36, 36.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010585259132894742 0.9882790021219723 2852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [06:13<00:00, 37.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0036942191687488746 1.485124312146819 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "votes = np.sum(result_matrix, axis=-1)\n",
    "labels = np.array(votes >= 3).astype(np.int)\n",
    "result_embed_np = np.array(result_embed)\n",
    "print(labels.sum(), result_embed_np.shape, labels.shape)\n",
    "logme_list = []\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    # f has shape of [N, D], y has shape [N]\n",
    "    logme = LogME(regression=False)\n",
    "    \n",
    "    score = logme.fit(result_embed_np, labels)\n",
    "    new_labels = logme.predict(result_embed_np)\n",
    "    diff = (new_labels != np.array(labels)).sum()\n",
    "    print(diff / new_labels.shape[0], score, new_labels.sum())\n",
    "    labels = new_labels\n",
    "    logme_list.append(logme)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "93601a9c-267b-4fa2-adfd-9347e0ba89ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:29<04:23, 29.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12099683025467264 -0.3656026754108087 4645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:54<03:43, 27.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017925456334025577 -0.10830362679650674 4599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:30<03:32, 30.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009618537545086894 -0.09016681011082492 4579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:05<03:11, 31.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005027871898568149 -0.08228058457368106 4567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:30<02:28, 29.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0039348562684446385 -0.07921580432518732 4553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [02:57<01:55, 28.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0021860312602470216 -0.07698214627534314 4539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [03:24<01:24, 28.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0025139359492840747 -0.07580077803363777 4526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [03:59<01:00, 30.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0019674281342223193 -0.07438947745710067 4518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [04:35<00:32, 32.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018581265712099683 -0.07359154878531099 4511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:10<00:00, 31.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001530221882172915 -0.07276646570251749 4505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "senti_input = []\n",
    "senti_label = []\n",
    "with open('/data/albert.xht/sentiment/senti_copr.json.embed') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        senti_input.append(content['feat'])\n",
    "        senti_label.append(int(content['label']))\n",
    "\n",
    "logme_senti_list = []\n",
    "\n",
    "senti_feat_np = np.array(senti_input)\n",
    "senti_label_np = np.array(senti_label)\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    # f has shape of [N, D], y has shape [N]\n",
    "    logme = LogME(regression=False)\n",
    "    \n",
    "    score = logme.fit(senti_feat_np, senti_label_np)\n",
    "    new_labels = logme.predict(senti_feat_np)\n",
    "    diff = (new_labels != np.array(senti_label_np)).sum()\n",
    "    print(diff / new_labels.shape[0], score, new_labels.sum())\n",
    "    senti_label_np = new_labels\n",
    "    logme_senti_list.append(logme)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2e1a73f8-21c1-4831-94de-401506cf3d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.833041958041958"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_dev = []\n",
    "senti_dev_label = []\n",
    "with open('/data/albert.xht/sentiment/senti_copr_dev.json.embed') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        senti_dev.append(content['feat'])\n",
    "        senti_dev_label.append(int(content['label']))\n",
    "\n",
    "dev_predict_label = logme_senti_list[-1].predict(np.array(senti_dev))\n",
    "(dev_predict_label == np.array(senti_dev_label)).sum()/dev_predict_label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0208282f-77fa-4137-bb97-ed8e705fd41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_dev_label[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "eb63de72-85d0-4825-9d2e-6a821150cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = logme_list[1].predict_prob(result_embed_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "976d5350-1086-46e3-8e36-c2d200b75323",
   "metadata": {},
   "outputs": [],
   "source": [
    "black = []\n",
    "white = []\n",
    "for text, label, resp in zip(result_text, labels, result_item):\n",
    "    if label[1] > 0.6:\n",
    "        black.append((label, text, resp))\n",
    "    elif label[1] < 0.4 and resp['score_list']['query_risk'][0][-1] > 0.5 and resp['score_list']['query_risk'][0][-1] < 0.7:\n",
    "        white.append((label, text, resp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "30d010e7-3f43-4a24-b2b6-9369c1e1133e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([0.61923084, 0.38076916]),\n",
       "  '啤酒加海鲜是导致痛风的真凶吗？',\n",
       "  {'text': '啤酒加海鲜是导致痛风的真凶吗？',\n",
       "   'topic': ['健康'],\n",
       "   'score_list': {'senti_query': [['负向', 0.9878260493278503],\n",
       "     ['中性', 0.011836989782750607],\n",
       "     ['正向', 0.00033692491706460714]],\n",
       "    'senti': [['负向', 0.9924633502960205], ['正向', 0.007536637131124735]],\n",
       "    'bias': [['偏见', 0.03395457565784454], ['正常', 0.9660454392433167]],\n",
       "    'ciron': [['讽刺', 0.049804456532001495], ['正常', 0.9501956105232239]],\n",
       "    'intent': [['主观评价/比较/判断', 0.00010593750630505383],\n",
       "     ['寻求建议/帮助', 0.9996864795684814],\n",
       "     ['其它', 0.00020762969506904483]],\n",
       "    'offensive': [['冒犯', 0.12625817954540253], ['正常', 0.8737418055534363]],\n",
       "    'query_risk': [['风险', 0.5278050899505615],\n",
       "     ['个人信息', 4.180899486527778e-05],\n",
       "     ['正常', 0.4721531271934509]],\n",
       "    'teenager': [['不良', 0.03140368312597275], ['正常', 0.968596339225769]]}}),\n",
       " (array([0.64696887, 0.35303113]),\n",
       "  '如果咨询师忽然去世，这场「心理咨询」还有哪些后续工作需要处理？',\n",
       "  {'text': '如果咨询师忽然去世，这场「心理咨询」还有哪些后续工作需要处理？',\n",
       "   'topic': ['心理健康'],\n",
       "   'score_list': {'senti_query': [['负向', 0.9373202919960022],\n",
       "     ['中性', 0.06089513748884201],\n",
       "     ['正向', 0.0017845873953774571]],\n",
       "    'senti': [['负向', 0.9972254037857056], ['正向', 0.0027746560517698526]],\n",
       "    'bias': [['偏见', 0.045331671833992004], ['正常', 0.954668402671814]],\n",
       "    'ciron': [['讽刺', 0.07602763175964355], ['正常', 0.9239723086357117]],\n",
       "    'intent': [['主观评价/比较/判断', 0.0002537461987230927],\n",
       "     ['寻求建议/帮助', 0.9996602535247803],\n",
       "     ['其它', 8.605149923823774e-05]],\n",
       "    'offensive': [['冒犯', 0.11627919971942902], ['正常', 0.8837207555770874]],\n",
       "    'query_risk': [['风险', 0.6463529467582703],\n",
       "     ['个人信息', 8.582128066336736e-05],\n",
       "     ['正常', 0.3535611629486084]],\n",
       "    'teenager': [['不良', 0.16516564786434174], ['正常', 0.8348343968391418]]}}),\n",
       " (array([0.66734463, 0.33265537]),\n",
       "  '国内考研真的提高了你的交际圈质量吗？',\n",
       "  {'text': '国内考研真的提高了你的交际圈质量吗？',\n",
       "   'topic': ['教育/科学'],\n",
       "   'score_list': {'senti_query': [['负向', 0.4594200551509857],\n",
       "     ['中性', 0.18262261152267456],\n",
       "     ['正向', 0.3579573929309845]],\n",
       "    'senti': [['负向', 0.9194808006286621], ['正向', 0.08051921427249908]],\n",
       "    'bias': [['偏见', 0.07277432084083557], ['正常', 0.9272257089614868]],\n",
       "    'ciron': [['讽刺', 0.21246130764484406], ['正常', 0.7875387072563171]],\n",
       "    'intent': [['主观评价/比较/判断', 0.3228326141834259],\n",
       "     ['寻求建议/帮助', 0.637159526348114],\n",
       "     ['其它', 0.04000789299607277]],\n",
       "    'offensive': [['冒犯', 0.19664603471755981], ['正常', 0.8033539056777954]],\n",
       "    'query_risk': [['风险', 0.67658931016922],\n",
       "     ['个人信息', 1.5749072190374136e-05],\n",
       "     ['正常', 0.3233950138092041]],\n",
       "    'teenager': [['不良', 0.15440469980239868], ['正常', 0.8455953001976013]]}}),\n",
       " (array([0.6325719, 0.3674281]),\n",
       "  '女性不结婚会如何？',\n",
       "  {'text': '女性不结婚会如何？',\n",
       "   'topic': ['情感'],\n",
       "   'score_list': {'senti_query': [['负向', 0.9815443754196167],\n",
       "     ['中性', 0.017749596387147903],\n",
       "     ['正向', 0.0007060715579427779]],\n",
       "    'senti': [['负向', 0.9940313696861267], ['正向', 0.005968623328953981]],\n",
       "    'bias': [['偏见', 0.32024085521698], ['正常', 0.67975914478302]],\n",
       "    'ciron': [['讽刺', 0.08223456144332886], ['正常', 0.9177653789520264]],\n",
       "    'intent': [['主观评价/比较/判断', 0.00010111771553056315],\n",
       "     ['寻求建议/帮助', 0.970717191696167],\n",
       "     ['其它', 0.029181696474552155]],\n",
       "    'offensive': [['冒犯', 0.4908236861228943], ['正常', 0.5091763138771057]],\n",
       "    'query_risk': [['风险', 0.6661329865455627],\n",
       "     ['个人信息', 3.404883318580687e-05],\n",
       "     ['正常', 0.33383291959762573]],\n",
       "    'teenager': [['不良', 0.048615776002407074], ['正常', 0.9513842463493347]]}}),\n",
       " (array([0.71310941, 0.28689059]),\n",
       "  '法国的预科是法国高等教育的精华吗?',\n",
       "  {'text': '法国的预科是法国高等教育的精华吗?',\n",
       "   'topic': ['国家'],\n",
       "   'score_list': {'senti_query': [['负向', 0.3576063811779022],\n",
       "     ['中性', 0.21414519846439362],\n",
       "     ['正向', 0.42824843525886536]],\n",
       "    'senti': [['负向', 0.977792501449585], ['正向', 0.022207461297512054]],\n",
       "    'bias': [['偏见', 0.06907344609498978], ['正常', 0.9309265613555908]],\n",
       "    'ciron': [['讽刺', 0.32762855291366577], ['正常', 0.6723713874816895]],\n",
       "    'intent': [['主观评价/比较/判断', 0.0012943544425070286],\n",
       "     ['寻求建议/帮助', 0.39717406034469604],\n",
       "     ['其它', 0.6015316843986511]],\n",
       "    'offensive': [['冒犯', 0.135072261095047], ['正常', 0.8649277091026306]],\n",
       "    'query_risk': [['风险', 0.5316483974456787],\n",
       "     ['个人信息', 4.7619938413845375e-05],\n",
       "     ['正常', 0.46830400824546814]],\n",
       "    'teenager': [['不良', 0.19810397922992706], ['正常', 0.8018960356712341]]}}),\n",
       " (array([0.63195356, 0.36804644]),\n",
       "  '你曾经因为什么，在大庭广众下失声痛哭？',\n",
       "  {'text': '你曾经因为什么，在大庭广众下失声痛哭？',\n",
       "   'topic': ['价值观'],\n",
       "   'score_list': {'senti_query': [['负向', 0.4601271152496338],\n",
       "     ['中性', 0.36997082829475403],\n",
       "     ['正向', 0.16990207135677338]],\n",
       "    'senti': [['负向', 0.9893766641616821], ['正向', 0.010623393580317497]],\n",
       "    'bias': [['偏见', 0.059167444705963135], ['正常', 0.9408325552940369]],\n",
       "    'ciron': [['讽刺', 0.08436904847621918], ['正常', 0.9156309366226196]],\n",
       "    'intent': [['主观评价/比较/判断', 2.101737482007593e-05],\n",
       "     ['寻求建议/帮助', 1.3674572073796298e-05],\n",
       "     ['其它', 0.9999653100967407]],\n",
       "    'offensive': [['冒犯', 0.17178937792778015], ['正常', 0.8282105922698975]],\n",
       "    'query_risk': [['风险', 0.5101386308670044],\n",
       "     ['个人信息', 0.00038751415559090674],\n",
       "     ['正常', 0.4894738793373108]],\n",
       "    'teenager': [['不良', 0.04082803800702095], ['正常', 0.9591719508171082]]}}),\n",
       " (array([0.61450448, 0.38549552]),\n",
       "  '如何评价清华大学计算机系禁止大一学生在寝室使用计算机？',\n",
       "  {'text': '如何评价清华大学计算机系禁止大一学生在寝室使用计算机？',\n",
       "   'topic': ['教育/科学'],\n",
       "   'score_list': {'senti_query': [['负向', 0.6275720000267029],\n",
       "     ['中性', 0.3593601882457733],\n",
       "     ['正向', 0.013067794032394886]],\n",
       "    'senti': [['负向', 0.9539933800697327], ['正向', 0.04600662738084793]],\n",
       "    'bias': [['偏见', 0.07097440958023071], ['正常', 0.9290255904197693]],\n",
       "    'ciron': [['讽刺', 0.20469091832637787], ['正常', 0.7953090667724609]],\n",
       "    'intent': [['主观评价/比较/判断', 0.9999850988388062],\n",
       "     ['寻求建议/帮助', 1.3839493476552889e-05],\n",
       "     ['其它', 1.0545921895754873e-06]],\n",
       "    'offensive': [['冒犯', 0.4043920636177063], ['正常', 0.5956079363822937]],\n",
       "    'query_risk': [['风险', 0.6921202540397644],\n",
       "     ['个人信息', 0.00015471893129870296],\n",
       "     ['正常', 0.30772507190704346]],\n",
       "    'teenager': [['不良', 0.0741436630487442], ['正常', 0.9258562922477722]]}}),\n",
       " (array([0.69577929, 0.30422071]),\n",
       "  '为什么感觉有时候玩战争策略游戏有点喜欢堆兵而轻视计谋使用？',\n",
       "  {'text': '为什么感觉有时候玩战争策略游戏有点喜欢堆兵而轻视计谋使用？',\n",
       "   'topic': ['游戏'],\n",
       "   'score_list': {'senti_query': [['负向', 0.04474528506398201],\n",
       "     ['中性', 0.3235502243041992],\n",
       "     ['正向', 0.631704568862915]],\n",
       "    'senti': [['负向', 0.7794912457466125], ['正向', 0.22050878405570984]],\n",
       "    'bias': [['偏见', 0.08287464827299118], ['正常', 0.9171253442764282]],\n",
       "    'ciron': [['讽刺', 0.029225429520010948], ['正常', 0.9707745909690857]],\n",
       "    'intent': [['主观评价/比较/判断', 0.9830152988433838],\n",
       "     ['寻求建议/帮助', 0.009508620947599411],\n",
       "     ['其它', 0.00747608533129096]],\n",
       "    'offensive': [['冒犯', 0.03683144971728325], ['正常', 0.9631685614585876]],\n",
       "    'query_risk': [['风险', 0.6133604049682617],\n",
       "     ['个人信息', 4.630841067410074e-05],\n",
       "     ['正常', 0.3865932822227478]],\n",
       "    'teenager': [['不良', 0.11823948472738266], ['正常', 0.8817605972290039]]}}),\n",
       " (array([0.63745584, 0.36254416]),\n",
       "  '你为了泡汉子，都get过什么技能？',\n",
       "  {'text': '你为了泡汉子，都get过什么技能？',\n",
       "   'topic': ['两性'],\n",
       "   'score_list': {'senti_query': [['负向', 0.11766250431537628],\n",
       "     ['中性', 0.23166902363300323],\n",
       "     ['正向', 0.6506685018539429]],\n",
       "    'senti': [['负向', 0.9720450639724731], ['正向', 0.027954943478107452]],\n",
       "    'bias': [['偏见', 0.16484440863132477], ['正常', 0.8351556062698364]],\n",
       "    'ciron': [['讽刺', 0.18635571002960205], ['正常', 0.8136442303657532]],\n",
       "    'intent': [['主观评价/比较/判断', 6.625850801356137e-05],\n",
       "     ['寻求建议/帮助', 2.557659536250867e-05],\n",
       "     ['其它', 0.9999080896377563]],\n",
       "    'offensive': [['冒犯', 0.35824501514434814], ['正常', 0.6417549848556519]],\n",
       "    'query_risk': [['风险', 0.6831875443458557],\n",
       "     ['个人信息', 0.00024645571829751134],\n",
       "     ['正常', 0.31656596064567566]],\n",
       "    'teenager': [['不良', 0.2195085883140564], ['正常', 0.7804914116859436]]}}),\n",
       " (array([0.61018007, 0.38981993]),\n",
       "  '你在中国的火车上有什么奇特的经历？',\n",
       "  {'text': '你在中国的火车上有什么奇特的经历？',\n",
       "   'topic': ['旅游'],\n",
       "   'score_list': {'senti_query': [['负向', 0.028941135853528976],\n",
       "     ['中性', 0.22392360866069794],\n",
       "     ['正向', 0.7471352815628052]],\n",
       "    'senti': [['负向', 0.9198192358016968], ['正向', 0.08018076419830322]],\n",
       "    'bias': [['偏见', 0.059860482811927795], ['正常', 0.9401394724845886]],\n",
       "    'ciron': [['讽刺', 0.04698234423995018], ['正常', 0.9530176520347595]],\n",
       "    'intent': [['主观评价/比较/判断', 0.000435373920481652],\n",
       "     ['寻求建议/帮助', 6.214597669895738e-05],\n",
       "     ['其它', 0.999502420425415]],\n",
       "    'offensive': [['冒犯', 0.12099259346723557], ['正常', 0.8790073394775391]],\n",
       "    'query_risk': [['风险', 0.5675622224807739],\n",
       "     ['个人信息', 0.00020123407011851668],\n",
       "     ['正常', 0.43223655223846436]],\n",
       "    'teenager': [['不良', 0.03285519406199455], ['正常', 0.9671448469161987]]}})]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6a9314f-1a05-4e7e-ae42-413c6ceb668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "black = []\n",
    "with open('/data/albert.xht/xiaodao/query_risk_v10/biake_qa_web_text_zh_train.json.offensive.all.black.v10.1', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/xiaodao/query_risk_v10/biake_qa_web_text_zh_train.json.offensive.all.v10.1') as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            if content['score_list']['offensive'][0][1] >= 0.8:\n",
    "                black.append(content)\n",
    "                fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a3f5c1-45dd-438c-a13e-8d767d2c6bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '为什么女司机被打那么多人同情「弱者」而选择性忽视「弱者」的过错可能导致的严重后果？',\n",
       "  'topic': ['风险'],\n",
       "  'score_list': {'senti_query': [['负向', 0.9979752898216248],\n",
       "    ['中性', 0.0019358349964022636],\n",
       "    ['正向', 8.883338887244463e-05]],\n",
       "   'senti': [['负向', 0.9958634376525879], ['正向', 0.004136575851589441]],\n",
       "   'bias': [['偏见', 0.27606531977653503], ['正常', 0.7239347100257874]],\n",
       "   'ciron': [['讽刺', 0.30140814185142517], ['正常', 0.6985918879508972]],\n",
       "   'intent': [['主观评价/比较/判断', 0.9471869468688965],\n",
       "    ['寻求建议/帮助', 0.05241207033395767],\n",
       "    ['其它', 0.0004009997355751693]],\n",
       "   'offensive': [['冒犯', 0.8063883185386658], ['正常', 0.19361165165901184]],\n",
       "   'query_risk': [['风险', 0.9768148064613342],\n",
       "    ['个人信息', 2.531747850298416e-05],\n",
       "    ['正常', 0.023159895092248917]],\n",
       "   'teenager': [['不良', 0.08330792188644409], ['正常', 0.9166920781135559]]}},\n",
       " {'text': '哪些行为会使男性显得俗不可耐？',\n",
       "  'topic': ['风险'],\n",
       "  'score_list': {'senti_query': [['负向', 0.9975232481956482],\n",
       "    ['中性', 0.0022904882207512856],\n",
       "    ['正向', 0.00018620019545778632]],\n",
       "   'senti': [['负向', 0.9974743723869324], ['正向', 0.0025255868677049875]],\n",
       "   'bias': [['偏见', 0.8707038164138794], ['正常', 0.12929613888263702]],\n",
       "   'ciron': [['讽刺', 0.219936802983284], ['正常', 0.7800632119178772]],\n",
       "   'intent': [['主观评价/比较/判断', 0.018358610570430756],\n",
       "    ['寻求建议/帮助', 0.9804294109344482],\n",
       "    ['其它', 0.0012119427556172013]],\n",
       "   'offensive': [['冒犯', 0.8814972639083862], ['正常', 0.11850273609161377]],\n",
       "   'query_risk': [['风险', 0.9824022650718689],\n",
       "    ['个人信息', 4.8542351578362286e-05],\n",
       "    ['正常', 0.017549242824316025]],\n",
       "   'teenager': [['不良', 0.8269442319869995], ['正常', 0.17305579781532288]]}},\n",
       " {'text': '怎样看连云港狗咬人、人杀狗被拘留一事？',\n",
       "  'topic': ['风险'],\n",
       "  'score_list': {'senti_query': [['负向', 0.991036057472229],\n",
       "    ['中性', 0.008514036424458027],\n",
       "    ['正向', 0.0004498483904171735]],\n",
       "   'senti': [['负向', 0.9993146657943726], ['正向', 0.0006853652885183692]],\n",
       "   'bias': [['偏见', 0.20078693330287933], ['正常', 0.7992130517959595]],\n",
       "   'ciron': [['讽刺', 0.16154398024082184], ['正常', 0.8384559750556946]],\n",
       "   'intent': [['主观评价/比较/判断', 0.9930296540260315],\n",
       "    ['寻求建议/帮助', 0.006871685851365328],\n",
       "    ['其它', 9.876121475826949e-05]],\n",
       "   'offensive': [['冒犯', 0.9825801253318787], ['正常', 0.01741984486579895]],\n",
       "   'query_risk': [['风险', 0.9983269572257996],\n",
       "    ['个人信息', 2.5617790015530773e-05],\n",
       "    ['正常', 0.0016474350122734904]],\n",
       "   'teenager': [['不良', 0.46525436639785767], ['正常', 0.5347456336021423]]}},\n",
       " {'text': '如何评论河南闹洞房伴娘被脱衣猥亵事件？',\n",
       "  'topic': ['风险'],\n",
       "  'score_list': {'senti_query': [['负向', 0.9875598549842834],\n",
       "    ['中性', 0.012051495723426342],\n",
       "    ['正向', 0.0003886313643306494]],\n",
       "   'senti': [['负向', 0.997477114200592], ['正向', 0.0025228769518435]],\n",
       "   'bias': [['偏见', 0.3311207592487335], ['正常', 0.6688792705535889]],\n",
       "   'ciron': [['讽刺', 0.21121346950531006], ['正常', 0.7887865900993347]],\n",
       "   'intent': [['主观评价/比较/判断', 0.9978170394897461],\n",
       "    ['寻求建议/帮助', 0.0021299056243151426],\n",
       "    ['其它', 5.3087336709722877e-05]],\n",
       "   'offensive': [['冒犯', 0.9740647077560425], ['正常', 0.02593526989221573]],\n",
       "   'query_risk': [['风险', 0.9975032210350037],\n",
       "    ['个人信息', 2.617199243104551e-05],\n",
       "    ['正常', 0.0024705869145691395]],\n",
       "   'teenager': [['不良', 0.3155621588230133], ['正常', 0.6844378709793091]]}},\n",
       " {'text': '男性对强奸案的态度是什么样的？',\n",
       "  'topic': ['风险'],\n",
       "  'score_list': {'senti_query': [['负向', 0.9950546026229858],\n",
       "    ['中性', 0.004795222543179989],\n",
       "    ['正向', 0.00015020862338133156]],\n",
       "   'senti': [['负向', 0.998477041721344], ['正向', 0.0015229376731440425]],\n",
       "   'bias': [['偏见', 0.11687721312046051], ['正常', 0.8831227421760559]],\n",
       "   'ciron': [['讽刺', 0.2114790827035904], ['正常', 0.7885209321975708]],\n",
       "   'intent': [['主观评价/比较/判断', 0.06392207741737366],\n",
       "    ['寻求建议/帮助', 0.8510618209838867],\n",
       "    ['其它', 0.08501619100570679]],\n",
       "   'offensive': [['冒犯', 0.9672746062278748], ['正常', 0.03272535651922226]],\n",
       "   'query_risk': [['风险', 0.9975797533988953],\n",
       "    ['个人信息', 2.7936312108067796e-05],\n",
       "    ['正常', 0.002392278518527746]],\n",
       "   'teenager': [['不良', 0.17853806912899017], ['正常', 0.8214619159698486]]}},\n",
       " {'text': '如何看待网传的：湘潭石潭城管队队长杀人事件？',\n",
       "  'topic': ['风险'],\n",
       "  'score_list': {'senti_query': [['负向', 0.9932072758674622],\n",
       "    ['中性', 0.006568058393895626],\n",
       "    ['正向', 0.0002246577787445858]],\n",
       "   'senti': [['负向', 0.9977065324783325], ['正向', 0.0022934165317565203]],\n",
       "   'bias': [['偏见', 0.04436592757701874], ['正常', 0.9556341171264648]],\n",
       "   'ciron': [['讽刺', 0.2525603473186493], ['正常', 0.7474396824836731]],\n",
       "   'intent': [['主观评价/比较/判断', 0.9998131394386292],\n",
       "    ['寻求建议/帮助', 9.542438783682883e-05],\n",
       "    ['其它', 9.136642620433122e-05]],\n",
       "   'offensive': [['冒犯', 0.8978687524795532], ['正常', 0.10213128477334976]],\n",
       "   'query_risk': [['风险', 0.9976245760917664],\n",
       "    ['个人信息', 0.00011122450086986646],\n",
       "    ['正常', 0.0022641844116151333]],\n",
       "   'teenager': [['不良', 0.16403351724147797], ['正常', 0.8359665274620056]]}},\n",
       " {'text': '如何评价大河网发表文章——《约架斗殴莫以爱国的名义》？',\n",
       "  'topic': ['风险'],\n",
       "  'score_list': {'senti_query': [['负向', 0.3146556317806244],\n",
       "    ['中性', 0.6183700561523438],\n",
       "    ['正向', 0.06697431951761246]],\n",
       "   'senti': [['负向', 0.9572159647941589], ['正向', 0.04278409853577614]],\n",
       "   'bias': [['偏见', 0.13705991208553314], ['正常', 0.8629401326179504]],\n",
       "   'ciron': [['讽刺', 0.20786796510219574], ['正常', 0.7921320796012878]],\n",
       "   'intent': [['主观评价/比较/判断', 0.9999853372573853],\n",
       "    ['寻求建议/帮助', 9.580820005794521e-06],\n",
       "    ['其它', 5.103697276354069e-06]],\n",
       "   'offensive': [['冒犯', 0.8201045989990234], ['正常', 0.17989543080329895]],\n",
       "   'query_risk': [['风险', 0.9887955188751221],\n",
       "    ['个人信息', 4.738517236546613e-05],\n",
       "    ['正常', 0.011157086119055748]],\n",
       "   'teenager': [['不良', 0.2021346092224121], ['正常', 0.7978653311729431]]}},\n",
       " {'text': '你遇到最渣的渣男是怎么样的？有什么渣到极点令人恨到颤栗的事情？',\n",
       "  'topic': ['风险'],\n",
       "  'score_list': {'senti_query': [['负向', 0.9977003931999207],\n",
       "    ['中性', 0.0021122705657035112],\n",
       "    ['正向', 0.0001873784203780815]],\n",
       "   'senti': [['负向', 0.9991440773010254], ['正向', 0.0008559414418414235]],\n",
       "   'bias': [['偏见', 0.6941642165184021], ['正常', 0.3058358132839203]],\n",
       "   'ciron': [['讽刺', 0.3583545684814453], ['正常', 0.6416454315185547]],\n",
       "   'intent': [['主观评价/比较/判断', 0.6408735513687134],\n",
       "    ['寻求建议/帮助', 0.2120320200920105],\n",
       "    ['其它', 0.14709441363811493]],\n",
       "   'offensive': [['冒犯', 0.9684791564941406], ['正常', 0.03152081370353699]],\n",
       "   'query_risk': [['风险', 0.9952327609062195],\n",
       "    ['个人信息', 0.00012987189984414726],\n",
       "    ['正常', 0.004637328442186117]],\n",
       "   'teenager': [['不良', 0.9443151950836182], ['正常', 0.05568479374051094]]}},\n",
       " {'text': '如何看待薛之谦转发一名男性未成年人（17岁）辱骂女子的视频并发起网络暴力？',\n",
       "  'topic': ['风险'],\n",
       "  'score_list': {'senti_query': [['负向', 0.995554506778717],\n",
       "    ['中性', 0.0042522866278886795],\n",
       "    ['正向', 0.00019322695152368397]],\n",
       "   'senti': [['负向', 0.9982408285140991], ['正向', 0.0017592123476788402]],\n",
       "   'bias': [['偏见', 0.08131174743175507], ['正常', 0.9186882972717285]],\n",
       "   'ciron': [['讽刺', 0.20026414096355438], ['正常', 0.7997359037399292]],\n",
       "   'intent': [['主观评价/比较/判断', 0.9996575117111206],\n",
       "    ['寻求建议/帮助', 0.00027236161986365914],\n",
       "    ['其它', 7.002367055974901e-05]],\n",
       "   'offensive': [['冒犯', 0.9212072491645813], ['正常', 0.07879271358251572]],\n",
       "   'query_risk': [['风险', 0.9949093461036682],\n",
       "    ['个人信息', 4.9422375013818964e-05],\n",
       "    ['正常', 0.005041252821683884]],\n",
       "   'teenager': [['不良', 0.17887023091316223], ['正常', 0.8211297988891602]]}},\n",
       " {'text': '如果被ISIS绑架，如何依靠自己的专业知识保命？',\n",
       "  'topic': ['风险'],\n",
       "  'score_list': {'senti_query': [['负向', 0.889133870601654],\n",
       "    ['中性', 0.10307762771844864],\n",
       "    ['正向', 0.007788478396832943]],\n",
       "   'senti': [['负向', 0.9941955208778381], ['正向', 0.0058044851757586]],\n",
       "   'bias': [['偏见', 0.14273317158222198], ['正常', 0.8572668433189392]],\n",
       "   'ciron': [['讽刺', 0.2787017524242401], ['正常', 0.7212982773780823]],\n",
       "   'intent': [['主观评价/比较/判断', 0.0366419218480587],\n",
       "    ['寻求建议/帮助', 0.9559316635131836],\n",
       "    ['其它', 0.0074264295399188995]],\n",
       "   'offensive': [['冒犯', 0.962915301322937], ['正常', 0.037084728479385376]],\n",
       "   'query_risk': [['风险', 0.9992401599884033],\n",
       "    ['个人信息', 7.705961616011336e-05],\n",
       "    ['正常', 0.0006827589240856469]],\n",
       "   'teenager': [['不良', 0.4443177580833435], ['正常', 0.5556822419166565]]}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71c9c7f7-af38-4016-b06c-05c655cf982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/logme_black.json', 'w') as fwobj:\n",
    "    for l in black:\n",
    "        content = {\n",
    "            'text':l[0],\n",
    "            'label':['风险']\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b4c471d-3909-4371-b922-29da8a911c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/embed_linear_small_white.json', 'w') as fwobj:\n",
    "    for l in white:\n",
    "        content = {\n",
    "            'text':l[-1],\n",
    "            'label':['正常']\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad8d8808-8d28-42c9-a481-39dbd85bc8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e98ca552-3bf0-4224-ae0f-ea6b238994e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "772017"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e8a005a-83d6-42c3-a65a-fed4dfb9c743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce42c4-44d5-404e-97c1-90c8c3e3dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def EmbedSimple(data, result_matrix):\n",
    "    \n",
    "    model = EmbedLinear(hidden_size=128, dropout_prob=0.1, num_labels=2)\n",
    "    model.compile(method=\"multiclass\", optimizer=\"Adam\", learning_rate=0.001, momentum=0.0)\n",
    "    \n",
    "    votes = np.sum(result_matrix, axis=-1)\n",
    "    labels = np.array(votes >= 3).astype(np.int)\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print('===start====', i)\n",
    "        result = model.fit(data, labels)\n",
    "        probs = model.predict_proba(result_matrix)\n",
    "        print(probs.shape)\n",
    "        labels = np.argmax(probs, axis=-1)\n",
    "    # return probs, model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6be52d5-9067-4df7-a0a2-4358d1cab388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = EmbedLinear(hidden_size=128, dropout_prob=0.1, num_labels=2)\n",
    "model.compile(method=\"multiclass\", optimizer=\"Adam\", learning_rate=0.001, momentum=0.0)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def SIMPLE(result_matrix):\n",
    "    votes = np.sum(result_matrix, axis=-1)\n",
    "    labels = np.array(votes >= 3).astype(np.int)\n",
    "    clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print('===start====', i)\n",
    "        result = clf.fit(result_matrix, labels)\n",
    "        probs = clf.predict_proba(result_matrix)\n",
    "        print(probs.shape)\n",
    "        labels = np.argmax(probs, axis=-1)\n",
    "    return probs, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1650d06-b254-4ea0-bea4-89b258205342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162260"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result_matrix = np.array(result_matrix)\n",
    "votes = np.sum(result_matrix, axis=-1)\n",
    "(votes>=3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "576fbd71-0d3a-4487-a7cb-b4b1113267d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===start==== 0\n",
      "(772017, 2)\n",
      "===start==== 1\n",
      "(772017, 2)\n",
      "===start==== 2\n",
      "(772017, 2)\n",
      "===start==== 3\n",
      "(772017, 2)\n",
      "===start==== 4\n",
      "(772017, 2)\n"
     ]
    }
   ],
   "source": [
    "probs, clf = SIMPLE(result_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d4c371c-352c-4ad5-9b1a-b249dbdc9d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1279460, 8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d878c09-4f02-47f2-aa4e-037b76346e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
