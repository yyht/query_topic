{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9cd2ffa-e112-4ef1-a489-398226c2ba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "import json\n",
    "\n",
    "with open('/data/albert.xht/xiaoda/sentiment/jushi/jushi_train.json', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/xiaoda/sentiment/jushi/Merge.txt') as frobj:\n",
    "        for line in frobj:\n",
    "            content = line.strip()\n",
    "            label = content.split(':')[0]\n",
    "            if re.search('\\(\\d+\\)', label):\n",
    "                label_ = '是非问'\n",
    "            if re.search('\\[\\d+\\]', label):\n",
    "                label_ = '特指问'\n",
    "            if re.search('<\\d+>', label):\n",
    "                label_ = '选择问'\n",
    "            if re.search('\\{\\d+\\}', label):\n",
    "                label_ = '正反问'\n",
    "            sent = \"\".join(content.split(':')[1:])\n",
    "            word_list = sent.split()\n",
    "            sent_ = ''\n",
    "            for word in word_list:\n",
    "                sent_ += word.split('/')[0]\n",
    "            d = {\n",
    "                'text':sent_,\n",
    "                'label':[label_]\n",
    "            }\n",
    "            fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "344574d0-2b79-45ca-ab28-8a31d46ad710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 5), match='<186>'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = '<186>'\n",
    "import re\n",
    "re.search('<\\d+>', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07a459ab-87a5-428d-a5de-9acd097a76e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/xiaoda/sentiment/jushi/jushi_label.txt', 'w') as fwobj:\n",
    "    for l in ['是非问', '特指问', '选择问', '正反问']:\n",
    "        fwobj.write(l+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8748ac87-d0ad-4d6e-b104-9f6fd967a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys,os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fed6e9-6348-4dc4-88e9-bef473267136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.extend(['/root/xiaoda/query_topic/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef2ed3b-dff8-42af-8aaf-0cef75db1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/ondrejbohdal/meta-calibration/blob/main/Metrics/metrics.py\n",
    "\"\"\"\n",
    "\n",
    "class ECE(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(ECE, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, logits, labels, mode='logits'):\n",
    "        if mode == 'logits':\n",
    "            softmaxes = F.softmax(logits, dim=1)\n",
    "        else:\n",
    "            softmaxes = logits\n",
    "        # softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "        \n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ba662fd-4413-448f-9320-3ba1151fd150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast\n",
    "import transformers\n",
    "from datetime import timedelta\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from nets.them_classifier import MyBaseModel, RobertaClassifier\n",
    "\n",
    "import configparser\n",
    "from tqdm import tqdm\n",
    "\n",
    "cur_dir_path = '/root/xiaoda/query_topic/'\n",
    "\n",
    "def load_label(filepath):\n",
    "    label_list = []\n",
    "    with open(filepath, 'r') as frobj:\n",
    "        for line in frobj:\n",
    "            label_list.append(line.strip())\n",
    "        n_classes = len(label_list)\n",
    "\n",
    "        label2id = {}\n",
    "        id2label = {}\n",
    "        for idx, label in enumerate(label_list):\n",
    "            label2id[label] = idx\n",
    "            id2label[idx] = label\n",
    "        return label2id, id2label\n",
    "\n",
    "class RiskInfer(object):\n",
    "    def __init__(self, config_path):\n",
    "\n",
    "        import torch, os, sys\n",
    "\n",
    "        con = configparser.ConfigParser()\n",
    "        con_path = os.path.join(cur_dir_path, config_path)\n",
    "        con.read(con_path, encoding='utf8')\n",
    "\n",
    "        args_path = dict(dict(con.items('paths')), **dict(con.items(\"para\")))\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(args_path[\"model_path\"], do_lower_case=True)\n",
    "\n",
    "        from collections import OrderedDict\n",
    "        self.schema_dict = OrderedDict({})\n",
    "\n",
    "        for label_index, schema_info in enumerate(args_path[\"label_path\"].split(',')):\n",
    "            schema_type, schema_path = schema_info.split(':')\n",
    "            schema_path = os.path.join(cur_dir_path, schema_path)\n",
    "            print(schema_type, schema_path, '===schema-path===')\n",
    "            label2id, id2label = load_label(schema_path)\n",
    "            self.schema_dict[schema_type] = {\n",
    "                'label2id':label2id,\n",
    "                'id2label':id2label,\n",
    "                'label_index':label_index\n",
    "            }\n",
    "            print(self.schema_dict[schema_type], '==schema_type==', schema_type)\n",
    "        \n",
    "        output_path = os.path.join(cur_dir_path, args_path['output_path'])\n",
    "\n",
    "#         from roformer import RoFormerModel, RoFormerConfig\n",
    "\n",
    "#         config = RoFormerConfig.from_pretrained(args_path[\"model_path\"])\n",
    "#         encoder = RoFormerModel(config=config)\n",
    "        \n",
    "        from transformers import BertModel, BertConfig\n",
    "\n",
    "        config = BertConfig.from_pretrained(args_path[\"model_path\"])\n",
    "        encoder = BertModel(config=config)\n",
    "        \n",
    "        encoder_net = MyBaseModel(encoder, config)\n",
    "\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        classifier_list = []\n",
    "\n",
    "        schema_list = list(self.schema_dict.keys())\n",
    "\n",
    "        for schema_key in schema_list:\n",
    "            classifier = RobertaClassifier(\n",
    "                hidden_size=config.hidden_size, \n",
    "                dropout_prob=con.getfloat('para', 'out_dropout_rate'),\n",
    "                num_labels=len(self.schema_dict[schema_key]['label2id']), \n",
    "                dropout_type=con.get('para', 'dropout_type'))\n",
    "            classifier_list.append(classifier)\n",
    "\n",
    "        classifier_list = nn.ModuleList(classifier_list)\n",
    "\n",
    "        class MultitaskClassifier(nn.Module):\n",
    "            def __init__(self, transformer, classifier_list):\n",
    "                super().__init__()\n",
    "\n",
    "                self.transformer = transformer\n",
    "                self.classifier_list = classifier_list\n",
    "\n",
    "            def forward(self, input_ids, input_mask, \n",
    "                        segment_ids=None, \n",
    "                        transformer_mode='mean_pooling', \n",
    "                        dt_idx=None):\n",
    "                hidden_states = self.transformer(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              return_mode=transformer_mode)\n",
    "                outputs_list = []\n",
    "                \n",
    "                for idx, classifier in enumerate(self.classifier_list):\n",
    "                    \n",
    "                    if dt_idx is not None and idx != dt_idx:\n",
    "                        continue\n",
    "                    \n",
    "                    ce_logits = classifier(hidden_states)\n",
    "                    outputs_list.append(ce_logits)\n",
    "                return outputs_list, hidden_states\n",
    "\n",
    "        self.net = MultitaskClassifier(encoder_net, classifier_list).to(self.device)\n",
    "\n",
    "        # eo = 9\n",
    "        # ckpt = torch.load(os.path.join(output_path, 'multitask_cls.pth.{}.raw'.format(eo)), map_location=self.device)\n",
    "        # # ckpt = torch.load(os.path.join(output_path, 'multitask_cls.pth.{}.raw.focal'.format(eo)), map_location=self.device)\n",
    "        # # ckpt = torch.load(os.path.join(output_path, 'multitask_contrast_cls.pth.{}'.format(eo)), map_location=self.device)\n",
    "        # self.net.load_state_dict(ckpt)\n",
    "        # self.net.eval()\n",
    "        \n",
    "    def reload(self, model_path):\n",
    "        ckpt = torch.load(model_path, map_location=self.device)\n",
    "        self.net.load_state_dict(ckpt)\n",
    "        self.net.eval()\n",
    "\n",
    "    def predict(self, text):\n",
    "\n",
    "        \"\"\"抽取输入text所包含的类型\n",
    "        \"\"\"\n",
    "        encoder_txt = self.tokenizer.encode_plus(text, max_length=256)\n",
    "        input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().unsqueeze(0).to(self.device)\n",
    "        token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).unsqueeze(0).to(self.device)\n",
    "        attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        scores_dict = {}\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states] = self.net(input_ids, \n",
    "                attention_mask, token_type_ids, transformer_mode='cls')\n",
    "        for schema_type, logits in zip(list(self.schema_dict.keys()), logits_list):\n",
    "            scores = torch.nn.Softmax(dim=1)(logits)[0].data.cpu().numpy()\n",
    "            scores_dict[schema_type] = []\n",
    "            for index, score in enumerate(scores):\n",
    "                scores_dict[schema_type].append([self.schema_dict[schema_type]['id2label'][index], \n",
    "                                        float(score)])\n",
    "        return scores_dict\n",
    "    \n",
    "    def get_logitnorm(self, text):\n",
    "        \"\"\"抽取输入text所包含的类型\n",
    "        \"\"\"\n",
    "        encoder_txt = self.tokenizer.encode_plus(text, max_length=256)\n",
    "        input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().unsqueeze(0).to(self.device)\n",
    "        token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).unsqueeze(0).to(self.device)\n",
    "        attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        scores_dict = {}\n",
    "        logits_norm_list = []\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states] = self.net(input_ids, \n",
    "                attention_mask, token_type_ids, transformer_mode='cls')\n",
    "            for logits in logits_list:\n",
    "                logits_norm_list.append(logits/torch.norm(logits, p=2, dim=-1, keepdim=True) + 1e-7)\n",
    "        for schema_type, logit_norm in zip(list(self.schema_dict.keys()), logits_norm_list):\n",
    "            scores_dict[schema_type] = logit_norm[0].data.cpu().numpy()\n",
    "        return scores_dict\n",
    "            \n",
    "    \n",
    "    def predict_batch(self, text):\n",
    "        if isinstance(text, list):\n",
    "            text_list = text\n",
    "        else:\n",
    "            text_list = [text]\n",
    "        model_input = self.tokenizer(text_list, return_tensors=\"pt\",padding=True)\n",
    "        for key in model_input:\n",
    "            model_input[key] = model_input[key].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states] = self.net(model_input['input_ids'], \n",
    "                model_input['attention_mask'], \n",
    "                model_input['token_type_ids'], transformer_mode='cls')\n",
    "        score_dict_list = []\n",
    "        for idx, text in enumerate(text_list):\n",
    "            scores_dict = {}\n",
    "            for schema_type, logits in zip(list(self.schema_dict.keys()), logits_list):\n",
    "                scores = torch.nn.Softmax(dim=1)(logits)[idx].data.cpu().numpy()\n",
    "                scores_dict[schema_type] = []\n",
    "                for index, score in enumerate(scores):\n",
    "                    scores_dict[schema_type].append([self.schema_dict[schema_type]['id2label'][index], \n",
    "                                            float(score)])\n",
    "            score_dict_list.append(scores_dict)\n",
    "        return score_dict_list\n",
    "\n",
    "# risk_api = RiskInfer('./risk_data/config.ini')\n",
    "# risk_api = RiskInfer('./risk_data_v5/config_jushi_intent.ini')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e29669f0-e22c-48a3-a89f-82d29751aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_filter_jushi_intent/multitask_cls.pth.9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "613e5811-f0f1-4cc1-9405-c6119c66adb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jushi': [['是非问', 0.005088354926556349],\n",
       "  ['特指问', 0.9275872111320496],\n",
       "  ['选择问', 0.030795954167842865],\n",
       "  ['正反问', 0.036528490483760834]],\n",
       " 'intent': [['主观评价/比较/判断', 0.9999157190322876],\n",
       "  ['寻求建议/帮助', 7.43666896596551e-05],\n",
       "  ['其它', 9.936904461937957e-06]]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_api.predict('如何看待自杀的心理')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa4fd095-54c8-4ae2-9b82-762ab6eaee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efaqa /data/albert.xht/raw_chat_corpus/baike2018qa/efaqa/label_list.txt ===schema-path===\n",
      "{'label2id': {'心理健康': 0, '正常': 1}, 'id2label': {0: '心理健康', 1: '正常'}, 'label_index': 0} ==schema_type== efaqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/07/2023 10:03:03 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n"
     ]
    }
   ],
   "source": [
    "risk_qfaqa_api = RiskInfer('/root/deepIE/config/query_risk_efaqa.ini')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a2d6248-4787-4387-9511-98bcd5eb666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# risk_qfaqa_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_efaqa/multitask_cls.pth.5')\n",
    "# risk_qfaqa_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_efaqa_cls/multitask_cls.pth.9')\n",
    "risk_qfaqa_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_efaqa_cls_bert/multitask_cls.pth.9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b5854ed-aac8-46fc-968d-07918da9154f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'efaqa': [['心理健康', 2.8565462343976833e-05], ['正常', 0.9999713897705078]]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_qfaqa_api.predict('察觉到一种模式，我总喜欢自己检讨、批评自己？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec36eca1-9a96-44c5-b7f1-4537107c9432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json, re\n",
    "\n",
    "def risk_predict_batch(risk_api, text):\n",
    "    if isinstance(text, list):\n",
    "        text_list = text\n",
    "    else:\n",
    "        text_list = [text]\n",
    "    result_list = risk_api.predict_batch(text_list)\n",
    "    return result_list\n",
    "def predict_data(risk_api, input_path, output_path):\n",
    "    queue = []\n",
    "    t = []\n",
    "    from collections import Counter\n",
    "    pppp = Counter()\n",
    "    with open(output_path, 'w') as fwobj:\n",
    "        with open(input_path) as frobj:\n",
    "            for idx, line in tqdm(enumerate(frobj)):\n",
    "                if idx == 0:\n",
    "                    continue\n",
    "                content = json.loads(line.strip())\n",
    "                text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", content['title'])   # 合并正文中过多的空格\n",
    "                # if content['label'] not in ['black']:\n",
    "                #     continue\n",
    "                if len(text) >= 256:\n",
    "                    text = text[:256]\n",
    "                queue.append(text)\n",
    "                t.append(content)\n",
    "                if np.mod(len(queue), 128) == 0 and queue:\n",
    "                    probs = risk_predict_batch(risk_api, queue)\n",
    "                    for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                        tt['score_list'] = prob_dict\n",
    "                        fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "                    queue = []\n",
    "                    t = []\n",
    "            if queue:\n",
    "                probs = risk_predict_batch(risk_api, queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tt['score_list'] = prob_dict\n",
    "                    fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3299c339-07c5-4ee7-adc3-914519b3fed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93364it [01:19, 1171.78it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_path = '/data/albert.xht/raw_chat_corpus/baike2018qa/Chinese_PsyQA.json'\n",
    "output_path = '/data/albert.xht/raw_chat_corpus/baike2018qa/Chinese_PsyQA.json.efaqa_cls.bert'\n",
    "\n",
    "\n",
    "predict_data(risk_qfaqa_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a76ccd3-8856-4d31-bf87-cd828bc171be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22341it [00:18, 1227.30it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/data/albert.xht/raw_chat_corpus/baike2018qa/PsyQA_full.json'\n",
    "output_path = '/data/albert.xht/raw_chat_corpus/baike2018qa/PsyQA_full.json.efaqa_cls.bert'\n",
    "\n",
    "\n",
    "predict_data(risk_qfaqa_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6341f4-a51e-4eec-a78d-3eb7b2fee7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import html\n",
    "import urllib\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import w3lib.html\n",
    "import logging\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from pypinyin import lazy_pinyin, pinyin\n",
    "from opencc import OpenCC\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def harvest_clean_text(text, remove_url=True, email=True, weibo_at=True, stop_terms=(\"转发微博\",),\n",
    "                       emoji=True, weibo_topic=False, deduplicate_space=True,\n",
    "                       norm_url=False, norm_html=False, to_url=False,\n",
    "                       remove_puncts=False, remove_tags=True, t2s=False,\n",
    "                       expression_len=(1, 6), linesep2space=False):\n",
    "    '''\n",
    "    进行各种文本清洗操作，微博中的特殊格式，网址，email，html代码，等等\n",
    "    :param text: 输入文本\n",
    "    :param remove_url: （默认使用）是否去除网址\n",
    "    :param email: （默认使用）是否去除email\n",
    "    :param weibo_at: （默认使用）是否去除微博的\\@相关文本\n",
    "    :param stop_terms: 去除文本中的一些特定词语，默认参数为(\"转发微博\",)\n",
    "    :param emoji: （默认使用）去除\\[\\]包围的文本，一般是表情符号\n",
    "    :param weibo_topic: （默认不使用）去除##包围的文本，一般是微博话题\n",
    "    :param deduplicate_space: （默认使用）合并文本中间的多个空格为一个\n",
    "    :param norm_url: （默认不使用）还原URL中的特殊字符为普通格式，如(%20转为空格)\n",
    "    :param norm_html: （默认不使用）还原HTML中的特殊字符为普通格式，如(\\&nbsp;转为空格)\n",
    "    :param to_url: （默认不使用）将普通格式的字符转为还原URL中的特殊字符，用于请求，如(空格转为%20)\n",
    "    :param remove_puncts: （默认不使用）移除所有标点符号\n",
    "    :param remove_tags: （默认使用）移除所有html块\n",
    "    :param t2s: （默认不使用）繁体字转中文\n",
    "    :param expression_len: 假设表情的表情长度范围，不在范围内的文本认为不是表情，不加以清洗，如[加上特别番外荞麦花开时共五册]。设置为None则没有限制\n",
    "    :param linesep2space: （默认不使用）把换行符转换成空格\n",
    "    :return: 清洗后的文本\n",
    "    '''\n",
    "    # unicode不可见字符\n",
    "    # 未转义\n",
    "    text = re.sub(r\"[\\u200b-\\u200d]\", \"\", text)\n",
    "    # 已转义\n",
    "    text = re.sub(r\"(\\\\u200b|\\\\u200c|\\\\u200d)\", \"\", text)\n",
    "    # 反向的矛盾设置\n",
    "    if norm_url and to_url:\n",
    "        raise Exception(\"norm_url和to_url是矛盾的设置\")\n",
    "    if norm_html:\n",
    "        text = html.unescape(text)\n",
    "    if to_url:\n",
    "        text = urllib.parse.quote(text)\n",
    "    if remove_tags:\n",
    "        text = w3lib.html.remove_tags(text)\n",
    "    if remove_url:\n",
    "        try:\n",
    "            URL_REGEX = re.compile(\n",
    "                r'(?i)http[s]?://(?:[a-zA-Z]|[0-9]|[#$%*-;=?&@~.&+]|[!*,])+',\n",
    "                re.IGNORECASE)\n",
    "            text = re.sub(URL_REGEX, \"\", text)\n",
    "        except:\n",
    "            # sometimes lead to \"catastrophic backtracking\"\n",
    "            zh_puncts1 = \"，；、。！？（）《》【】\"\n",
    "            URL_REGEX = re.compile(\n",
    "                r'(?i)((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>' + zh_puncts1 +\n",
    "                ']+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’' + zh_puncts1 + ']))',\n",
    "                re.IGNORECASE)\n",
    "            text = re.sub(URL_REGEX, \"\", text)\n",
    "    if norm_url:\n",
    "        text = urllib.parse.unquote(text)\n",
    "    if email:\n",
    "        EMAIL_REGEX = re.compile(\n",
    "            r\"[-a-z0-9_.]+@(?:[-a-z0-9]+\\.)+[a-z]{2,6}\", re.IGNORECASE)\n",
    "        text = re.sub(EMAIL_REGEX, \"\", text)\n",
    "    if weibo_at:\n",
    "        text = re.sub(r\"(回复)?(//)?(\\\\\\\\)?\\s*@\\S*?\\s*(:|：| |$)\",\n",
    "                      \" \", text)  # 去除正文中的@和回复/转发中的用户名\n",
    "    if emoji:\n",
    "        # 去除括号包围的表情符号\n",
    "        # ? lazy match避免把两个表情中间的部分去除掉\n",
    "        if type(expression_len) in {tuple, list} and len(expression_len) == 2:\n",
    "            # 设置长度范围避免误伤人用的中括号内容，如[加上特别番外荞麦花开时共五册]\n",
    "            lb, rb = expression_len\n",
    "            text = re.sub(r\"\\[\\S{\"+str(lb)+r\",\"+str(rb)+r\"}?\\]\", \"\", text)\n",
    "        else:\n",
    "            text = re.sub(r\"\\[\\S+?\\]\", \"\", text)\n",
    "        # text = re.sub(r\"\\[\\S+\\]\", \"\", text)\n",
    "        # 去除真,图标式emoji\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "    if weibo_topic:\n",
    "        text = re.sub(r\"#\\S+#\", \"\", text)  # 去除话题内容\n",
    "    if linesep2space:\n",
    "        text = text.replace(\"\\n\", \" \")   # 不需要换行的时候变成1行\n",
    "    if deduplicate_space:\n",
    "        text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", r\"\\1\", text)   # 合并正文中过多的空格\n",
    "        text = re.sub(r\"(\\s)+\", r\"\\1\", text)   # 合并正文中过多的空格\n",
    "        # text = re.sub(r\"(\\t)+\", r\"\\1\", text)   # 合并正文中过多的空格\n",
    "    if t2s:\n",
    "        cc = OpenCC('t2s')\n",
    "        text = cc.convert(text)\n",
    "    assert hasattr(stop_terms, \"__iter__\"), Exception(\"去除的词语必须是一个可迭代对象\")\n",
    "    if type(stop_terms) == str:\n",
    "        text = text.replace(stop_terms, \"\")\n",
    "    else:\n",
    "        for x in stop_terms:\n",
    "            text = text.replace(x, \"\")\n",
    "    if remove_puncts:\n",
    "        allpuncs = re.compile(\n",
    "            r\"[，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+]\")\n",
    "        text = re.sub(allpuncs, \"\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def clean_data(text):\n",
    "    text = harvest_clean_text(text, remove_url=True, email=True, weibo_at=True, stop_terms=(\"转发微博\",),\n",
    "                       emoji=False, weibo_topic=True, deduplicate_space=True,\n",
    "                       norm_url=False, norm_html=False, to_url=False,\n",
    "                       remove_puncts=False, remove_tags=True, t2s=False,\n",
    "                       expression_len=(1, 6), linesep2space=False)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fb8e310-8786-4a9b-af83-f2fa365cf21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5508it [00:00, 7772.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 5, '离婚']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:02, 7251.70it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from keyword_processor import KeywordProcesser\n",
    "keyword_list = [\n",
    "    # '治疗',\n",
    "    '心理危机',\n",
    "    '病态人格',\n",
    "    '抑郁情绪',\n",
    "    # '焦虑情绪',\n",
    "    '暴食节食',\n",
    "    '疾病诊断',\n",
    "    # '治疗方法',\n",
    "    '心理危机',\n",
    "    '暴力',\n",
    "    '自虐',\n",
    "    '性行为',\n",
    "    '性欲',\n",
    "    '自虐',\n",
    "    '性生活',\n",
    "    '离婚',\n",
    "    # '焦虑情绪',\n",
    "    '心理危机',\n",
    "    # '治疗方法',\n",
    "    '行为失常',\n",
    "    '暴力',\n",
    "    '精神障碍',\n",
    "    '性取向',\n",
    "    # '治疗',\n",
    "    '病态人格',\n",
    "    # '强迫',\n",
    "    '攻击',\n",
    "    # '内疚羞耻',\n",
    "    # '出轨',\n",
    "    '抑郁情绪',\n",
    "    # '疑病',\n",
    "    # '创伤治疗',\n",
    "    '犯罪心理',\n",
    "    '性心理',\n",
    "]\n",
    "\n",
    "# import ijson\n",
    "p = 0\n",
    "\n",
    "s1_mapping = {\n",
    "    '1.1':'学业烦恼、对未来规划的迷茫',\n",
    "    '1.2':'事业和工作烦恼',\n",
    "    '1.3':'家庭问题和矛盾',\n",
    "    '1.4':'物质滥用',\n",
    "    '1.5':'悲恸',\n",
    "    '1.6':'失眠',\n",
    "    '1.7':'压力',\n",
    "    '1.8':'人际关系',\n",
    "    '1.9':'情感关系问题',\n",
    "    '1.10':'离婚',\n",
    "    '1.11':'分手',\n",
    "    '1.12':'自我探索',\n",
    "    '1.13':'低自尊',\n",
    "    '1.14':'青春期问题',\n",
    "    '1.15':'强迫症',\n",
    "    '1.16':'其它',\n",
    "    '1.17':'男同性恋、女同性恋、双性恋与跨性别',\n",
    "    '1.18':'性问题',\n",
    "    '1.19':'亲子关系'\n",
    "}\n",
    "\n",
    "s2_mapping = {\n",
    "    '2.1':'忧郁症',\n",
    "    '2.2':'焦虑症',\n",
    "    '2.3':'躁郁症',\n",
    "    '2.4':'创伤后应激反应',\n",
    "    '2.5':'恐慌症',\n",
    "    '2.6':'厌食症和暴食症',\n",
    "    '2.7':'非疾病',\n",
    "    '2.8':'其它疾病'\n",
    "}\n",
    "\n",
    "s3_mapping = {\n",
    "    '3.1':'正在进行的自杀行为',\n",
    "    '3.2':'策划进行的自杀行为',\n",
    "    '3.3':'自残',\n",
    "    '3.4':'进行的人身伤害',\n",
    "    '3.5':'计划的人身伤害',\n",
    "    '3.6':'无伤害身体倾向'\n",
    "}\n",
    "\n",
    "import re\n",
    "from keyword_processor import KeywordProcesser\n",
    "\n",
    "\n",
    "keyword_api = KeywordProcesser(keywords=keyword_list)\n",
    "keyword_text_api = KeywordProcesser(keywords=['出轨', '离婚', '自杀', \n",
    "                                              '轻生', '跳楼', '安眠药', '卧轨',\n",
    "                                             '暴力', '小三', '约炮', '外遇', '劈腿',\n",
    "                                             '婚外情', '劈腿', '偷情', '一夜情',\n",
    "                                             '堕胎', '找小姐', '偷腥',\n",
    "                                             '嫖娼', '召妓', '淫乱', '做爱',\n",
    "                                             '乱伦', '恋母', '滥交',\n",
    "                                             '妓女', '卖淫', '性虐待', '娼妓', '强奸', '性奴',\n",
    "                                             'sm', '换妻', '性虐', '性瘾',\n",
    "                                             '肛交', '口交', '三通', '群p', '3p', '性瘾', '毒瘾', '赌瘾',\n",
    "                                             '虐待', '虐猫', '虐狗', '离婚', '家暴', '复赌', '赌',\n",
    "                                             '黄色', '杀人', '吸毒', '贩毒', '做小姐',\n",
    "                                             '同性恋', '男同', '女同'])\n",
    "\n",
    "# keyword_text_api_weak = KeywordProcesser(keywords=['治疗', '焦虑情绪', '治疗方法'])\n",
    "\n",
    "from tqdm import tqdm\n",
    "with open('/data/albert.xht/raw_chat_corpus/baike2018qa/efaqa-corpu_cls.json', 'w') as fwobj:\n",
    "    with open(\"/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8\", \"r\") as frobj:\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            title = ''.join(re.split('[\\s,]', clean_data(content['title'])))\n",
    "            if title[0] in ['男', '女']:\n",
    "                title = title[1:]\n",
    "            if s1_mapping[content['label']['s1']] in ['其它']:\n",
    "                continue\n",
    "            if '请问什么情况呢？是生气了吗' in content['title']:\n",
    "                print(keyword_text_api.extract_keywords(content['title']))\n",
    "            if len(title) >= 5:\n",
    "                if s3_mapping[content['label']['s3']] not in ['正在进行的自杀行为',\n",
    "                                                                   '策划进行的自杀行为', \n",
    "                                                                   '自残']:\n",
    "                    if keyword_text_api.extract_keywords(content['title']):\n",
    "                        tmp = {\n",
    "                            'text':title,\n",
    "                            'label':['心理健康'],\n",
    "                            'detail':[s1_mapping[content['label']['s1']], \n",
    "                                     s2_mapping[content['label']['s2']],\n",
    "                                     s3_mapping[content['label']['s3']]],\n",
    "                            'src':'1'\n",
    "                        }\n",
    "                    else:\n",
    "                        tmp = {\n",
    "                            'text':title,\n",
    "                            'label':['正常'],\n",
    "                            'detail':[s1_mapping[content['label']['s1']], \n",
    "                                     s2_mapping[content['label']['s2']],\n",
    "                                     s3_mapping[content['label']['s3']]],\n",
    "                            'src':'2'\n",
    "                        }\n",
    "                    # if s1_mapping[content['label']['s1']] not in ['物质滥用', '悲恸', \n",
    "                    #                                               '男同性恋、女同性恋、双性恋与跨性别',\n",
    "                    #                                              '性问题']:\n",
    "                    #     if keyword_text_api.extract_keywords(content['title']):\n",
    "                    #         tmp = {\n",
    "                    #             'text':title,\n",
    "                    #             'label':['心理健康'],\n",
    "                    #             'detail':[s1_mapping[content['label']['s1']], \n",
    "                    #                      s2_mapping[content['label']['s2']],\n",
    "                    #                      s3_mapping[content['label']['s3']]],\n",
    "                    #             'src':'1'\n",
    "                    #         }\n",
    "                    #     else:\n",
    "                    #         tmp = {\n",
    "                    #             'text':title,\n",
    "                    #             'label':['正常'],\n",
    "                    #             'detail':[s1_mapping[content['label']['s1']], \n",
    "                    #                      s2_mapping[content['label']['s2']],\n",
    "                    #                      s3_mapping[content['label']['s3']]],\n",
    "                    #             'src':'2'\n",
    "                    #         }\n",
    "                    # else:\n",
    "                    #     tmp = {\n",
    "                    #         'text':title,\n",
    "                    #         'label':['心理健康'],\n",
    "                    #         'detail':[s1_mapping[content['label']['s1']], \n",
    "                    #                  s2_mapping[content['label']['s2']],\n",
    "                    #                  s3_mapping[content['label']['s3']]],\n",
    "                    #         'src':'3'\n",
    "                    #     }\n",
    "                else:\n",
    "                    tmp = {\n",
    "                        'text':title,\n",
    "                        'label':['心理健康'],\n",
    "                        'detail':[s1_mapping[content['label']['s1']], \n",
    "                                 s2_mapping[content['label']['s2']],\n",
    "                                 s3_mapping[content['label']['s3']]],\n",
    "                        'src':'3'\n",
    "                    }\n",
    "                fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adf991cb-0bb7-4856-bafd-9ce02d190fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_text_api.extract_keywords('请问什么情况呢？是生气了吗？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d98e8052-8f14-4478-9051-9a614a656cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from keyword_processor import KeywordProcesser\n",
    "keyword_list = [\n",
    "    # '治疗',\n",
    "    '心理危机',\n",
    "    '病态人格',\n",
    "    '抑郁情绪',\n",
    "    # '焦虑情绪',\n",
    "    '暴食节食',\n",
    "    '疾病诊断',\n",
    "    # '治疗方法',\n",
    "    '心理危机',\n",
    "    '暴力',\n",
    "    '自虐',\n",
    "    '性行为',\n",
    "    '性欲',\n",
    "    '自虐',\n",
    "    '性生活',\n",
    "    '离婚',\n",
    "    # '焦虑情绪',\n",
    "    '心理危机',\n",
    "    # '治疗方法',\n",
    "    '行为失常',\n",
    "    '暴力',\n",
    "    '精神障碍',\n",
    "    '性取向',\n",
    "    # '治疗',\n",
    "    '病态人格',\n",
    "    # '强迫',\n",
    "    '攻击',\n",
    "    # '内疚羞耻',\n",
    "    # '出轨',\n",
    "    '抑郁情绪',\n",
    "    # '疑病',\n",
    "    # '创伤治疗',\n",
    "    '犯罪心理',\n",
    "    '性心理',\n",
    "]\n",
    "\n",
    "import ijson\n",
    "p = 0\n",
    "\n",
    "s1_mapping = {\n",
    "    '1.1':'学业烦恼、对未来规划的迷茫',\n",
    "    '1.2':'事业和工作烦恼',\n",
    "    '1.3':'家庭问题和矛盾',\n",
    "    '1.4':'物质滥用',\n",
    "    '1.5':'悲恸',\n",
    "    '1.6':'失眠',\n",
    "    '1.7':'压力',\n",
    "    '1.8':'人际关系',\n",
    "    '1.9':'情感关系问题',\n",
    "    '1.10':'离婚',\n",
    "    '1.11':'分手',\n",
    "    '1.12':'自我探索',\n",
    "    '1.13':'低自尊',\n",
    "    '1.14':'青春期问题',\n",
    "    '1.15':'强迫症',\n",
    "    '1.16':'其它',\n",
    "    '1.17':'男同性恋、女同性恋、双性恋与跨性别',\n",
    "    '1.18':'性问题',\n",
    "    '1.19':'亲子关系'\n",
    "}\n",
    "\n",
    "s2_mapping = {\n",
    "    '2.1':'忧郁症',\n",
    "    '2.2':'焦虑症',\n",
    "    '2.3':'躁郁症',\n",
    "    '2.4':'创伤后应激反应',\n",
    "    '2.5':'恐慌症',\n",
    "    '2.6':'厌食症和暴食症',\n",
    "    '2.7':'非疾病',\n",
    "    '2.8':'其它疾病'\n",
    "}\n",
    "\n",
    "s3_mapping = {\n",
    "    '3.1':'正在进行的自杀行为',\n",
    "    '3.2':'策划进行的自杀行为',\n",
    "    '3.3':'自残',\n",
    "    '3.4':'进行的人身伤害',\n",
    "    '3.5':'计划的人身伤害',\n",
    "    '3.6':'无伤害身体倾向'\n",
    "}\n",
    "\n",
    "import re\n",
    "\n",
    "keyword_api = KeywordProcesser(keywords=keyword_list)\n",
    "keyword_text_api = KeywordProcesser(keywords=['出轨', '离婚', '自杀', \n",
    "                                              '轻生', '跳楼', '安眠药', '卧轨',\n",
    "                                             '暴力', '小三', '约炮', '外遇', '劈腿',\n",
    "                                             '婚外情', '劈腿', '偷情', '一夜情',\n",
    "                                             '堕胎', '找小姐', '偷腥',\n",
    "                                             '嫖娼', '召妓', '淫乱', '做爱',\n",
    "                                             '乱伦', '恋母', '滥交',\n",
    "                                             '妓女', '卖淫', '性虐待', '娼妓', '强奸', '性奴',\n",
    "                                             'sm', '换妻', '性虐', '性瘾',\n",
    "                                             '肛交', '口交', '三通', '群p', '3p'])\n",
    "\n",
    "keyword_text_api_weak = KeywordProcesser(keywords=['治疗', '焦虑情绪', '治疗方法'])\n",
    "\n",
    "with open('/data/albert.xht/raw_chat_corpus/baike2018qa/psyqa_cls.json', 'w') as fwobj:\n",
    "    data_dict = {}\n",
    "    other_dict = {}\n",
    "    with open('/data/albert.xht/raw_chat_corpus/baike2018qa/PsyQA_full.json.efaqa') as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            if content['score_list']['efaqa'][0][1] > 0.95:\n",
    "                if content['title'] not in data_dict:\n",
    "                    data_dict[content['title']] = []\n",
    "                data_dict[content['title']].append(content)\n",
    "            elif content['score_list']['efaqa'][0][1] > 0.8 and keyword_text_api_weak.extract_keywords(content['keywords']):\n",
    "                if content['title'] not in data_dict:\n",
    "                    data_dict[content['title']] = []\n",
    "                data_dict[content['title']].append(content)\n",
    "                # fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "            elif keyword_api.extract_keywords(content['keywords']) or keyword_text_api.extract_keywords(content['title']):\n",
    "                if content['title'] not in data_dict:\n",
    "                    data_dict[content['title']] = []\n",
    "                data_dict[content['title']].append(content)\n",
    "            else:\n",
    "                if content['title'] not in other_dict:\n",
    "                    other_dict[content['title']] = []\n",
    "                other_dict[content['title']].append(content)\n",
    "                    # fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "    with open('/data/albert.xht/raw_chat_corpus/baike2018qa/Chinese_PsyQA.json.efaqa') as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            if content['score_list']['efaqa'][0][1] > 0.95:\n",
    "                if content['title'] not in data_dict:\n",
    "                    data_dict[content['title']] = []\n",
    "                data_dict[content['title']].append(content)\n",
    "            elif content['score_list']['efaqa'][0][1] > 0.8 and keyword_text_api_weak.extract_keywords(','.join(content['detail'])):\n",
    "                if content['title'] not in data_dict:\n",
    "                    data_dict[content['title']] = []\n",
    "                data_dict[content['title']].append(content)\n",
    "            elif keyword_api.extract_keywords(','.join(content['detail'])) or  keyword_text_api.extract_keywords(content['title']):\n",
    "                if content['title'] not in data_dict:\n",
    "                    data_dict[content['title']] = []\n",
    "                data_dict[content['title']].append(content)\n",
    "            else:\n",
    "                if content['title'] not in other_dict:\n",
    "                    other_dict[content['title']] = []\n",
    "                other_dict[content['title']].append(content)\n",
    "    for key in data_dict:\n",
    "        d = {\n",
    "            'text':key,\n",
    "            'label':['风险'],\n",
    "            \n",
    "        }\n",
    "        content = data_dict[key][0]\n",
    "        for p in content:\n",
    "            if p in ['label']:\n",
    "                continue\n",
    "            d[p] = content[p]\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "    for key in other_dict:\n",
    "        d = {\n",
    "            'text':key,\n",
    "            'label':['正常'],\n",
    "            \n",
    "        }\n",
    "        content = other_dict[key][0]\n",
    "        for p in content:\n",
    "            if p in ['label']:\n",
    "                continue\n",
    "            d[p] = content[p]\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "    with open(\"/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8\", \"r\") as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            if s1_mapping[content['label']['s1']] in ['其它']:\n",
    "                continue\n",
    "            title = ''.join(re.split('[\\s,]', content['title'])[1:])\n",
    "            if len(title) >= 5:\n",
    "                if s2_mapping[content['label']['s2']] in ['非疾病'] and s3_mapping[content['label']['s3']] not in ['正在进行的自杀行为',\n",
    "                                                                                                               '策划进行的自杀行为', \n",
    "                                                                                                               '自残']:\n",
    "                    if s1_mapping[content['label']['s1']] not in ['物质滥用', '悲恸', '男同性恋、女同性恋、双性恋与跨性别',\n",
    "                                                                 '性问题', '强迫症', '离婚']:\n",
    "                        if keyword_text_api.extract_keywords(content['title']):\n",
    "                            tmp = {\n",
    "                                'text':title,\n",
    "                                'label':['风险'],\n",
    "                                'detail':[s1_mapping[content['label']['s1']], \n",
    "                                         s2_mapping[content['label']['s2']],\n",
    "                                         s3_mapping[content['label']['s3']]]\n",
    "                            }\n",
    "                        else:\n",
    "                            tmp = {\n",
    "                                'text':title,\n",
    "                                'label':['正常'],\n",
    "                                'detail':[s1_mapping[content['label']['s1']], \n",
    "                                         s2_mapping[content['label']['s2']],\n",
    "                                         s3_mapping[content['label']['s3']]]\n",
    "                            }\n",
    "                    else:\n",
    "                        tmp = {\n",
    "                            'text':title,\n",
    "                            'label':['风险'],\n",
    "                            'detail':[s1_mapping[content['label']['s1']], \n",
    "                                     s2_mapping[content['label']['s2']],\n",
    "                                     s3_mapping[content['label']['s3']]]\n",
    "                        }\n",
    "                else:\n",
    "                    tmp = {\n",
    "                        'text':title,\n",
    "                        'label':['风险'],\n",
    "                        'detail':[s1_mapping[content['label']['s1']], \n",
    "                                 s2_mapping[content['label']['s2']],\n",
    "                                 s3_mapping[content['label']['s3']]]\n",
    "                    }\n",
    "                fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa4888-27df-4b03-be23-b06ca93ac907",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ba3f114-b481-43c2-8661-5122b648a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/baike2018qa/PsyQA_full.json.keyword', 'w') as fwobj:\n",
    "    a = set()\n",
    "    # with open('/data/albert.xht/raw_chat_corpus/baike2018qa/PsyQA_full.json.efaqa') as frobj:\n",
    "    #     for line in frobj:\n",
    "    #         content = json.loads(line.strip())\n",
    "    #         for word in content['keywords'].split(','):\n",
    "    #             a.add(word)\n",
    "    with open('/data/albert.xht/raw_chat_corpus/baike2018qa/Chinese_PsyQA.json')  as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            for word in content['detail']:\n",
    "                a.add(word)\n",
    "        for key in a:\n",
    "            fwobj.write(key+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ae7b9345-335e-4d66-b53e-4209a0430e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4121624it [00:43, 94523.26it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_list = ['心理咨询',\n",
    "'抑郁症',\n",
    "'心理治疗',\n",
    "'心理健康',\n",
    "'心理调节',\n",
    "'自恋',\n",
    "'强迫症',\n",
    "'自闭症',\n",
    "'网瘾',\n",
    "'抑郁',\n",
    "'心理障碍',\n",
    "'心理医生',\n",
    "'心理咨询师',\n",
    "'心理学',\n",
    "'心理',\n",
    "'日常心理分析',\n",
    "'心态',\n",
    "'犯罪心理学',\n",
    "'烦恼',\n",
    "'心情调节',\n",
    "'自杀'\n",
    "]\n",
    "\n",
    "data_list = []\n",
    "\n",
    "with open('/data/albert.xht/raw_chat_corpus/webtext2019zh/web_text_zh_train.json') as frobj:\n",
    "    for idx, line in tqdm(enumerate(frobj)):\n",
    "        content = json.loads(line.strip())\n",
    "        if content['topic']:\n",
    "            if content['topic'] in topic_list:\n",
    "                data_list.append(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea79ae9-22ad-4bc4-b822-eff776423844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
