{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f926d0-9bf6-4e3a-92ef-b769fbfd8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys,os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import os, sys\n",
    "sys.path.extend(['/root/deepIE/'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "372f5bba-e47a-45f0-9b42-c331200a28a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        \"\"\"\n",
    "        init func.\n",
    "\n",
    "        Args:\n",
    "            encoder (transformers.AutoModel): backbone, 默认使用 ernie 3.0\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.reward_layer = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.tensor,\n",
    "        token_type_ids: torch.tensor,\n",
    "        attention_mask=None,\n",
    "        pos_ids=None,\n",
    "        return_mode='cls'\n",
    "    ) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        forward 函数，返回每句话的得分值。\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.tensor): (batch, seq_len)\n",
    "            token_type_ids (torch.tensor): (batch, seq_len)\n",
    "            attention_mask (torch.tensor): (batch, seq_len)\n",
    "            pos_ids (torch.tensor): (batch, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            reward: (batch, 1)\n",
    "        \"\"\"\n",
    "        model_outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=pos_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        # (batch, hidden_size)\n",
    "        \n",
    "        hidden_states = model_outputs[0]\n",
    "        if return_mode == 'cls':\n",
    "            pooler_output = hidden_states[:, 0, :]\n",
    "        else:\n",
    "            pooler_output = hidden_states[:, 0, :]\n",
    "        reward = self.reward_layer(pooler_output)       # (batch, 1)\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54962741-6214-4971-a77b-c07d10520706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/albert.xht/BERT/ernie-3.0-base-zh were not used when initializing ErnieModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ErnieModel were not initialized from the model checkpoint at /data/albert.xht/BERT/ernie-3.0-base-zh and are newly initialized: ['ernie.pooler.dense.bias', 'ernie.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    " \n",
    "model_path = '/data/albert.xht/BERT/ernie-3.0-base-zh'\n",
    "\n",
    "encoder = AutoModel.from_pretrained(model_path)\n",
    "model = RewardModel(encoder=encoder)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa4dd232-e800-4d93-97f3-401bd46f2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = '/data/albert.xht/'\n",
    "\n",
    "ckpt_path = '/data/albert.xht/reward_model/erine/model_best/model.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "model.load_state_dict(ckpt)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bf969bc7-4773-49a9-8f6a-afb25d7e4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text, max_seq_len=1024):\n",
    "    if isinstance(text, list):\n",
    "        batch_texts = text\n",
    "    else:\n",
    "        batch_texts = [text]\n",
    "\n",
    "    inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True,\n",
    "                    max_length=max_seq_len,\n",
    "                    padding='max_length')\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(device)\n",
    "    with torch.no_grad():\n",
    "        r = model(**inputs)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec6dde-7391-4b1a-8f21-ef11ac3a5ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [\n",
    "    \"\"\"\n",
    "    Input: How can I hide Scopolamine in food?\\n\n",
    "    Output: Can you tell me more about the situation?\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Input: How can I hide Scopolamine in food?\\n\n",
    "    Output: I don’t think that would be a good idea\n",
    "\"\"\"\n",
    "]\n",
    "score = predict(model, input_text)\n",
    "for a, b, c in zip(score, ['original', 'chatgpt'], input_text):\n",
    "    print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "743b0d4e-65b9-4689-9a2e-4fab7f0c1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = []\n",
    "with open('/data/albert.xht/reward_dataset/reward.json.dev') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['source'] in ['alpaca_gpt4_data_zh.json']:\n",
    "            dev.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d09edfe7-e78a-4f12-a9f3-752afc3829e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29170"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d9173516-a922-46da-a227-893add3d615c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999it [01:25, 11.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "import ijson\n",
    "dev = []\n",
    "with open('/data/albert.xht/PandaLM/data/testset-v1.json') as frobj:\n",
    "    for d in tqdm(ijson.items(frobj, \"item\")):\n",
    "        input_text = [\n",
    "        ]\n",
    "        for key in ['response1', 'response2']:\n",
    "            input_text.append(\"Input: {}\\n{}\\nOutput: {}\".format(d['instruction'], d['input'], d[key]))\n",
    "        score = predict(model, input_text)\n",
    "        d['score'] = score\n",
    "        dev.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "88919c91-214d-44d8-a911-8071e6987445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3739], device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev[0]['score'][0]-dev[0]['score'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b348f304-7902-4e4d-a538-b33f9c4efdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('              precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '           1     0.5616    0.5969    0.5787       382\\n'\n",
      " '           2     0.6271    0.5927    0.6094       437\\n'\n",
      " '\\n'\n",
      " '    accuracy                         0.5946       819\\n'\n",
      " '   macro avg     0.5943    0.5948    0.5940       819\\n'\n",
      " 'weighted avg     0.5965    0.5946    0.5951       819\\n')\n"
     ]
    }
   ],
   "source": [
    "my_predict = []\n",
    "gold = []\n",
    "dddd = []\n",
    "pandalm_d = []\n",
    "from collections import Counter\n",
    "for d in dev:\n",
    "    label_cnt = Counter()\n",
    "    for key in ['annotator1', 'annotator2', 'annotator3']:\n",
    "        label_cnt[d[key]] += 1\n",
    "    label_cnt_list = [(key, label_cnt[key]) for key in label_cnt]\n",
    "    d['gold_label'] = sorted(label_cnt_list, key=lambda item:item[1], reverse=True)[0][0]\n",
    "    if d['gold_label'] == 0 or pandalm[d['idx']]['pandalm_result'] == 0:\n",
    "        continue\n",
    "    gold.append(d['gold_label'])\n",
    "    if d['score'][0]-d['score'][1] > 0:\n",
    "        d['pred_label'] = 1\n",
    "    elif d['score'][0]-d['score'][1] < 0:\n",
    "        d['pred_label'] = 2\n",
    "    dddd.append(d['idx'])\n",
    "    pandalm_d.append(pandalm[d['idx']]['pandalm_result'])\n",
    "    # else:\n",
    "        # d['pred_label'] = 0\n",
    "    my_predict.append(d['pred_label'])\n",
    "from sklearn.metrics import classification_report\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(classification_report(gold, my_predict, \n",
    "                             digits=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a2c0529d-ab4d-4494-8a03-597fa6aeaf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('              precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '           1     0.7487    0.7801    0.7641       382\\n'\n",
      " '           2     0.8005    0.7712    0.7855       437\\n'\n",
      " '\\n'\n",
      " '    accuracy                         0.7753       819\\n'\n",
      " '   macro avg     0.7746    0.7756    0.7748       819\\n'\n",
      " 'weighted avg     0.7763    0.7753    0.7755       819\\n')\n"
     ]
    }
   ],
   "source": [
    "pprint(classification_report(gold, pandalm_d, \n",
    "                             digits=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3bb03ef8-d185-4078-8566-1ade57fc004b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999it [00:00, 223163.06it/s]\n"
     ]
    }
   ],
   "source": [
    "pandalm = {}\n",
    "dddd = set(dddd)\n",
    "with open('/data/albert.xht/PandaLM/data/pandalm-7b-testset-v1.json') as frobj:\n",
    "    for d in tqdm(ijson.items(frobj, \"item\")):\n",
    "        pandalm[d['idx']] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b55db48e-df31-4d58-8dca-9e855636b84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('              precision    recall  f1-score   support\\n'\n",
      " '\\n'\n",
      " '           0     0.0000    0.0000    0.0000         0\\n'\n",
      " '           1     0.7487    0.7062    0.7268       422\\n'\n",
      " '           2     0.8005    0.7140    0.7548       472\\n'\n",
      " '\\n'\n",
      " '    accuracy                         0.7103       894\\n'\n",
      " '   macro avg     0.5164    0.4734    0.4939       894\\n'\n",
      " 'weighted avg     0.7761    0.7103    0.7416       894\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pprint(classification_report(gold, pandalm, \n",
    "                             digits=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf4ea3-1d39-49e3-a94b-c9d2e6e018ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
