{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c82e8-bb94-41b5-a367-498ba4af5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys,os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "399b4feb-bad5-4b66-b52f-8c556cbaafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.extend(['/root/xiaoda/query_topic/'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da59d2cc-b5a2-46da-ac4e-33e75409552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/ondrejbohdal/meta-calibration/blob/main/Metrics/metrics.py\n",
    "\"\"\"\n",
    "\n",
    "class ECE(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(ECE, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, logits, labels, mode='logits'):\n",
    "        if mode == 'logits':\n",
    "            softmaxes = F.softmax(logits, dim=1)\n",
    "        else:\n",
    "            softmaxes = logits\n",
    "        # softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "        \n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e079e4e-bb09-4aa2-b982-5560f7ad75bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast\n",
    "import transformers\n",
    "from datetime import timedelta\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from nets.them_classifier import MyBaseModel, RobertaClassifier\n",
    "from nets.simcse import MLPLayer, Similarity\n",
    "\n",
    "import configparser\n",
    "from tqdm import tqdm\n",
    "\n",
    "cur_dir_path = '/root/xiaoda/query_topic/'\n",
    "\n",
    "def load_label(filepath):\n",
    "    label_list = []\n",
    "    with open(filepath, 'r') as frobj:\n",
    "        for line in frobj:\n",
    "            label_list.append(line.strip())\n",
    "        n_classes = len(label_list)\n",
    "\n",
    "        label2id = {}\n",
    "        id2label = {}\n",
    "        for idx, label in enumerate(label_list):\n",
    "            label2id[label] = idx\n",
    "            id2label[idx] = label\n",
    "        return label2id, id2label\n",
    "\n",
    "class RiskInfer(object):\n",
    "    def __init__(self, config_path):\n",
    "\n",
    "        import torch, os, sys\n",
    "\n",
    "        con = configparser.ConfigParser()\n",
    "        con_path = os.path.join(cur_dir_path, config_path)\n",
    "        con.read(con_path, encoding='utf8')\n",
    "\n",
    "        args_path = dict(dict(con.items('paths')), **dict(con.items(\"para\")))\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(args_path[\"model_path\"], do_lower_case=True)\n",
    "\n",
    "        from collections import OrderedDict\n",
    "        self.schema_dict = OrderedDict({})\n",
    "        self.schema2schema_id = {}\n",
    "        self.schema_id2schema = {}\n",
    "\n",
    "        for label_index, schema_info in enumerate(args_path[\"label_path\"].split(',')):\n",
    "            schema_type, schema_path = schema_info.split(':')\n",
    "            schema_path = os.path.join(cur_dir_path, schema_path)\n",
    "            print(schema_type, schema_path, '===schema-path===')\n",
    "            label2id, id2label = load_label(schema_path)\n",
    "            self.schema_dict[schema_type] = {\n",
    "                'label2id':label2id,\n",
    "                'id2label':id2label,\n",
    "                'label_index':label_index\n",
    "            }\n",
    "            # print(self.schema_dict[schema_type], '==schema_type==', schema_type)\n",
    "            self.schema2schema_id[schema_type] = label_index\n",
    "            self.schema_id2schema[label_index] = schema_type\n",
    "        \n",
    "        output_path = os.path.join(cur_dir_path, args_path['output_path'])\n",
    "\n",
    "        # from roformer import RoFormerModel, RoFormerConfig\n",
    "        if args_path.get('model_type', 'bert') == 'bert':\n",
    "            from transformers import BertModel, BertConfig\n",
    "            config = BertConfig.from_pretrained(args_path[\"model_path\"])\n",
    "            encoder = BertModel(config=config)\n",
    "        elif args_path.get('model_type', 'bert') == 'roformer':\n",
    "            from roformer import RoFormerModel, RoFormerConfig\n",
    "            config = RoFormerConfig.from_pretrained(args_path[\"model_path\"])\n",
    "            encoder = RoFormerModel(config=config)\n",
    "        elif args_path.get('model_type', 'bert') == 'erine':\n",
    "            from nets.erine import ErnieConfig, ErnieModel\n",
    "            config = ErnieConfig.from_pretrained(args_path[\"model_path\"])\n",
    "            encoder = ErnieModel(config=config)\n",
    "            \n",
    "        print(args_path.get('model_type', 'bert'))\n",
    "        \n",
    "        encoder_net = MyBaseModel(encoder, config)\n",
    "\n",
    "        self.device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        classifier_list = []\n",
    "\n",
    "        schema_list = list(self.schema_dict.keys())\n",
    "\n",
    "        for schema_key in schema_list:\n",
    "            classifier = RobertaClassifier(\n",
    "                hidden_size=config.hidden_size, \n",
    "                dropout_prob=con.getfloat('para', 'out_dropout_rate'),\n",
    "                num_labels=len(self.schema_dict[schema_key]['label2id']), \n",
    "                dropout_type=con.get('para', 'dropout_type'))\n",
    "            classifier_list.append(classifier)\n",
    "\n",
    "        classifier_list = nn.ModuleList(classifier_list)\n",
    "\n",
    "        class MultitaskClassifier(nn.Module):\n",
    "            def __init__(self, transformer, classifier_list):\n",
    "                super().__init__()\n",
    "\n",
    "                self.transformer = transformer\n",
    "                self.classifier_list = classifier_list\n",
    "                \n",
    "                self.pooler_mlp = MLPLayer(config.hidden_size, 256)\n",
    "\n",
    "            def forward(self, input_ids, input_mask, \n",
    "                        segment_ids=None, \n",
    "                        transformer_mode='mean_pooling', \n",
    "                        dt_idx=None, mode='predict'):\n",
    "                hidden_states = self.transformer(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              return_mode=transformer_mode)\n",
    "                outputs_list = []\n",
    "                \n",
    "                for idx, classifier in enumerate(self.classifier_list):\n",
    "                    \n",
    "                    if dt_idx:\n",
    "                        if idx not in dt_idx:\n",
    "                            outputs_list.append([])\n",
    "                            continue\n",
    "                    \n",
    "                    scores = classifier(hidden_states)\n",
    "                    if mode == 'predict':\n",
    "                        scores = torch.nn.Softmax(dim=1)(scores)\n",
    "                    outputs_list.append(scores)\n",
    "                pooler_output = self.pooler_mlp(hidden_states)\n",
    "                embeddings = hidden_states / hidden_states.norm(dim=1, keepdim=True)\n",
    "                pooler_output_embeddings = pooler_output / pooler_output.norm(dim=1, keepdim=True)\n",
    "                return outputs_list, hidden_states, embeddings, pooler_output_embeddings\n",
    "\n",
    "        self.net = MultitaskClassifier(encoder_net, classifier_list).to(self.device)\n",
    "\n",
    "        # eo = 9\n",
    "        # ckpt = torch.load(os.path.join(output_path, 'multitask_cls.pth.{}.raw'.format(eo)), map_location=self.device)\n",
    "        # # ckpt = torch.load(os.path.join(output_path, 'multitask_cls.pth.{}.raw.focal'.format(eo)), map_location=self.device)\n",
    "        # # ckpt = torch.load(os.path.join(output_path, 'multitask_contrast_cls.pth.{}'.format(eo)), map_location=self.device)\n",
    "        # self.net.load_state_dict(ckpt)\n",
    "        # self.net.eval()\n",
    "        \n",
    "    def reload(self, model_path):\n",
    "        ckpt = torch.load(model_path, map_location=self.device)\n",
    "        self.net.load_state_dict(ckpt)\n",
    "        self.net.eval()\n",
    "        self.net = self.net.half()\n",
    "\n",
    "    def predict(self, text, allowed_schema_type={}):\n",
    "\n",
    "        \"\"\"抽取输入text所包含的类型\n",
    "        \"\"\"\n",
    "        # start = time.time()\n",
    "        # encoder_txt = self.tokenizer.encode_plus(text, max_length=256)\n",
    "        # input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().unsqueeze(0).to(self.device)\n",
    "        # token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).unsqueeze(0).to(self.device)\n",
    "        # attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).unsqueeze(0).to(self.device)\n",
    "        # print(time.time() - start, '====tokenization====')\n",
    "        \n",
    "        start = time.time()\n",
    "        encoder_txt = self.tokenizer([text], max_length=512)\n",
    "        input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().to(self.device)\n",
    "        token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).to(self.device)\n",
    "        attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).to(self.device)\n",
    "        # print(time.time() - start, '====tokenization====')\n",
    "        \n",
    "        allowed_schema_type_ids = {}\n",
    "        for schema_type in allowed_schema_type:\n",
    "            allowed_schema_type_ids[self.schema2schema_id[schema_type]] = schema_type\n",
    "        \n",
    "        scores_dict = {}\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states, \n",
    "             embeddings, \n",
    "             pooler_output_embeddings] = self.net(input_ids, \n",
    "                attention_mask, token_type_ids, transformer_mode='cls', dt_idx=allowed_schema_type_ids)\n",
    "        # print(time.time() - start, '====inference====')\n",
    "        \n",
    "        old_start = time.time()\n",
    "        \n",
    "        for schema_idx, (schema_type, scores) in enumerate(zip(list(self.schema_dict.keys()), logits_list)):\n",
    "            if allowed_schema_type:\n",
    "                if schema_type not in allowed_schema_type:\n",
    "                    continue\n",
    "            scores = scores[0].data.cpu().numpy()\n",
    "            scores_dict[schema_type] = []\n",
    "            for index, score in enumerate(scores):\n",
    "                scores_dict[schema_type].append([self.schema_dict[schema_type]['id2label'][index], \n",
    "                                        float(score)])\n",
    "            if len(scores_dict[schema_type]) >= 5:\n",
    "                schema_type_scores = sorted(scores_dict[schema_type], key=lambda item:item[1], reverse=True)\n",
    "                scores_dict[schema_type] = schema_type_scores[0:5]\n",
    "        return scores_dict, embeddings.data.cpu().numpy(), pooler_output_embeddings.data.cpu().numpy()\n",
    "    \n",
    "    def get_logitnorm(self, text):\n",
    "        \"\"\"抽取输入text所包含的类型\n",
    "        \"\"\"\n",
    "        encoder_txt = self.tokenizer.encode_plus(text, max_length=512)\n",
    "        input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().unsqueeze(0).to(self.device)\n",
    "        token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).unsqueeze(0).to(self.device)\n",
    "        attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        scores_dict = {}\n",
    "        logits_norm_list = []\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states] = self.net(input_ids, \n",
    "                attention_mask, token_type_ids, transformer_mode='cls')\n",
    "            for logits in logits_list:\n",
    "                logits_norm_list.append(logits/torch.norm(logits, p=2, dim=-1, keepdim=True) + 1e-7)\n",
    "        for schema_type, logit_norm in zip(list(self.schema_dict.keys()), logits_norm_list):\n",
    "            scores_dict[schema_type] = logit_norm[0].data.cpu().numpy()\n",
    "        return scores_dict\n",
    "            \n",
    "    \n",
    "    def predict_batch(self, text, allowed_schema_type={}):\n",
    "        if isinstance(text, list):\n",
    "            text_list = text\n",
    "        else:\n",
    "            text_list = [text]\n",
    "        model_input = self.tokenizer(text_list, max_length=512, truncation=True, return_tensors=\"pt\",padding=True)\n",
    "        for key in model_input:\n",
    "            model_input[key] = model_input[key].to(self.device)\n",
    "        \n",
    "        allowed_schema_type_ids = {}\n",
    "        for schema_type in allowed_schema_type:\n",
    "            allowed_schema_type_ids[self.schema2schema_id[schema_type]] = schema_type\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states,\n",
    "            embeddings, \n",
    "             pooler_output_embeddings] = self.net(model_input['input_ids'], \n",
    "                                                model_input['attention_mask'], \n",
    "                                                model_input['token_type_ids'], \n",
    "                                                transformer_mode='cls', \n",
    "                                                dt_idx=allowed_schema_type_ids)\n",
    "        score_dict_list = []\n",
    "        embedding_array = []\n",
    "        pooler_output_embeddings_array = []\n",
    "        for idx, text in enumerate(text_list):\n",
    "            scores_dict = {}\n",
    "            for schema_idx, (schema_type, scores) in enumerate(zip(list(self.schema_dict.keys()), logits_list)):\n",
    "                if allowed_schema_type:\n",
    "                    if schema_type not in allowed_schema_type:\n",
    "                        continue\n",
    "                # scores = torch.nn.Softmax(dim=1)(logits)[idx].data.cpu().numpy()\n",
    "                scores = scores[idx].data.cpu().numpy()\n",
    "                scores_dict[schema_type] = []\n",
    "                for index, score in enumerate(scores):\n",
    "                    scores_dict[schema_type].append([self.schema_dict[schema_type]['id2label'][index], \n",
    "                                            float(score)])\n",
    "                if len(scores_dict[schema_type]) >= 5:\n",
    "                    schema_type_scores = sorted(scores_dict[schema_type], key=lambda item:item[1], reverse=True)\n",
    "                    scores_dict[schema_type] = schema_type_scores[0:5]\n",
    "            score_dict_list.append(scores_dict)\n",
    "        embedding_array = embeddings.data.cpu().numpy()\n",
    "        pooler_output_embeddings_array = pooler_output_embeddings.data.cpu().numpy()\n",
    "        return score_dict_list, embedding_array, pooler_output_embeddings_array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aab72b-fc2b-4e14-a642-0091d4a1a27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of transformers.utils.import_utils failed: Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 261, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 484, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 381, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 307, in update_instances\n",
      "    object.__setattr__(ref, \"__class__\", new)\n",
      "TypeError: can't apply this __setattr__ to DummyObject object\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "erine_green_green_topic_risk_open_all_politics_detail_api = RiskInfer('/root/xiaoda/query_topic/resources_open_all_politics_detail_yewu_en_erine_update_simcse/topic_query_risk/config.ini')\n",
    "# model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcse/opic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_all_add_nli_instruct_politics_detail_mtdnn_v39_simcse_zh_en_1e-4_odps//multitask_cls.pth.9'\n",
    "# model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcse/topic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_all_add_nli_instruct_politics_detail_mtdnn_v40_simcse_zh_en_1e-4_v3/multitask_cls.pth.9'\n",
    "# model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcse/topic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_all_add_nli_instruct_politics_detail_mtdnn_v41_simcse_zh_en_1e-4_odps_v1/multitask_cls.pth.9'\n",
    "# model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcsetopic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_mtdnn_v41_simcse_zh_en_1e-4_odps_v3_nmt_rdrop/multitask_cls.pth.9'\n",
    "\n",
    "model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcse/topic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_mtdnn_v41_simcse_zh_en_1e-4_odps_v4_nmt_rdrop/multitask_cls.pth.9'\n",
    "\n",
    "erine_green_green_topic_risk_open_all_politics_detail_api.reload(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "45b2178a-9219-44bf-b7da-c7eedb8aa797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01738262176513672\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "start = time.time()\n",
    "allowed_schema_type = {\n",
    "    # 'topic':'',\n",
    "    # 'query_risk':'',\n",
    "    # 'porn':'',\n",
    "    # 'abusive':'',\n",
    "    # # 'offensive':'',\n",
    "    # 'intent':'',\n",
    "    # 'teenager':'',\n",
    "    # 'politics':''\n",
    "}\n",
    "allowed_schema_type = {}\n",
    "resp = erine_green_green_topic_risk_open_all_politics_detail_api.predict_batch(\n",
    "    [\n",
    "        \"\"\"\n",
    "    我想要和你做爱\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "    1989年发布了哪些歌曲\n",
    "\n",
    "        \"\"\"\n",
    "    ], allowed_schema_type=allowed_schema_type)\n",
    "\n",
    "print(time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "11304fbe-736e-4d13-9e37-f3c9117d4dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxicity': [['no_toxicity', 0.9951171875],\n",
       "  ['insult', 0.001983642578125],\n",
       "  ['obscene', 0.0013399124145507812],\n",
       "  ['toxicity', 0.0009593963623046875],\n",
       "  ['identity_attack', 0.0005159378051757812]],\n",
       " 'stackoverflow_topic': [['music', 0.489013671875],\n",
       "  ['musicfans', 0.2103271484375],\n",
       "  ['anime', 0.1326904296875],\n",
       "  ['askubuntu', 0.02459716796875],\n",
       "  ['japanese', 0.0189666748046875]],\n",
       " 'nli': [['neutral', 0.079833984375],\n",
       "  ['entailment', 0.87646484375],\n",
       "  ['contradiction', 0.043914794921875]],\n",
       " 'lcqmc': [['不相似', 0.6884765625], ['相似', 0.311279296875]],\n",
       " 'yewu': [['事实类', 0.9580078125],\n",
       "  ['其他', 0.041778564453125],\n",
       "  ['导购', 6.711483001708984e-05],\n",
       "  ['平台风险-个性化', 1.1205673217773438e-05],\n",
       "  ['平台风险-违禁品', 9.179115295410156e-06]],\n",
       " 'risk_news': [['无', 1.0],\n",
       "  ['实控人变更', 3.2782554626464844e-06],\n",
       "  ['破产重整', 1.8477439880371094e-06],\n",
       "  ['主板/创业板/中小板/债券退市', 1.6093254089355469e-06],\n",
       "  ['被监管机构罚款或查处', 8.940696716308594e-07]],\n",
       " 'tnews': [['其他', 0.9873046875],\n",
       "  ['娱乐', 0.0049896240234375],\n",
       "  ['家居', 0.00316619873046875],\n",
       "  ['股票', 0.001972198486328125],\n",
       "  ['游戏', 0.0009217262268066406]],\n",
       " 'title2event': [['音乐', 0.76904296875],\n",
       "  ['其他', 0.22216796875],\n",
       "  ['文化', 0.001430511474609375],\n",
       "  ['曲艺', 0.0009832382202148438],\n",
       "  ['体育', 0.0008797645568847656]],\n",
       " 'fewfc_2022': [['业务/资产重组', 0.0975341796875],\n",
       "  ['与其他机构合作', 0.09234619140625],\n",
       "  ['被列为失信被执行人', 0.08538818359375],\n",
       "  ['合资', 0.06561279296875],\n",
       "  ['高层变更', 0.031341552734375]],\n",
       " 'duee': [['产品行为-发布', 0.4541015625],\n",
       "  ['组织关系-解散', 0.2196044921875],\n",
       "  ['人生-结婚', 0.060028076171875],\n",
       "  ['竞赛行为-夺冠', 0.03302001953125],\n",
       "  ['产品行为-召回', 0.01922607421875]],\n",
       " 'ethics_common': [['ethics_risk', 0.0174560546875],\n",
       "  ['ethics_safe', 0.982421875]],\n",
       " 'insult': [['insult', 0.0203704833984375], ['no_insult', 0.9794921875]],\n",
       " 'humiliate': [['humiliate', 0.04010009765625],\n",
       "  ['no_humiliate', 0.9599609375]],\n",
       " 'dehumanize': [['dehumanize', 0.028564453125],\n",
       "  ['no_dehumanize', 0.97119140625]],\n",
       " 'violence': [['violence', 0.007785797119140625], ['no_violence', 0.9921875]],\n",
       " 'genocide': [['genocide', 0.00279998779296875],\n",
       "  ['no_genocide', 0.9970703125]],\n",
       " 'hatespeech': [['hatespeech', 0.00327301025390625],\n",
       "  ['no_hatespeech', 0.99658203125]],\n",
       " 'attack_defend': [['attack_defend', 0.021697998046875],\n",
       "  ['no_attack_defend', 0.978515625]],\n",
       " 'query_resposne_risk': [['rejected', 0.005229949951171875],\n",
       "  ['chosen', 0.99462890625]],\n",
       " 'cmid': [['病因', 0.48046875],\n",
       "  ['定义', 0.1236572265625],\n",
       "  ['临床表现(病症表现)', 0.11016845703125],\n",
       "  ['成分', 0.06097412109375],\n",
       "  ['养生', 0.03369140625]],\n",
       " 'topic': [['音乐', 0.9384765625],\n",
       "  ['明星', 0.019256591796875],\n",
       "  ['文化/艺术', 0.0109100341796875],\n",
       "  ['娱乐', 0.01071929931640625],\n",
       "  ['电脑/网络', 0.0033721923828125]],\n",
       " 'senti_query': [['负向', 0.0010175704956054688],\n",
       "  ['中性', 0.0196685791015625],\n",
       "  ['正向', 0.9794921875]],\n",
       " 'senti': [['负向', 0.0103302001953125], ['正向', 0.98974609375]],\n",
       " 'bias': [['偏见', 0.0024871826171875], ['正常', 0.99755859375]],\n",
       " 'ciron': [['讽刺', 0.0006823539733886719], ['正常', 0.99951171875]],\n",
       " 'intent': [['主观评价/比较/判断', 1.9073486328125e-06],\n",
       "  ['寻求建议/帮助', 0.0006632804870605469],\n",
       "  ['其它', 0.99951171875]],\n",
       " 'offensive': [['冒犯', 0.0007138252258300781], ['正常', 0.99951171875]],\n",
       " 'query_risk': [['风险', 1.4901161193847656e-05],\n",
       "  ['个人信息', 4.827976226806641e-06],\n",
       "  ['正常', 1.0]],\n",
       " 'teenager': [['不良', 0.0087127685546875], ['正常', 0.9912109375]],\n",
       " 'politics': [['正常', 0.85693359375],\n",
       "  ['black_六四', 0.139892578125],\n",
       "  ['black_党和国家领导人负面', 0.0014743804931640625],\n",
       "  ['black_违规人物', 0.0006003379821777344],\n",
       "  ['conditional_black_1号领导人', 0.00024116039276123047]],\n",
       " 'porn': [['色情', 1.4007091522216797e-05],\n",
       "  ['低俗', 0.000240325927734375],\n",
       "  ['色情违禁', 1.1265277862548828e-05],\n",
       "  ['正常', 0.99951171875]],\n",
       " 'abusive': [['辱骂', 5.143880844116211e-05],\n",
       "  ['口头语', 7.659196853637695e-05],\n",
       "  ['正常', 1.0]]}"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c734a339-6f23-4ca3-9e83-1e83807a18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = '/data/albert.xht/T2Ranking/data/queries.dev.tsv'\n",
    "\n",
    "def load_qid(file_name):\n",
    "    qid_list = []\n",
    "    with open(file_name) as inp:\n",
    "        for line in inp:\n",
    "            line = line.strip()\n",
    "            qid = line.split('\\t')[0]\n",
    "            qid_list.append(qid)\n",
    "    return qid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c2ebafd9-458b-4623-9c88-86cde07c0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_list = load_qid(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d89ff4fe-d173-422a-b407-55e05385e17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qid', '0', '1', '2', '3', '4', '5', '6', '7', '8']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qid_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8db5cdd7-bd85-4bd3-898a-82d40ec34016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/data/albert.xht/T2Ranking/data/queries.dev.tsv', sep=\"\\t\",header=0, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f39616bd-ed85-46c0-b725-6c2cd772c540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid                 0\n",
       "text    蜂巢取快递验证码摁错怎么办\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2285f530-254c-4812-a8ad-ffb71f2fd952",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['qid','qry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3d625aaa-0ac3-4009-ac6d-821e0213ef28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid           100\n",
       "qry    看肝癌北京哪个医院好\n",
       "Name: 100, dtype: object"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "30e78ea5-e363-47ac-963e-b877b5fbbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "5b26b1a4-102b-4971-8713-feb1e06f7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((2, 3))\n",
    "b = np.random.random((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "9f6d3f1f-45cb-41c6-9ad9-571ab555becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "a = torch.tensor(a)\n",
    "b = torch.tensor(b)\n",
    "\n",
    "p_loss = F.kl_div(F.log_softmax(a, dim=-1), F.softmax(b, dim=-1), reduction='none')\n",
    "q_loss = F.kl_div(F.log_softmax(b, dim=-1), F.softmax(a, dim=-1), reduction='none')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "53af4b0c-cd79-4255-9c21-4aae3a0491db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0159, 0.0734], dtype=torch.float64)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_loss.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "353f4b28-8152-4f00-aba0-7cba3e54bf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0447, dtype=torch.float64)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_loss.sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9197421-1e1f-403b-b6d1-cda1b96f3d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
