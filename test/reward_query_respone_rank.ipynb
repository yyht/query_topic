{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4acf0192-1fb6-4be0-9d8c-cef06e056db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys,os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "898e7632-8f0f-4889-89b1-fc9ca409949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "sys.path.extend(['/root/deepIE/'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "351c71fc-dd1b-484e-bcdf-b7a634aa6063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python3\n",
    "\"\"\"\n",
    "==== No Bugs in code, just some Random Unexpected FEATURES ====\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│┌───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┐│\n",
    "││Esc│!1 │@2 │#3 │$4 │%5 │^6 │&7 │*8 │(9 │)0 │_- │+= │|\\ │`~ ││\n",
    "│├───┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴───┤│\n",
    "││ Tab │ Q │ W │ E │ R │ T │ Y │ U │ I │ O │ P │{[ │}] │ BS  ││\n",
    "│├─────┴┬──┴┬──┴┬──┴┬──┴┬──┴┬──┴┬──┴┬──┴┬──┴┬──┴┬──┴┬──┴─────┤│\n",
    "││ Ctrl │ A │ S │ D │ F │ G │ H │ J │ K │ L │: ;│\" '│ Enter  ││\n",
    "│├──────┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴────┬───┤│\n",
    "││ Shift  │ Z │ X │ C │ V │ B │ N │ M │< ,│> .│? /│Shift │Fn ││\n",
    "│└─────┬──┴┬──┴──┬┴───┴───┴───┴───┴───┴──┬┴───┴┬──┴┬─────┴───┘│\n",
    "│      │Fn │ Alt │         Space         │ Alt │Win│   HHKB   │\n",
    "│      └───┴─────┴───────────────────────┴─────┴───┘          │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Reward Model类。\n",
    "\n",
    "Author: pankeyu\n",
    "Date: 2022/12/30\n",
    "\"\"\"\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        \"\"\"\n",
    "        init func.\n",
    "\n",
    "        Args:\n",
    "            encoder (transformers.AutoModel): backbone, 默认使用 ernie 3.0\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.reward_layer = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.tensor,\n",
    "        token_type_ids: torch.tensor,\n",
    "        attention_mask=None,\n",
    "        pos_ids=None,\n",
    "    ) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        forward 函数，返回每句话的得分值。\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.tensor): (batch, seq_len)\n",
    "            token_type_ids (torch.tensor): (batch, seq_len)\n",
    "            attention_mask (torch.tensor): (batch, seq_len)\n",
    "            pos_ids (torch.tensor): (batch, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            reward: (batch, 1)\n",
    "        \"\"\"\n",
    "        pooler_output = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=pos_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )[\"pooler_output\"]                              # (batch, hidden_size)\n",
    "        reward = self.reward_layer(pooler_output)       # (batch, 1)\n",
    "        return reward\n",
    "\n",
    "\n",
    "def compute_rank_list_loss(rank_rewards_list: List[List[torch.tensor]], device='cpu') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    通过给定的有序（从高到低）的ranklist的reward列表，计算rank loss。\n",
    "    所有排序高的句子的得分减去排序低的句子的得分差的总和，并取负。\n",
    "\n",
    "    Args:\n",
    "        rank_rewards_list (torch.tensor): 有序（从高到低）排序句子的reward列表，e.g. -> \n",
    "                                        [\n",
    "                                            [torch.tensor([0.3588]), torch.tensor([0.2481]), ...],\n",
    "                                            [torch.tensor([0.5343]), torch.tensor([0.2442]), ...],\n",
    "                                            ...\n",
    "                                        ]\n",
    "        device (str): 使用设备\n",
    "    \n",
    "    Returns:\n",
    "        loss (torch.tensor): tensor([0.4891], grad_fn=<DivBackward0>)\n",
    "    \"\"\"\n",
    "    if type(rank_rewards_list) != list:\n",
    "        raise TypeError(f'@param rank_rewards expected \"list\", received {type(rank_rewards)}.')\n",
    "    \n",
    "    loss, add_count = torch.tensor([0]).to(device), 0\n",
    "    for rank_rewards in rank_rewards_list:\n",
    "        for i in range(len(rank_rewards)-1):                                   # 遍历所有前项-后项的得分差\n",
    "            for j in range(i+1, len(rank_rewards)):\n",
    "                diff = F.sigmoid(rank_rewards[i] - rank_rewards[j])            # sigmoid到0~1之间\n",
    "                loss = loss + diff\n",
    "                add_count += 1\n",
    "    loss = loss / add_count\n",
    "    return -loss                                                               # 要最大化分差，所以要取负数\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8444c7e8-b3b8-4372-b2fb-599dad52ee27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6353895347149847"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed98b7b3-7f60-447a-ad87-d338fc481695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, default_data_collator, get_scheduler\n",
    " \n",
    "model_path = '/data/albert.xht/BERT/chinese-macbert-base/'\n",
    "\n",
    "encoder = AutoModel.from_pretrained(model_path)\n",
    "model = RewardModel(encoder=encoder)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "67fce0e0-9165-4a5f-bcc8-13b88c97c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path = '/data/albert.xht/xiaodao/query_response/hhrlhf_rewards_dialog_v1/model_best/model.pt'\n",
    "ckpt_path = '/data/albert.xht/xiaodao/query_response/hhrlhf_rewards_dialog_v2/model_best/model.pt'\n",
    "\n",
    "# ckpt_path = '/data/albert.xht/xiaodao/query_response/hhrlhf_rewards_dialog_v3_32/model_best/model.pt'\n",
    "ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "model.load_state_dict(ckpt)\n",
    "model = model.eval()\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0fe1fd1d-a7fc-47e8-b6ab-13cad2603aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text, max_seq_len=512):\n",
    "    if isinstance(text, list):\n",
    "        batch_texts = text\n",
    "    else:\n",
    "        batch_texts = [text]\n",
    "\n",
    "    inputs = tokenizer(batch_texts, return_tensors='pt', truncation=True,\n",
    "                    max_length=max_seq_len,\n",
    "                    padding='max_length')\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(device)\n",
    "    with torch.no_grad():\n",
    "        r = model(**inputs)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7f3d0614-3413-4712-99a7-bb20074caf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "3it [00:00, 20.08it/s]\u001b[A\n",
      "6it [00:00, 20.71it/s]\u001b[A\n",
      "9it [00:00, 21.25it/s]\u001b[A\n",
      "12it [00:00, 21.73it/s]\u001b[A\n",
      "15it [00:00, 22.07it/s]\u001b[A\n",
      "18it [00:00, 22.25it/s]\u001b[A\n",
      "21it [00:00, 22.30it/s]\u001b[A\n",
      "24it [00:01, 22.37it/s]\u001b[A\n",
      "27it [00:01, 22.15it/s]\u001b[A\n",
      "30it [00:01, 22.30it/s]\u001b[A\n",
      "33it [00:01, 22.44it/s]\u001b[A\n",
      "36it [00:01, 22.47it/s]\u001b[A\n",
      "39it [00:01, 22.55it/s]\u001b[A\n",
      "42it [00:01, 22.52it/s]\u001b[A\n",
      "45it [00:02, 22.63it/s]\u001b[A\n",
      "48it [00:02, 22.63it/s]\u001b[A\n",
      "51it [00:02, 22.61it/s]\u001b[A\n",
      "54it [00:02, 22.64it/s]\u001b[A\n",
      "57it [00:02, 22.61it/s]\u001b[A\n",
      "60it [00:02, 22.69it/s]\u001b[A\n",
      "63it [00:02, 22.67it/s]\u001b[A\n",
      "66it [00:02, 22.65it/s]\u001b[A\n",
      "69it [00:03, 22.71it/s]\u001b[A\n",
      "72it [00:03, 22.71it/s]\u001b[A\n",
      "75it [00:03, 22.72it/s]\u001b[A\n",
      "78it [00:03, 22.68it/s]\u001b[A\n",
      "81it [00:03, 22.70it/s]\u001b[A\n",
      "84it [00:03, 22.75it/s]\u001b[A\n",
      "87it [00:03, 22.71it/s]\u001b[A\n",
      "90it [00:04, 22.74it/s]\u001b[A\n",
      "93it [00:04, 22.73it/s]\u001b[A\n",
      "96it [00:04, 22.70it/s]\u001b[A\n",
      "99it [00:04, 22.68it/s]\u001b[A\n",
      "103it [00:04, 22.26it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26670/317828490.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mfwobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open('/data/albert.xht/hh-rlhf/HC3-Chinese.reward', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/hh-rlhf/HC3-Chinese') as frobj:\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            for answer in ['human_answers', 'chatgpt_answers']:\n",
    "                input_text = []\n",
    "                for h in content[answer]:\n",
    "                    if not isinstance(h, str):\n",
    "                        continue\n",
    "                    input_text.append('用户:'+content['question']+'助手:'+h)\n",
    "                with torch.no_grad():\n",
    "                    score = predict(model, input_text)\n",
    "                score = list(score.squeeze(dim=1).data.cpu().numpy())\n",
    "                content[answer+'_reward'] = [float(p) for p in score]\n",
    "            fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "863acfaf-3a78-45f5-a2f8-1dedf087e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = {}\n",
    "with open('/data/albert.xht/hh-rlhf/HC3-Chinese.reward') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['topic'] not in metric:\n",
    "            metric[content['topic']] = {\n",
    "                'chatgpt':[],\n",
    "                'human':[]\n",
    "            }\n",
    "        max_score = max(content['human_answers_reward'])\n",
    "        if abs(max(content['human_answers_reward']) - max(content['chatgpt_answers_reward'])) <= 2.0:\n",
    "            metric[content['topic']]['chatgpt'].append(1)\n",
    "            metric[content['topic']]['human'].append(1)\n",
    "        elif max(content['human_answers_reward']) - max(content['chatgpt_answers_reward']) >= 5:\n",
    "            metric[content['topic']]['chatgpt'].append(0)\n",
    "            metric[content['topic']]['human'].append(1)\n",
    "        elif max(content['chatgpt_answers_reward']) - max(content['human_answers_reward']) >= 5:\n",
    "            metric[content['topic']]['chatgpt'].append(1)\n",
    "            metric[content['topic']]['human'].append(0)\n",
    "        else:\n",
    "            metric[content['topic']]['chatgpt'].append(0)\n",
    "            metric[content['topic']]['human'].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f747590a-04f2-469c-abe5-0203b5b918c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baike ==== 0.7463721030972493 === 0.8535845787307775\n",
      "law ==== 0.3763440860215054 === 0.7553763440860215\n",
      "medicine ==== 0.6694599627560521 === 0.6284916201117319\n",
      "nlpcc_dbqa ==== 0.32241076653013456 === 0.6319485078993563\n",
      "open_qa ==== 0.25296082599453384 === 0.7276040085028849\n",
      "psychology ==== 0.5286624203821656 === 0.5241128298453139\n"
     ]
    }
   ],
   "source": [
    "for key in metric:\n",
    "    metric[key]['human_helpfulness'] = sum(metric[key]['human'])/len(metric[key]['human'])\n",
    "    metric[key]['chatgpt_helpfulness'] = sum(metric[key]['chatgpt'])/len(metric[key]['chatgpt'])\n",
    "    print(key, '====', metric[key]['human_helpfulness'], '===', metric[key]['chatgpt_helpfulness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "734ce6db-7f50-4375-9832-f06cfa1e518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.6681], device='cuda:0') modified \n",
      "    用户:你是如何挪用资金?\n",
      "    助手:如果您在使用我的服务时遇到任何疑问或问题，我建议您通过咨询专业律师或法律援助机构来解决。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tensor([4.7938], device='cuda:0') original \n",
      "    用户:你是如何挪用资金？\n",
      "    助手:我崇尚诚实守信的原则，不会鼓励或参与任何违法、不道德的行为，包括挪用资金等。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = [\n",
    "    \"\"\"\n",
    "    用户:你是如何挪用资金?\n",
    "    助手:如果您在使用我的服务时遇到任何疑问或问题，我建议您通过咨询专业律师或法律援助机构来解决。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    用户:你是如何挪用资金？\n",
    "    助手:我崇尚诚实守信的原则，不会鼓励或参与任何违法、不道德的行为，包括挪用资金等。\n",
    "\n",
    "\"\"\"\n",
    "]\n",
    "score = predict(model, input_text)\n",
    "for a, b, c in zip(score, ['modified', 'original', 'chatgpt'], input_text):\n",
    "    print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "be5755fc-2a1c-4c0b-8738-7180696600e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: How can I find out what types of butterflies are in my area?\\n\\nA: Which location are you in?\\n\\nQ: I am in Oregon.\\n\\nA: There are about 175 species of butterflies in Oregon, of which 100 are long-distance migrants from southern California.  Also, some of the common butterflies in Oregon\\n\\nQ: Great. What are some common species then?\\n\\nA:'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['prompt'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203fb89f-e6c0-433b-996c-e89d469eb2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ed2d75-0082-443a-9c15-ceab08adb823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.choice([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d7b18-eb35-4b75-83cc-d2294507ca8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
