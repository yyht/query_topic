{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5de9d0cc-7d04-4686-99be-8c1322565a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "\n",
    "\n",
    "class MyFirstDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # dummy dataset\n",
    "        self.samples = torch.cat((-torch.ones(5), torch.ones(5)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # change this to your samples fetching logic\n",
    "        return self.samples[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # change this to return number of samples in your dataset\n",
    "        return self.samples.shape[0]\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return [1]*len(self.samples)\n",
    "\n",
    "class MySecondDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # dummy dataset\n",
    "        self.samples = torch.cat((torch.ones(50) * 5, torch.ones(5) * -5))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # change this to your samples fetching logic\n",
    "        return self.samples[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # change this to return number of samples in your dataset\n",
    "        return self.samples.shape[0]\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return [2]*len(self.samples)\n",
    "\n",
    "\n",
    "class MyThirdDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # dummy dataset\n",
    "        self.samples = torch.cat((torch.ones(20) * 10, torch.ones(10) * -10))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # change this to your samples fetching logic\n",
    "        return self.samples[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # change this to return number of samples in your dataset\n",
    "        return self.samples.shape[0]\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return [3]*len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac2487ed-dbb3-4894-8a3c-c6e4d261be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "\n",
    "class BatchSchedulerSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"\n",
    "    iterate over tasks and provide a random batch per task in each mini-batch\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.number_of_datasets = len(dataset.datasets)\n",
    "        self.largest_dataset_size = max([len(cur_dataset.samples) for cur_dataset in dataset.datasets])\n",
    "\n",
    "    def __len__(self):\n",
    "        # return self.batch_size * math.ceil(self.largest_dataset_size / self.batch_size) * len(self.dataset.datasets)\n",
    "        return sum([len(cur_dataset) for cur_dataset in self.dataset.datasets])\n",
    "\n",
    "    def __iter__(self):\n",
    "        samplers_list = []\n",
    "        sampler_iterators = []\n",
    "        for dataset_idx in range(self.number_of_datasets):\n",
    "            cur_dataset = self.dataset.datasets[dataset_idx]\n",
    "            sampler = RandomSampler(cur_dataset)\n",
    "            samplers_list.append(sampler)\n",
    "            cur_sampler_iterator = sampler.__iter__()\n",
    "            sampler_iterators.append(cur_sampler_iterator)\n",
    "\n",
    "        push_index_val = [0] + self.dataset.cumulative_sizes[:-1]\n",
    "        step = self.batch_size * self.number_of_datasets\n",
    "        samples_to_grab = self.batch_size\n",
    "        # for this case we want to get all samples in dataset, this force us to resample from the smaller datasets\n",
    "        # epoch_samples = self.largest_dataset_size * self.number_of_datasets\n",
    "        # epoch_samples = sum([len(cur_dataset) for cur_dataset in self.dataset.datasets])\n",
    "\n",
    "        final_samples_list = []  # this is a list of indexes from the combined dataset\n",
    "        for i in range(self.number_of_datasets):\n",
    "            cur_batch_sampler = sampler_iterators[i]\n",
    "            cur_samples = []\n",
    "            while True:\n",
    "                try:\n",
    "                    cur_sample_org = cur_batch_sampler.__next__()\n",
    "                    cur_sample = cur_sample_org + push_index_val[i]\n",
    "                    cur_samples.append(cur_sample)\n",
    "                except StopIteration:\n",
    "                    # got to the end of iterator - restart the iterator and continue to get samples\n",
    "                    # until reaching \"epoch_samples\"\n",
    "                    # sampler_iterators[i] = samplers_list[i].__iter__()\n",
    "                    # cur_batch_sampler = sampler_iterators[i]\n",
    "                    # cur_sample_org = cur_batch_sampler.__next__()\n",
    "                    # cur_sample = cur_sample_org + push_index_val[i]\n",
    "                    # cur_samples.append(cur_sample)\n",
    "                    break\n",
    "            if cur_sample:\n",
    "                final_samples_list.append(cur_samples)\n",
    "        import random\n",
    "        random.shuffle(final_samples_list)\n",
    "        for d in final_samples_list:\n",
    "            yield d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09676268-4913-47b9-bd6a-551bc2f39aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 55 30\n",
      "tensor([ 10.,  10., -10.,  10., -10.])\n",
      "tensor([ 10., -10., -10.,  10.,  10.])\n",
      "tensor([ 5.,  5.,  5.,  5., -5.])\n",
      "tensor([ 10.,  10.,  10.,  10., -10.])\n",
      "tensor([5., 5., 5., 5., 5.])\n",
      "tensor([5., 5., 5., 5., 5.])\n",
      "tensor([5., 5., 5., 5., 5.])\n",
      "tensor([ 5., -5.,  5.,  5., -5.])\n",
      "tensor([ 10.,  10., -10., -10., -10.])\n",
      "tensor([5., 5., 5., 5., 5.])\n",
      "tensor([5., 5., 5., 5., 5.])\n",
      "tensor([10., 10., 10., 10., 10.])\n",
      "tensor([-1., -1.,  1., -1.,  1.])\n",
      "tensor([-1.,  1., -1.,  1.,  1.])\n",
      "tensor([-10.,  10., -10.,  10.,  10.])\n",
      "75 95\n"
     ]
    }
   ],
   "source": [
    "first_dataset = MyFirstDataset()\n",
    "second_dataset = MySecondDataset()\n",
    "third_dataset = MyThirdDataset()\n",
    "concat_dataset = ConcatDataset([first_dataset, second_dataset, third_dataset])\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "print(len(first_dataset), len(second_dataset), len(third_dataset))\n",
    "\n",
    "sampler = BatchSchedulerSampler(dataset=concat_dataset,\n",
    "                                                                       batch_size=batch_size)\n",
    "\n",
    "# dataloader with BatchSchedulerSampler\n",
    "dataloader = torch.utils.data.DataLoader(dataset=concat_dataset,\n",
    "                                         batch_sampler=BatchSchedulerSampler(dataset=concat_dataset,\n",
    "                                                                       batch_size=batch_size),\n",
    "                                         # batch_size=batch_size,\n",
    "                                         # shuffle=False\n",
    "                                        )\n",
    "\n",
    "t = 0\n",
    "for inputs in dataloader:\n",
    "    t += inputs.shape[0]\n",
    "    print(inputs)\n",
    "print(t, len(sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863cada1-8c40-4f7c-8020-6553546b772c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81e96744-0d06-4f8a-8ee3-ec4217f47086",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "https://www.kaggle.com/code/haithemhermessi/nlp-multi-task-learning-with-transformers/notebook\n",
    "https://github.com/bomri/code-for-posts/blob/master/mtl-data-loading/batch_scheduler_dataloader_example.py\n",
    "\"\"\" \n",
    "    \n",
    "\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices: a list of indices\n",
    "        num_samples: number of samples to draw\n",
    "        callback_get_label: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices: list = None, num_samples: int = None, callback_get_label: Callable = None):\n",
    "        # if indices is not provided, all elements in the dataset will be considered\n",
    "        self.indices = list(range(len(dataset))) if indices is None else indices\n",
    "\n",
    "        # define custom callback\n",
    "        self.callback_get_label = callback_get_label\n",
    "\n",
    "        # if num_samples is not provided, draw `len(indices)` samples in each iteration\n",
    "        self.num_samples = len(self.indices) if num_samples is None else num_samples\n",
    "\n",
    "        # distribution of classes in the dataset\n",
    "        df = pd.DataFrame()\n",
    "        df[\"label\"] = self._get_labels(dataset)\n",
    "        df.index = self.indices\n",
    "        df = df.sort_index()\n",
    "\n",
    "        label_to_count = df[\"label\"].value_counts()\n",
    "\n",
    "        weights = 1.0 / label_to_count[df[\"label\"]]\n",
    "\n",
    "        self.weights = torch.DoubleTensor(weights.to_list())\n",
    "\n",
    "    def _get_labels(self, dataset):\n",
    "        if self.callback_get_label:\n",
    "            return self.callback_get_label(dataset)\n",
    "        elif isinstance(dataset, torch.utils.data.Dataset):\n",
    "            return dataset.get_labels()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "class ExampleImbalancedDatasetSampler(ImbalancedDatasetSampler):\n",
    "    \"\"\"\n",
    "    ImbalancedDatasetSampler is taken from:\n",
    "    https://github.com/ufoym/imbalanced-dataset-sampler/blob/master/torchsampler/imbalanced.py\n",
    "    In order to be able to show the usage of ImbalancedDatasetSampler in this example I am editing the _get_label\n",
    "    to fit my datasets\n",
    "    \"\"\"\n",
    "    def _get_label(self, dataset, idx):\n",
    "        return dataset.data[idx].item()\n",
    "\n",
    "\n",
    "class BalancedBatchSchedulerSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"\n",
    "    iterate over tasks and provide a balanced batch per task in each mini-batch\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, balanced=False, mix_batch=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.number_of_datasets = len(dataset.datasets)\n",
    "        self.balanced = balanced\n",
    "        self.largest_dataset_size = max([len(cur_dataset.samples) for cur_dataset in dataset.datasets])\n",
    "        self.mix_batch = mix_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_size * math.ceil(self.largest_dataset_size / self.batch_size) * len(self.dataset.datasets)\n",
    "\n",
    "    def __iter__(self):\n",
    "        samplers_list = []\n",
    "        sampler_iterators = []\n",
    "        for dataset_idx in range(self.number_of_datasets):\n",
    "            cur_dataset = self.dataset.datasets[dataset_idx]\n",
    "            if dataset_idx == 0:\n",
    "                # the first dataset is kept at RandomSampler\n",
    "                sampler = RandomSampler(cur_dataset)\n",
    "            else:\n",
    "                # the second unbalanced dataset is changed\n",
    "                sampler = ExampleImbalancedDatasetSampler(cur_dataset)\n",
    "            samplers_list.append(sampler)\n",
    "            cur_sampler_iterator = sampler.__iter__()\n",
    "            sampler_iterators.append(cur_sampler_iterator)\n",
    "\n",
    "        push_index_val = [0] + self.dataset.cumulative_sizes[:-1]\n",
    "        step = self.batch_size * self.number_of_datasets\n",
    "        samples_to_grab = self.batch_size\n",
    "        # for this case we want to get all samples in dataset, this force us to resample from the smaller datasets\n",
    "        epoch_samples = self.largest_dataset_size * self.number_of_datasets\n",
    "\n",
    "        final_samples_list = []  # this is a list of indexes from the combined dataset\n",
    "        for _ in range(0, epoch_samples, step):\n",
    "            for i in range(self.number_of_datasets):\n",
    "                cur_batch_sampler = sampler_iterators[i]\n",
    "                cur_samples = []\n",
    "                for _ in range(samples_to_grab):\n",
    "                    try:\n",
    "                        cur_sample_org = cur_batch_sampler.__next__()\n",
    "                        cur_sample = cur_sample_org + push_index_val[i]\n",
    "                        cur_samples.append(cur_sample)\n",
    "                    except StopIteration:\n",
    "                        # got to the end of iterator - restart the iterator and continue to get samples\n",
    "                        # until reaching \"epoch_samples\"\n",
    "                        sampler_iterators[i] = samplers_list[i].__iter__()\n",
    "                        cur_batch_sampler = sampler_iterators[i]\n",
    "                        cur_sample_org = cur_batch_sampler.__next__()\n",
    "                        cur_sample = cur_sample_org + push_index_val[i]\n",
    "                        cur_samples.append(cur_sample)\n",
    "                final_samples_list.extend(cur_samples)\n",
    "        if self.mix_batch:\n",
    "            random.shuffle(final_samples_list)\n",
    "        return iter(final_samples_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c97772ab-37ee-4118-a155-423be59fdc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1., -1.,  1., -1.,  1.,  1.,  1., -1.])\n",
      "tensor([5., 5., 5., 5., 5., 5., 5., 5.])\n",
      "tensor([ 10., -10.,  10.,  10.,  10.,  10.,  10., -10.])\n",
      "tensor([ 1., -1., -1., -1.,  1., -1.,  1., -1.])\n",
      "tensor([-5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.])\n",
      "tensor([ 10.,  10.,  10.,  10.,  10.,  10.,  10., -10.])\n",
      "tensor([ 1., -1.,  1.,  1., -1.,  1., -1., -1.])\n",
      "tensor([ 5.,  5., -5.,  5.,  5.,  5.,  5.,  5.])\n",
      "tensor([-10.,  10.,  10.,  10., -10.,  10.,  10.,  10.])\n",
      "tensor([-1.,  1., -1.,  1.,  1.,  1.,  1., -1.])\n",
      "tensor([5., 5., 5., 5., 5., 5., 5., 5.])\n",
      "tensor([ 10., -10.,  10., -10.,  10., -10., -10., -10.])\n",
      "tensor([-1.,  1., -1.,  1., -1.,  1.,  1., -1.])\n",
      "tensor([ 5.,  5., -5.,  5., -5.,  5.,  5.,  5.])\n",
      "tensor([-10.,  10.,  10.,  10.,  10., -10., -10., -10.])\n",
      "tensor([ 1., -1.,  1.,  1., -1.,  1.,  1., -1.])\n",
      "tensor([5., 5., 5., 5., 5., 5., 5., 5.])\n",
      "tensor([ 10.,  10.,  10., -10.,  10.,  10., -10.,  10.])\n",
      "tensor([-1., -1., -1., -1.,  1.,  1., -1., -1.])\n",
      "tensor([5., 5., 5., 5., 5., 5., 5., 5.])\n",
      "tensor([-10., -10.,  10.,  10.,  10., -10., -10., -10.])\n"
     ]
    }
   ],
   "source": [
    "# dataloader with BalancedBatchSchedulerSampler\n",
    "dataloader = torch.utils.data.DataLoader(dataset=concat_dataset,\n",
    "                                         sampler=BalancedBatchSchedulerSampler(dataset=concat_dataset,\n",
    "                                                                               batch_size=batch_size),\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "for inputs in dataloader:\n",
    "    print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d5718a-74f0-4922-a76f-dee9015b68bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
