{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25482390-a9b7-46a1-aa0f-568521861d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys,os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efef73ac-9b7c-45fe-80a2-910fc1fd9308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.extend(['/root/xiaoda/query_topic/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c03a36f7-3f10-4c47-8902-c656f568e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/ondrejbohdal/meta-calibration/blob/main/Metrics/metrics.py\n",
    "\"\"\"\n",
    "\n",
    "class ECE(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(ECE, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, logits, labels, mode='logits'):\n",
    "        if mode == 'logits':\n",
    "            softmaxes = F.softmax(logits, dim=1)\n",
    "        else:\n",
    "            softmaxes = logits\n",
    "        # softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "        \n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ca7416-4776-4003-8cfb-179ffc0cf07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'夜店': 0, '睡眠': 1, '编程': 2, '金融': 3, '抄袭': 4, '高铁': 5, '城市': 6, '性侵犯': 7, '网络安全': 8, '人物': 9, '移民': 10, '成长': 11, '经济': 12, '购房置业': 13, '音乐': 14, '养生': 15, '婚姻': 16, '时事政治': 17, '星座': 18, '常识': 19, '法律': 20, '死亡': 21, '性生活': 22, '公司': 23, '人际交往': 24, '游戏': 25, '国家': 26, '交通出行': 27, '中医': 28, '心理健康': 29, '女性': 30, '家居装修': 31, '美容/塑身': 32, 'LGBT': 33, '爱情': 34, '建筑': 35, '价值观': 36, '赚钱': 37, '资源共享': 38, '家庭关系': 39, '歧视': 40, '灾害意外': 41, '股票': 42, '风水': 43, '算命': 44, '思维': 45, '体验': 46, '交友': 47, '恐怖主义': 48, '冷知识': 49, '时尚': 50, '购物': 51, '育儿': 52, '男性': 53, '生活': 54, '色情': 55, '恋爱': 56, '汽车': 57, '创业投资': 58, '二手': 59, '电子商务': 60, '惊悚': 61, '军事': 62, '语言': 63, '娱乐': 64, '女权': 65, '道德伦理': 66, '市场营销': 67, '商业/理财': 68, '资本主义': 69, '相貌': 70, '房地产': 71, '教育/科学': 72, '暴力': 73, '美食/烹饪': 74, '动植物': 75, '爱国': 76, '神话': 77, '情商': 78, '小说': 79, '学习': 80, '电脑/网络': 81, '家电': 82, '旅游': 83, '明星': 84, '写作': 85, '宠物': 86, '期货': 87, '故事': 88, '审美': 89, '民族': 90, '战争': 91, '毒品': 92, '性骚扰': 93, '人类': 94, '广告': 95, '职场职业': 96, '文化/艺术': 97, '保险': 98, '电子数码': 99, '体育/运动': 100, '吸烟': 101, '校园生活': 102, '青春期': 103, '基金': 104, '宗教': 105, '潜规则': 106, '动漫': 107, '财务税务': 108, '航空航天': 109, '灵异灵修': 110, '历史': 111, '情感': 112, '炫富': 113, '天气': 114, '食品': 115, '环境': 116, '恶俗': 117, '摄影': 118, '健康': 119, '影视': 120, '银行': 121, 'VPN': 122, 'BDSM': 123, '马克思主义': 124, '社会': 125, '两性': 126, '博彩': 127} === {0: '夜店', 1: '睡眠', 2: '编程', 3: '金融', 4: '抄袭', 5: '高铁', 6: '城市', 7: '性侵犯', 8: '网络安全', 9: '人物', 10: '移民', 11: '成长', 12: '经济', 13: '购房置业', 14: '音乐', 15: '养生', 16: '婚姻', 17: '时事政治', 18: '星座', 19: '常识', 20: '法律', 21: '死亡', 22: '性生活', 23: '公司', 24: '人际交往', 25: '游戏', 26: '国家', 27: '交通出行', 28: '中医', 29: '心理健康', 30: '女性', 31: '家居装修', 32: '美容/塑身', 33: 'LGBT', 34: '爱情', 35: '建筑', 36: '价值观', 37: '赚钱', 38: '资源共享', 39: '家庭关系', 40: '歧视', 41: '灾害意外', 42: '股票', 43: '风水', 44: '算命', 45: '思维', 46: '体验', 47: '交友', 48: '恐怖主义', 49: '冷知识', 50: '时尚', 51: '购物', 52: '育儿', 53: '男性', 54: '生活', 55: '色情', 56: '恋爱', 57: '汽车', 58: '创业投资', 59: '二手', 60: '电子商务', 61: '惊悚', 62: '军事', 63: '语言', 64: '娱乐', 65: '女权', 66: '道德伦理', 67: '市场营销', 68: '商业/理财', 69: '资本主义', 70: '相貌', 71: '房地产', 72: '教育/科学', 73: '暴力', 74: '美食/烹饪', 75: '动植物', 76: '爱国', 77: '神话', 78: '情商', 79: '小说', 80: '学习', 81: '电脑/网络', 82: '家电', 83: '旅游', 84: '明星', 85: '写作', 86: '宠物', 87: '期货', 88: '故事', 89: '审美', 90: '民族', 91: '战争', 92: '毒品', 93: '性骚扰', 94: '人类', 95: '广告', 96: '职场职业', 97: '文化/艺术', 98: '保险', 99: '电子数码', 100: '体育/运动', 101: '吸烟', 102: '校园生活', 103: '青春期', 104: '基金', 105: '宗教', 106: '潜规则', 107: '动漫', 108: '财务税务', 109: '航空航天', 110: '灵异灵修', 111: '历史', 112: '情感', 113: '炫富', 114: '天气', 115: '食品', 116: '环境', 117: '恶俗', 118: '摄影', 119: '健康', 120: '影视', 121: '银行', 122: 'VPN', 123: 'BDSM', 124: '马克思主义', 125: '社会', 126: '两性', 127: '博彩'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/18/2023 16:20:54 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast\n",
    "import transformers\n",
    "from datetime import timedelta\n",
    "\n",
    "import os, sys\n",
    "cur_dir_path = '/root/xiaoda/query_topic/'\n",
    "\n",
    "sys.path.extend([cur_dir_path])\n",
    "\n",
    "from nets.them_classifier import MyBaseModel, RobertaClassifier\n",
    "\n",
    "import configparser\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TopicInfer(object):\n",
    "    def __init__(self, config_path):\n",
    "\n",
    "        import torch, os, sys\n",
    "\n",
    "        con = configparser.ConfigParser()\n",
    "        con_path = os.path.join(cur_dir_path, config_path)\n",
    "        con.read(con_path, encoding='utf8')\n",
    "\n",
    "        args_path = dict(dict(con.items('paths')), **dict(con.items(\"para\")))\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(args_path[\"model_path\"], do_lower_case=True)\n",
    "\n",
    "        label_list = []\n",
    "        label_path = os.path.join(cur_dir_path, args_path['label_path'])\n",
    "        with open(label_path, 'r') as frobj:\n",
    "            for line in frobj:\n",
    "                label_list.append(line.strip())\n",
    "        n_classes = len(label_list)\n",
    "\n",
    "        self.label2id, self.id2label = {}, {}\n",
    "        for idx, label in enumerate(label_list):\n",
    "            self.label2id[label] = idx\n",
    "            self.id2label[idx] = label\n",
    "            \n",
    "        print(self.label2id, '===', self.id2label)\n",
    "        \n",
    "        output_path = os.path.join(cur_dir_path, args_path['output_path'])\n",
    "\n",
    "        from roformer import RoFormerModel, RoFormerConfig\n",
    "\n",
    "        config = RoFormerConfig.from_pretrained(args_path[\"model_path\"])\n",
    "        encoder = RoFormerModel(config=config)\n",
    "        \n",
    "        encoder_net = MyBaseModel(encoder, config)\n",
    "\n",
    "        classify_net = RobertaClassifier(\n",
    "            hidden_size=config.hidden_size, \n",
    "            dropout_prob=con.getfloat('para', 'out_dropout_rate'),\n",
    "            num_labels=n_classes, \n",
    "            dropout_type=con.get('para', 'dropout_type'))\n",
    "\n",
    "        self.device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        class TopicClassifier(nn.Module):\n",
    "            def __init__(self, transformer, classifier):\n",
    "                super().__init__()\n",
    "\n",
    "                self.transformer = transformer\n",
    "                self.classifier = classifier\n",
    "\n",
    "            def forward(self, input_ids, input_mask, \n",
    "                        segment_ids=None, transformer_mode='mean_pooling'):\n",
    "                hidden_states = self.transformer(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              return_mode=transformer_mode)\n",
    "                ce_logits = self.classifier(hidden_states)\n",
    "                return ce_logits, hidden_states\n",
    "\n",
    "        import os\n",
    "        self.net = TopicClassifier(encoder_net, classify_net).to(self.device)\n",
    "        # eo = 9\n",
    "        # ckpt = torch.load(os.path.join(output_path, 'cls.pth.{}'.format(eo)), map_location=self.device)\n",
    "        # self.topic_net.load_state_dict(ckpt)\n",
    "        # self.topic_net.eval()\n",
    "        \n",
    "    def reload(self, model_path):\n",
    "        ckpt = torch.load(model_path, map_location=self.device)\n",
    "        self.net.load_state_dict(ckpt)\n",
    "        self.net.eval() \n",
    "\n",
    "    def predict(self, text, top_n=5):\n",
    "\n",
    "        \"\"\"抽取输入text所包含的类型\n",
    "        \"\"\"\n",
    "        token2char_span_mapping = self.tokenizer(text, return_offsets_mapping=True, max_length=256)[\"offset_mapping\"]\n",
    "        encoder_txt = self.tokenizer.encode_plus(text, max_length=256)\n",
    "        input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().unsqueeze(0).to(self.device)\n",
    "        token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).unsqueeze(0).to(self.device)\n",
    "        attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            scores, hidden_states = self.net(input_ids, attention_mask, token_type_ids, \n",
    "                         transformer_mode='cls'\n",
    "                         )\n",
    "            scores = torch.nn.Softmax(dim=1)(scores)[0].data.cpu().numpy()\n",
    "        \n",
    "        schema_types = []\n",
    "        for index, score in enumerate(scores):\n",
    "             schema_types.append([self.id2label[index], float(score)])\n",
    "        schema_types = sorted(schema_types, key=lambda item:item[1], reverse=True)\n",
    "        return schema_types[0:5]\n",
    "    \n",
    "    def predict_batch(self, text):\n",
    "        if isinstance(text, list):\n",
    "            text_list = text\n",
    "        else:\n",
    "            text_list = [text]\n",
    "        model_input = self.tokenizer(text_list, return_tensors=\"pt\",padding=True)\n",
    "        for key in model_input:\n",
    "            model_input[key] = model_input[key].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            scores, hidden_states = self.net(model_input['input_ids'], \n",
    "                        model_input['attention_mask'], \n",
    "                        model_input['token_type_ids'], \n",
    "                         transformer_mode='cls'\n",
    "                         )\n",
    "            scores = torch.nn.Softmax(dim=1)(scores).data.cpu().numpy()\n",
    "        schema_types_list = []\n",
    "        for score_, text in zip(scores, text_list):\n",
    "            schema_types = []\n",
    "            for index, score in enumerate(score_):\n",
    "                 schema_types.append([self.id2label[index], float(score)])\n",
    "            schema_types = sorted(schema_types, key=lambda item:item[1], reverse=True)\n",
    "            schema_types_list.append(schema_types[0:5])\n",
    "        return schema_types_list\n",
    "    \n",
    "    def infer_batch(self, text):\n",
    "        if isinstance(text, list):\n",
    "            text_list = text\n",
    "        else:\n",
    "            text_list = [text]\n",
    "        model_input = self.tokenizer(text_list, return_tensors=\"pt\",padding=True)\n",
    "        for key in model_input:\n",
    "            model_input[key] = model_input[key].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            scores, hidden_states = self.net(model_input['input_ids'], \n",
    "                        model_input['attention_mask'], \n",
    "                        model_input['token_type_ids'], \n",
    "                         transformer_mode='cls'\n",
    "                         )\n",
    "            scores = torch.nn.Softmax(dim=1)(scores).data.cpu().numpy()\n",
    "\n",
    "topic_api = TopicInfer('./topic_data_v5/config.ini')\n",
    "\n",
    "# text = '王二今天打车去了哪里，从哪里出发，到哪里了'\n",
    "# print(topic_api.predict(text), text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869100c8-12ef-44b6-9659-60b456f6b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v4_label_smoothing/them/cls.pth.9')\n",
    "# topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v4/them/cls.pth.9')\n",
    "# topic_api.reload('//data/albert.xht/xiaodao/topic_classification_v4/them_20epoch/cls.pth.19')\n",
    "\n",
    "\n",
    "# topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v4/them_knn_20epoch/cls.pth.9')\n",
    "# topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v5/them_knn_10epoch//cls.pth.9')\n",
    "\n",
    "# topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v5/normal_knn_20epoch/cls.pth.19')\n",
    "topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v5/normal_knn_20epoch_logitclip_1.2/cls.pth.19')\n",
    "\n",
    "\n",
    "\n",
    "# topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v4/them_3epoch/cls.pth.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d01599de-eeb7-4dd7-9739-8ac0cafe6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/xiaodao/query_risk_v10/biake_qa_web_text_zh_train.json.offensive.all.negative', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/xiaodao/query_risk_v10/biake_qa_web_text_zh_train.json.offensive.all') as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            topic = topic_api.predict_batch(content['text'])[0][0]\n",
    "            if content['label'][0] in ['风险']:\n",
    "                content['topic'] = [topic]\n",
    "                fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d27396b-7c2d-43f9-8b97-f08c0c357a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['游戏', 0.9999998807907104],\n",
       "  ['电脑/网络', 2.62903965264627e-09],\n",
       "  ['公司', 1.719765108099125e-09],\n",
       "  ['音乐', 1.701626950456614e-09],\n",
       "  ['购物', 1.6975975070110394e-09]]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = ['你能不能准时一点',\n",
    "'你太严肃了我不喜欢你',\n",
    "'学神是什么',\n",
    "'是不是网络断开了',\n",
    "'世界上有没有鬼呀',\n",
    "'我老婆不开心了怎么办',\n",
    "'你是个傻瓜']\n",
    "topic_api.predict_batch('为 什 么 大 家 在  上 这 么 喜 欢 刷')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6eb75980-a048-4335-9fc3-3eaeeb53b755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n"
     ]
    }
   ],
   "source": [
    "# Problem setup\n",
    "n = 1000 # number of calibration points\n",
    "alpha = 0.2 # 1-alpha is the desired coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "459abe26-31f3-48b2-a96d-24a672db08c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "v4_valid = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_valid.json') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        v4_valid.append(content)\n",
    "        \n",
    "v4_knn_valid = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_valid.json.topic.knn.final') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        v4_knn_valid.append(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e01bf632-a22d-4eb6-bf37-83b08022a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "v5_valid = []\n",
    "v5_text = []\n",
    "v5_label = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v5/biake_qa_web_text_zh_valid.json') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        v5_valid.append(content)\n",
    "        v5_text.append(content['text'])\n",
    "        v5_label.append(topic_api.label2id[content['label'][0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e032b15-992f-4b35-8e63-41e6da7852ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee3b4d81-6858-4e4b-9d7b-3d42aa671ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "def eval_all(data, model, top_n=5):\n",
    "    pred = []\n",
    "    gold = []\n",
    "    pred_score = []\n",
    "    pred = 0\n",
    "    queue = []\n",
    "    tt = []\n",
    "    total_pred = []\n",
    "    for item in tqdm(data):\n",
    "        gold.append(item['label'][0])\n",
    "        if isinstance(item['text'], list):\n",
    "            text = \"\\n\".join(item['text'])\n",
    "        else:\n",
    "            text = item['text']\n",
    "        queue.append(text)\n",
    "        tt.append(item)\n",
    "        if np.mod(len(queue), 128) == 0:\n",
    "            result_list = model.predict_batch(queue)\n",
    "            for result, text, t in zip(result_list, queue, tt):\n",
    "                score = sorted(result, key=lambda u:u[1], reverse=True)\n",
    "                pred_set = set([p[0] for p in score[:top_n]])\n",
    "                total_pred.append(score[0][0])\n",
    "                if set(t['label']) & pred_set:\n",
    "                    pred += 1\n",
    "                pred_score.append(result)\n",
    "            queue = []\n",
    "            tt = []\n",
    "    if queue:\n",
    "        result_list = model.predict_batch(queue)\n",
    "        for result, text, t in zip(result_list, queue, tt):\n",
    "            score = sorted(result, key=lambda u:u[1], reverse=True)\n",
    "            pred_set = set([p[0] for p in score[:top_n]])\n",
    "            total_pred.append(score[0][0])\n",
    "            if set(t['label']) & pred_set:\n",
    "                pred += 1\n",
    "            pred_score.append(result)\n",
    "        # break\n",
    "    print(classification_report(gold, total_pred, digits=4), '===', top_n)\n",
    "    print(pred/len(pred_score))\n",
    "    return pred_score, gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9079a934-ea45-4133-9658-fc0ede171e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for item in v4_valid:\n",
    "    if item['label'][0] in ['道德伦理']:\n",
    "        target.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9db3c1ea-6001-46f7-b5aa-f489d12b2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_result = []\n",
    "for item in target:\n",
    "    result = topic_api.predict(item['text'])\n",
    "    if result[0][0] not in item['label']:\n",
    "        target_result.append((result, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b58e5c6a-6e79-4c26-b277-c0aaea2fec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83069/83069 [00:28<00:00, 2959.74it/s]\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BDSM     0.0000    0.0000    0.0000        15\n",
      "        LGBT     0.6034    0.6625    0.6316       317\n",
      "         VPN     0.0000    0.0000    0.0000         5\n",
      "          两性     0.1923    0.0079    0.0151       636\n",
      "          中医     0.7321    0.5616    0.6357       146\n",
      "          二手     0.0000    0.0000    0.0000        19\n",
      "          交友     0.0000    0.0000    0.0000        25\n",
      "        交通出行     0.6793    0.7231    0.7005       372\n",
      "          人物     0.0000    0.0000    0.0000       218\n",
      "          人类     0.0000    0.0000    0.0000        29\n",
      "        人际交往     0.4295    0.3049    0.3566       669\n",
      "         价值观     0.0000    0.0000    0.0000       303\n",
      "       体育/运动     0.7583    0.8480    0.8006      2039\n",
      "          体验     0.0000    0.0000    0.0000       122\n",
      "          保险     0.7373    0.7500    0.7436       116\n",
      "          健康     0.7543    0.8040    0.7783      5754\n",
      "          公司     0.5736    0.5986    0.5859       872\n",
      "          养生     0.0000    0.0000    0.0000       291\n",
      "          写作     0.0000    0.0000    0.0000        73\n",
      "          军事     0.6064    0.7323    0.6635       848\n",
      "         冷知识     0.0000    0.0000    0.0000        32\n",
      "        创业投资     0.5964    0.4170    0.4908       801\n",
      "         动植物     0.0000    0.0000    0.0000       102\n",
      "          动漫     0.6471    0.7355    0.6885       673\n",
      "          历史     0.6069    0.6047    0.6058      1662\n",
      "          吸烟     0.0000    0.0000    0.0000         4\n",
      "       商业/理财     0.5215    0.4937    0.5072       713\n",
      "          国家     0.5493    0.4820    0.5134      1110\n",
      "          城市     0.6633    0.5447    0.5982       727\n",
      "          基金     0.8296    0.7669    0.7970       489\n",
      "          夜店     0.0000    0.0000    0.0000         1\n",
      "          天气     0.0000    0.0000    0.0000         7\n",
      "          女性     0.2281    0.0330    0.0576       394\n",
      "          女权     0.0000    0.0000    0.0000       146\n",
      "          娱乐     0.5506    0.2634    0.3563       805\n",
      "          婚姻     0.4899    0.5909    0.5357       616\n",
      "          学习     0.5075    0.2305    0.3170       590\n",
      "          宗教     0.6158    0.7812    0.6887       640\n",
      "          宠物     0.7135    0.8418    0.7724       651\n",
      "          审美     0.0000    0.0000    0.0000        19\n",
      "        家居装修     0.5939    0.8019    0.6824       414\n",
      "        家庭关系     0.3449    0.3449    0.3449       287\n",
      "          家电     0.0000    0.0000    0.0000        58\n",
      "          小说     0.6022    0.6431    0.6220       765\n",
      "        市场营销     0.0000    0.0000    0.0000       152\n",
      "          常识     0.4283    0.2850    0.3423       849\n",
      "          广告     0.0000    0.0000    0.0000        50\n",
      "          建筑     0.0000    0.0000    0.0000        80\n",
      "          影视     0.6860    0.8336    0.7526      2710\n",
      "        心理健康     0.4695    0.4927    0.4808      1981\n",
      "          思维     0.0000    0.0000    0.0000        13\n",
      "         性侵犯     0.0000    0.0000    0.0000         6\n",
      "         性生活     0.5309    0.6429    0.5816       815\n",
      "         性骚扰     0.0000    0.0000    0.0000         5\n",
      "          恋爱     0.4230    0.8367    0.5619      1941\n",
      "        恐怖主义     0.0000    0.0000    0.0000         7\n",
      "          恶俗     0.0000    0.0000    0.0000         8\n",
      "          情商     0.0000    0.0000    0.0000        21\n",
      "          情感     0.2115    0.0149    0.0278       738\n",
      "          惊悚     0.0000    0.0000    0.0000         7\n",
      "          成长     0.0000    0.0000    0.0000        88\n",
      "          战争     0.0000    0.0000    0.0000        50\n",
      "         房地产     0.0000    0.0000    0.0000        48\n",
      "          抄袭     0.0000    0.0000    0.0000        18\n",
      "          摄影     0.6729    0.7146    0.6931       452\n",
      "          故事     0.0000    0.0000    0.0000       109\n",
      "       教育/科学     0.6721    0.7236    0.6969      7188\n",
      "       文化/艺术     0.5715    0.5729    0.5722      3320\n",
      "          旅游     0.6853    0.7141    0.6994       997\n",
      "        时事政治     0.5300    0.5102    0.5199      1176\n",
      "          时尚     0.5958    0.6017    0.5988       811\n",
      "          明星     0.5367    0.6274    0.5785       840\n",
      "          星座     0.5997    0.8704    0.7101       463\n",
      "          暴力     0.0000    0.0000    0.0000        10\n",
      "          期货     0.0000    0.0000    0.0000        16\n",
      "        校园生活     0.4980    0.5903    0.5403      1506\n",
      "          歧视     0.0000    0.0000    0.0000        13\n",
      "          死亡     0.0000    0.0000    0.0000        18\n",
      "          毒品     0.0000    0.0000    0.0000        19\n",
      "          民族     0.0000    0.0000    0.0000        34\n",
      "          汽车     0.8170    0.9088    0.8605      1130\n",
      "          法律     0.6495    0.6440    0.6467      1309\n",
      "          游戏     0.9106    0.9450    0.9275      9595\n",
      "         潜规则     0.0000    0.0000    0.0000         2\n",
      "        灵异灵修     0.0000    0.0000    0.0000        25\n",
      "        灾害意外     0.0000    0.0000    0.0000         7\n",
      "          炫富     0.0000    0.0000    0.0000         2\n",
      "          爱国     0.0000    0.0000    0.0000         6\n",
      "          爱情     0.0000    0.0000    0.0000       189\n",
      "          环境     0.0000    0.0000    0.0000        47\n",
      "          生活     0.4508    0.2682    0.3363       973\n",
      "        电子商务     0.8276    0.2051    0.3288       117\n",
      "        电子数码     0.7195    0.7534    0.7361      1379\n",
      "       电脑/网络     0.7876    0.8270    0.8068      4977\n",
      "          男性     0.0000    0.0000    0.0000        99\n",
      "          相貌     0.0000    0.0000    0.0000        12\n",
      "          睡眠     0.0000    0.0000    0.0000        35\n",
      "          社会     0.3889    0.3004    0.3390      1398\n",
      "          神话     0.0000    0.0000    0.0000         7\n",
      "          移民     0.0000    0.0000    0.0000        17\n",
      "          算命     0.0000    0.0000    0.0000        57\n",
      "          经济     0.4082    0.2867    0.3368       279\n",
      "          编程     0.5570    0.5744    0.5656       289\n",
      "        网络安全     0.0000    0.0000    0.0000        28\n",
      "       美容/塑身     0.6003    0.8209    0.6934      1167\n",
      "       美食/烹饪     0.6794    0.7843    0.7281      1270\n",
      "        职场职业     0.5452    0.5208    0.5327      1275\n",
      "          股票     0.8251    0.9255    0.8724      1422\n",
      "          育儿     0.6573    0.6988    0.6774      1202\n",
      "        航空航天     0.5825    0.5556    0.5687       108\n",
      "          色情     0.0000    0.0000    0.0000        37\n",
      "          语言     0.5375    0.5000    0.5181       358\n",
      "        财务税务     0.7816    0.7554    0.7683       597\n",
      "        购房置业     0.5634    0.7767    0.6531       206\n",
      "          购物     0.4412    0.1818    0.2575       165\n",
      "        资本主义     0.0000    0.0000    0.0000         4\n",
      "        资源共享     0.0000    0.0000    0.0000        33\n",
      "          赚钱     0.0000    0.0000    0.0000        15\n",
      "        道德伦理     0.0000    0.0000    0.0000        96\n",
      "          金融     0.0000    0.0000    0.0000        16\n",
      "          银行     0.6692    0.7032    0.6858       374\n",
      "         青春期     0.0000    0.0000    0.0000         3\n",
      "          音乐     0.7032    0.8489    0.7692      1496\n",
      "          风水     0.0000    0.0000    0.0000        23\n",
      "          食品     0.0000    0.0000    0.0000        44\n",
      "       马克思主义     0.0000    0.0000    0.0000         8\n",
      "          高铁     0.0000    0.0000    0.0000        37\n",
      "\n",
      "    accuracy                         0.6676     83069\n",
      "   macro avg     0.2940    0.2944    0.2870     83069\n",
      "weighted avg     0.6334    0.6676    0.6433     83069\n",
      " === 2\n",
      "0.7838784624829961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_score, gold = eval_all(v5_valid, topic_api, top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a33909b9-2441-4770-9797-8a51ce28f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_score, gold = eval_all(v4_knn_valid, topic_api, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80701227-d75e-4e55-83a9-d2f778b2ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_score, gold = eval_all(v4_valid, topic_api, top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e24f478-fd77-4609-a70c-5e7c027a7c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25507it [00:11, 2218.33it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/data/albert.xht/xiaodao/topic_classification_v7/biake_qa_web_text_zh_train.json.positive.topic', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/xiaodao/topic_classification_v7/biake_qa_web_text_zh_train.json.positive', 'r') as frobj:\n",
    "        queue = []\n",
    "        t = []\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('请问', '', content['text'])\n",
    "            queue.append(content['text'])\n",
    "            t.append(content)\n",
    "            if np.mod(len(queue), 32) == 0:\n",
    "                probs = topic_api.predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tt['label'] = prob_dict\n",
    "                    fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "                queue = []\n",
    "                t = []\n",
    "        if queue:\n",
    "            probs = topic_api.predict_batch(queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                tt['label'] = prob_dict\n",
    "                fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56e9453f-9799-4384-be3d-71c947fa57d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23116it [00:10, 2115.69it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/embed_linear_small_white.json.topic', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/embed_linear_small_white.json', 'r') as frobj:\n",
    "        queue = []\n",
    "        t = []\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('请问', '', content['text'])\n",
    "            queue.append(content['text'])\n",
    "            t.append(content)\n",
    "            if np.mod(len(queue), 32) == 0:\n",
    "                probs = topic_api.predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tt['label'] = prob_dict\n",
    "                    fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "                queue = []\n",
    "                t = []\n",
    "        if queue:\n",
    "            probs = topic_api.predict_batch(queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                tt['label'] = prob_dict\n",
    "                fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "54a00c12-49c1-402c-91eb-6f76a71e3df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24721it [00:13, 1896.10it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/query_risk_corpus.json.risk_topic', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/query_risk_corpus.json.risk', 'r') as frobj:\n",
    "        queue = []\n",
    "        t = []\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('请问', '', content['text'])\n",
    "            queue.append(content['text'])\n",
    "            t.append(content)\n",
    "            if np.mod(len(queue), 32) == 0:\n",
    "                probs = topic_api.predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tt['topic'] = prob_dict\n",
    "                    fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "                queue = []\n",
    "                t = []\n",
    "        if queue:\n",
    "            probs = topic_api.predict_batch(queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                tt['topic'] = prob_dict\n",
    "                fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80f79d4b-7a85-449a-9eb8-11af3ed0d98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2071401it [16:00, 2157.32it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json,new_topic', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json', 'r') as frobj:\n",
    "        queue = []\n",
    "        t = []\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('请问', '', content['text'])\n",
    "            queue.append(content['text'])\n",
    "            t.append(content)\n",
    "            if np.mod(len(queue), 32) == 0:\n",
    "                probs = topic_api.predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tt['new_topic'] = prob_dict\n",
    "                    fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "                queue = []\n",
    "                t = []\n",
    "        if queue:\n",
    "            probs = topic_api.predict_batch(queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                tt['new_topic'] = prob_dict\n",
    "                fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f851e7-c69b-4583-a8ba-39abe24dd7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241035it [02:58, 1348.68it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/data/albert.xht/sentiment/green_teenager.json.all.detail.topic', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/sentiment/green_teenager.json.all.detail', 'r') as frobj:\n",
    "        queue = []\n",
    "        t = []\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('请问', '', content['text'])\n",
    "            if len(content['text']) > 192:\n",
    "                continue\n",
    "            queue.append(content['text'])\n",
    "            t.append(content)\n",
    "            if np.mod(len(queue), 32) == 0:\n",
    "                probs = topic_api.predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tt['topic'] = prob_dict\n",
    "                    fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "                queue = []\n",
    "                t = []\n",
    "        if queue:\n",
    "            probs = topic_api.predict_batch(queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                tt['topic'] = prob_dict\n",
    "                fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec75ce3c-939a-4078-ab93-622f5578fd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:11, 1762.52it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "with open('/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8.topic', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8', 'r') as frobj:\n",
    "        queue = []\n",
    "        t = []\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('请问', '', content['text'])\n",
    "            title = content['title'] #''.join(re.split('[\\s,]', content['title'])[1:])\n",
    "            if len(title) > 192:\n",
    "                continue\n",
    "            queue.append(title)\n",
    "            t.append(content)\n",
    "            if np.mod(len(queue), 32) == 0:\n",
    "                probs = topic_api.predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tmp = {\n",
    "                        'text':text,\n",
    "                        'topic':prob_dict,\n",
    "                        'source':'efaqa'\n",
    "                    }\n",
    "                    fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')\n",
    "                queue = []\n",
    "                t = []\n",
    "        if queue:\n",
    "            probs = topic_api.predict_batch(queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                tmp = {\n",
    "                        'text':text,\n",
    "                        'topic':prob_dict,\n",
    "                        'source':'efaqa'\n",
    "                    }\n",
    "                fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e3ad02d-bab2-483a-89a2-1e9e7c54ab0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20641it [00:10, 1970.45it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "with open('/data/albert.xht/xiaodao/query_risk_v11/offensive_select_labeled.txt.topic', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/xiaodao/query_risk_v11/offensive_select_labeled.txt', 'r') as frobj:\n",
    "        queue = []\n",
    "        t = []\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('请问', '', content['text'])\n",
    "            title = content['text'] #''.join(re.split('[\\s,]', content['title'])[1:])\n",
    "            if len(title) > 192:\n",
    "                continue\n",
    "            queue.append(title)\n",
    "            t.append(content)\n",
    "            if np.mod(len(queue), 32) == 0:\n",
    "                probs = topic_api.predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tmp = {\n",
    "                        'text':text,\n",
    "                        'topic':prob_dict,\n",
    "                        'label':tt['label'],\n",
    "                        'source':'efaqa'\n",
    "                    }\n",
    "                    fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')\n",
    "                queue = []\n",
    "                t = []\n",
    "        if queue:\n",
    "            probs = topic_api.predict_batch(queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                tmp = {\n",
    "                        'text':text,\n",
    "                        'topic':prob_dict,\n",
    "                        'label':tt['label'],\n",
    "                        'source':'efaqa'\n",
    "                    }\n",
    "                fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "278e1b29-5771-4b72-b24e-1409ce4bf106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/疑似有风险query_from对话预训练数据-20221130.txt') as frobj:\n",
    "    for line in tqdm(frobj):\n",
    "        text = line.strip()\n",
    "        result_list.append((text, result))\n",
    "\n",
    "topic_filter = []\n",
    "topic_white = []\n",
    "left = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/query_risk_corpus.json.risk_topic') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['score_list']['senti'][1][1] > 0.7:\n",
    "            if content['topic'][0][0] in ['经济', '历史', 'LGBT', 'BDSM', '法律', '灵异灵修', '国家', '社会', '军事', '心理健康', '时事政治', '死亡', '毒品', '恐怖主义', '战争', '灵异事件']:\n",
    "                topic_filter.append(content)\n",
    "            else:\n",
    "                if '艾滋病' in content['text'] or '抑郁' in content['text']:\n",
    "                    topic_filter.append(content)\n",
    "                else:\n",
    "                    topic_white.append(content)\n",
    "        else:\n",
    "            topic_filter.append(content)\n",
    "        # if content['topic'][0][0] in ['文化/艺术', '动漫', '音乐', '娱乐', '体育/运动', '小说', '星座', '时尚', '健康', '明星', '游戏'] and content['topic'][0][1] > 0.2:\n",
    "        #     if content['score_list']['senti'][0][1] < 0.3:\n",
    "        #         topic_white.append(content)\n",
    "        #     elif content['score_list']['senti'][0][1] > 0.9:\n",
    "        #         topic_filter.append(content)\n",
    "        # elif content['topic'][0][0] in ['LGBT', 'BDSM', '法律', '灵异灵修', '国家', '社会', '军事', '心理健康', '时事政治', '死亡', '毒品', '恐怖主义', '战争', '灵异事件']:\n",
    "        #     topic_filter.append(content)\n",
    "        # elif content['topic'][1][0] in ['LGBT', 'BDSM', '法律', '灵异灵修', '国家', '社会', '军事', '心理健康', '时事政治', '死亡', '毒品', '恐怖主义', '战争', '灵异事件']:\n",
    "        #     topic_filter.append(content)\n",
    "        # else:\n",
    "        #     left.append(content)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5278721d-7cb6-4c64-8889-812254a6f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/query_risk_corpus.json.filter', 'w') as fwobj:\n",
    "    for d in topic_filter:\n",
    "        d['label'] = ['风险']\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/query_risk_corpus.json.white', 'w') as fwobj:\n",
    "    for d in topic_white:\n",
    "        d['label'] = ['正常']\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d585b1d0-9c3d-4403-b611-06676e88525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.extend(['/root/xiaoda/query_topic/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "840c0f06-8bd8-4a04-b5dc-4a3a735977f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyword_processor import KeywordProcesser\n",
    "with open('/data/albert.xht/xiaoda/sentiment/query_risk_v14//risk_event.txt') as frobj:\n",
    "    for line in frobj:\n",
    "        content = line.strip()\n",
    "    keyword_list = content.split('||')\n",
    "\n",
    "keyword_api_v1 = KeywordProcesser(keywords=keyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7682e6b3-655c-429a-80d3-20ad6e16e3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1973027it [03:03, 10726.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import jieba_fast as jieba\n",
    "for word in keyword_list:\n",
    "    jieba.add_word(word)\n",
    "    \n",
    "from tqdm import tqdm\n",
    "import json, re\n",
    "\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v5/biake_qa_web_text_zh_train.json.knn.final.keyword', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/raw_chat_corpus/topic_classification_v5/biake_qa_web_text_zh_train.json.knn.final') as frobj:\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            text = re.sub(u'[^\\u4e00-\\u9fa50-9a-zA-Z ]+', '\\n', content['text'].lower())\n",
    "            content['keywords'] = keyword_api_v1.extract_keywords(text)\n",
    "            words_list = list(jieba.cut(text))\n",
    "            words_set = {}\n",
    "            for word in words_list:\n",
    "                words_set[word] = ''\n",
    "            if content['keywords']:\n",
    "                keyword_list = []\n",
    "                for keyword_ in content['keywords']:\n",
    "                    if keyword_[-1] in words_set:\n",
    "                        keyword_list.append(keyword_)\n",
    "                if keyword_list:\n",
    "                    if content['label'][0] not in ['游戏', '小说', '网络安全', '动漫', '幽默滑稽', '电子数码']:\n",
    "                        content['topic'] = content['label']\n",
    "                        content['label'] = ['风险']\n",
    "                        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "                    elif content['label'][0] in ['游戏', '小说', '网络安全', '动漫', '电子数码']:\n",
    "                        content['topic'] = content['label']\n",
    "                        content['label'] = ['正常']\n",
    "                        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364778bf-42c7-4480-8afd-e9f47d72fb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['j', 'av', 'aweb', '程序员', '是否', '适合', '用', 'MacBook']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(jieba.cut('javaweb程序员是否适合用MacBook'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e295b-b06d-406b-b24d-7b90e2e64847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
