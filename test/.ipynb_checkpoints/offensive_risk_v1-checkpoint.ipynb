{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f046be31-01e8-4154-b613-8eca6a0f1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys,os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f518fb8f-7b68-4aa6-a4a2-cb1471012a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.extend(['/root/xiaoda/query_topic/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d41337-e70e-407e-a5ef-8e2992c5574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/ondrejbohdal/meta-calibration/blob/main/Metrics/metrics.py\n",
    "\"\"\"\n",
    "\n",
    "class ECE(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(ECE, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, logits, labels, mode='logits'):\n",
    "        if mode == 'logits':\n",
    "            softmaxes = F.softmax(logits, dim=1)\n",
    "        else:\n",
    "            softmaxes = logits\n",
    "        # softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "        \n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60293fec-05fa-4682-81c3-401c53b85c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast\n",
    "import transformers\n",
    "from datetime import timedelta\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from nets.them_classifier import MyBaseModel, RobertaClassifier\n",
    "\n",
    "import configparser\n",
    "from tqdm import tqdm\n",
    "\n",
    "cur_dir_path = '/root/xiaoda/query_topic/'\n",
    "\n",
    "def load_label(filepath):\n",
    "    label_list = []\n",
    "    with open(filepath, 'r') as frobj:\n",
    "        for line in frobj:\n",
    "            label_list.append(line.strip())\n",
    "        n_classes = len(label_list)\n",
    "\n",
    "        label2id = {}\n",
    "        id2label = {}\n",
    "        for idx, label in enumerate(label_list):\n",
    "            label2id[label] = idx\n",
    "            id2label[idx] = label\n",
    "        return label2id, id2label\n",
    "\n",
    "class RiskInfer(object):\n",
    "    def __init__(self, config_path):\n",
    "\n",
    "        import torch, os, sys\n",
    "\n",
    "        con = configparser.ConfigParser()\n",
    "        con_path = os.path.join(cur_dir_path, config_path)\n",
    "        con.read(con_path, encoding='utf8')\n",
    "\n",
    "        args_path = dict(dict(con.items('paths')), **dict(con.items(\"para\")))\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(args_path[\"model_path\"], do_lower_case=True)\n",
    "\n",
    "        from collections import OrderedDict\n",
    "        self.schema_dict = OrderedDict({})\n",
    "\n",
    "        for label_index, schema_info in enumerate(args_path[\"label_path\"].split(',')):\n",
    "            schema_type, schema_path = schema_info.split(':')\n",
    "            schema_path = os.path.join(cur_dir_path, schema_path)\n",
    "            print(schema_type, schema_path, '===schema-path===')\n",
    "            label2id, id2label = load_label(schema_path)\n",
    "            self.schema_dict[schema_type] = {\n",
    "                'label2id':label2id,\n",
    "                'id2label':id2label,\n",
    "                'label_index':label_index\n",
    "            }\n",
    "            print(self.schema_dict[schema_type], '==schema_type==', schema_type)\n",
    "        \n",
    "        output_path = os.path.join(cur_dir_path, args_path['output_path'])\n",
    "\n",
    "        from roformer import RoFormerModel, RoFormerConfig\n",
    "\n",
    "        config = RoFormerConfig.from_pretrained(args_path[\"model_path\"])\n",
    "        encoder = RoFormerModel(config=config)\n",
    "        \n",
    "        encoder_net = MyBaseModel(encoder, config)\n",
    "\n",
    "        self.device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        classifier_list = []\n",
    "\n",
    "        schema_list = list(self.schema_dict.keys())\n",
    "\n",
    "        for schema_key in schema_list:\n",
    "            classifier = RobertaClassifier(\n",
    "                hidden_size=config.hidden_size, \n",
    "                dropout_prob=con.getfloat('para', 'out_dropout_rate'),\n",
    "                num_labels=len(self.schema_dict[schema_key]['label2id']), \n",
    "                dropout_type=con.get('para', 'dropout_type'))\n",
    "            classifier_list.append(classifier)\n",
    "\n",
    "        classifier_list = nn.ModuleList(classifier_list)\n",
    "\n",
    "        class MultitaskClassifier(nn.Module):\n",
    "            def __init__(self, transformer, classifier_list):\n",
    "                super().__init__()\n",
    "\n",
    "                self.transformer = transformer\n",
    "                self.classifier_list = classifier_list\n",
    "\n",
    "            def forward(self, input_ids, input_mask, \n",
    "                        segment_ids=None, \n",
    "                        transformer_mode='mean_pooling', \n",
    "                        dt_idx=None, mode='predict'):\n",
    "                hidden_states = self.transformer(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              return_mode=transformer_mode)\n",
    "                outputs_list = []\n",
    "                \n",
    "                for idx, classifier in enumerate(self.classifier_list):\n",
    "                    \n",
    "                    if dt_idx:\n",
    "                        if idx not in dt_idx:\n",
    "                            continue\n",
    "                    \n",
    "                    scores = classifier(hidden_states)\n",
    "                    if mode == 'predict':\n",
    "                        scores = torch.nn.Softmax(dim=1)(scores)\n",
    "                    outputs_list.append(scores)\n",
    "                return outputs_list, hidden_states\n",
    "\n",
    "        self.net = MultitaskClassifier(encoder_net, classifier_list).to(self.device)\n",
    "\n",
    "        # eo = 9\n",
    "        # ckpt = torch.load(os.path.join(output_path, 'multitask_cls.pth.{}.raw'.format(eo)), map_location=self.device)\n",
    "        # # ckpt = torch.load(os.path.join(output_path, 'multitask_cls.pth.{}.raw.focal'.format(eo)), map_location=self.device)\n",
    "        # # ckpt = torch.load(os.path.join(output_path, 'multitask_contrast_cls.pth.{}'.format(eo)), map_location=self.device)\n",
    "        # self.net.load_state_dict(ckpt)\n",
    "        # self.net.eval()\n",
    "        \n",
    "    def reload(self, model_path):\n",
    "        ckpt = torch.load(model_path, map_location=self.device)\n",
    "        self.net.load_state_dict(ckpt)\n",
    "        self.net.eval()\n",
    "\n",
    "    def predict(self, text, allowed_schema_type_ids={}):\n",
    "\n",
    "        \"\"\"抽取输入text所包含的类型\n",
    "        \"\"\"\n",
    "        encoder_txt = self.tokenizer.encode_plus(text, max_length=256)\n",
    "        input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().unsqueeze(0).to(self.device)\n",
    "        token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).unsqueeze(0).to(self.device)\n",
    "        attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        scores_dict = {}\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states] = self.net(input_ids, \n",
    "                attention_mask, token_type_ids, transformer_mode='cls', dt_idx=allowed_schema_type_ids)\n",
    "        for schema_idx, (schema_type, scores) in enumerate(zip(list(self.schema_dict.keys()), logits_list)):\n",
    "            print(scores, allowed_schema_type_ids, schema_idx)\n",
    "            if allowed_schema_type_ids:\n",
    "                if schema_idx not in allowed_schema_type_ids:\n",
    "                    continue\n",
    "            # scores = torch.nn.Softmax(dim=1)(logits)[0].data.cpu().numpy()\n",
    "            scores = scores[0].data.cpu().numpy()\n",
    "            scores_dict[schema_type] = []\n",
    "            for index, score in enumerate(scores):\n",
    "                scores_dict[schema_type].append([self.schema_dict[schema_type]['id2label'][index], \n",
    "                                        float(score)])\n",
    "        return scores_dict\n",
    "    \n",
    "    def get_logitnorm(self, text):\n",
    "        \"\"\"抽取输入text所包含的类型\n",
    "        \"\"\"\n",
    "        encoder_txt = self.tokenizer.encode_plus(text, max_length=256)\n",
    "        input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().unsqueeze(0).to(self.device)\n",
    "        token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).unsqueeze(0).to(self.device)\n",
    "        attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        scores_dict = {}\n",
    "        logits_norm_list = []\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states] = self.net(input_ids, \n",
    "                attention_mask, token_type_ids, transformer_mode='cls')\n",
    "            for logits in logits_list:\n",
    "                logits_norm_list.append(logits/torch.norm(logits, p=2, dim=-1, keepdim=True) + 1e-7)\n",
    "        for schema_type, logit_norm in zip(list(self.schema_dict.keys()), logits_norm_list):\n",
    "            scores_dict[schema_type] = logit_norm[0].data.cpu().numpy()\n",
    "        return scores_dict\n",
    "            \n",
    "    \n",
    "    def predict_batch(self, text, allowed_schema_type_ids={}):\n",
    "        if isinstance(text, list):\n",
    "            text_list = text\n",
    "        else:\n",
    "            text_list = [text]\n",
    "        model_input = self.tokenizer(text_list, return_tensors=\"pt\",padding=True)\n",
    "        for key in model_input:\n",
    "            model_input[key] = model_input[key].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states] = self.net(model_input['input_ids'], \n",
    "                model_input['attention_mask'], \n",
    "                model_input['token_type_ids'], transformer_mode='cls', dt_idx=allowed_schema_type_ids)\n",
    "        score_dict_list = []\n",
    "        for idx, text in enumerate(text_list):\n",
    "            scores_dict = {}\n",
    "            for schema_idx, (schema_type, scores) in enumerate(zip(list(self.schema_dict.keys()), logits_list)):\n",
    "                if allowed_schema_type_ids:\n",
    "                    if schema_idx not in allowed_schema_type_ids:\n",
    "                        continue\n",
    "                # scores = torch.nn.Softmax(dim=1)(logits)[idx].data.cpu().numpy()\n",
    "                scores = scores[idx].data.cpu().numpy()\n",
    "                scores_dict[schema_type] = []\n",
    "                for index, score in enumerate(scores):\n",
    "                    scores_dict[schema_type].append([self.schema_dict[schema_type]['id2label'][index], \n",
    "                                            float(score)])\n",
    "            score_dict_list.append(scores_dict)\n",
    "        return score_dict_list\n",
    "\n",
    "# risk_api = RiskInfer('./risk_data/config.ini')\n",
    "# risk_api = RiskInfer('./risk_data_v5/config_offensive_risk.ini')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9630ac4-7b63-4405-8a6b-ff547cdf973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_filter_senti_query_risk_v11_offensive_v5/multitask_cls.pth.9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "750b78de-71d3-44cf-bcc1-e1e1a0c135be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senti_query /data/albert.xht/xiaoda/sentiment/senti/senti_query_label.txt ===schema-path===\n",
      "{'label2id': {'负向': 0, '中性': 1, '正向': 2}, 'id2label': {0: '负向', 1: '中性', 2: '正向'}, 'label_index': 0} ==schema_type== senti_query\n",
      "senti /data/albert.xht/xiaoda/sentiment/senti/senti_label.txt ===schema-path===\n",
      "{'label2id': {'负向': 0, '正向': 1}, 'id2label': {0: '负向', 1: '正向'}, 'label_index': 1} ==schema_type== senti\n",
      "bias /data/albert.xht/xiaoda/sentiment/bias/bias_label.txt ===schema-path===\n",
      "{'label2id': {'偏见': 0, '正常': 1}, 'id2label': {0: '偏见', 1: '正常'}, 'label_index': 2} ==schema_type== bias\n",
      "ciron /data/albert.xht/xiaoda/sentiment/ciron/ciron_label.txt ===schema-path===\n",
      "{'label2id': {'讽刺': 0, '正常': 1}, 'id2label': {0: '讽刺', 1: '正常'}, 'label_index': 3} ==schema_type== ciron\n",
      "intent /data/albert.xht/xiaoda/sentiment/intention_data_v2-1/label.txt ===schema-path===\n",
      "{'label2id': {'主观评价/比较/判断': 0, '寻求建议/帮助': 1, '其它': 2}, 'id2label': {0: '主观评价/比较/判断', 1: '寻求建议/帮助', 2: '其它'}, 'label_index': 4} ==schema_type== intent\n",
      "offensive /data/albert.xht/xiaoda/sentiment/offensive/offensive_label.txt ===schema-path===\n",
      "{'label2id': {'冒犯': 0, '正常': 1}, 'id2label': {0: '冒犯', 1: '正常'}, 'label_index': 5} ==schema_type== offensive\n",
      "query_risk /data/albert.xht/xiaoda/sentiment/query_risk_v12/query_risk_label.txt ===schema-path===\n",
      "{'label2id': {'风险': 0, '个人信息': 1, '正常': 2}, 'id2label': {0: '风险', 1: '个人信息', 2: '正常'}, 'label_index': 6} ==schema_type== query_risk\n",
      "teenager /data/albert.xht/xiaoda/sentiment/teenager//teenager_label.txt ===schema-path===\n",
      "{'label2id': {'不良': 0, '正常': 1}, 'id2label': {0: '不良', 1: '正常'}, 'label_index': 7} ==schema_type== teenager\n",
      "politics /data/albert.xht/xiaoda/sentiment/green_politics/green_politics_label.txt ===schema-path===\n",
      "{'label2id': {'涉政': 0, '正常': 1}, 'id2label': {0: '涉政', 1: '正常'}, 'label_index': 8} ==schema_type== politics\n",
      "porn /data/albert.xht/xiaoda/sentiment/green_porn/green_porn_label.txt ===schema-path===\n",
      "{'label2id': {'色情': 0, '正常': 1}, 'id2label': {0: '色情', 1: '正常'}, 'label_index': 9} ==schema_type== porn\n",
      "abusive /data/albert.xht/xiaoda/sentiment/green_abusive/green_abusive_label.txt ===schema-path===\n",
      "{'label2id': {'辱骂': 0, '正常': 1}, 'id2label': {0: '辱骂', 1: '正常'}, 'label_index': 10} ==schema_type== abusive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/20/2023 09:13:48 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "01/20/2023 09:13:48 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "01/20/2023 09:13:48 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "01/20/2023 09:13:48 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "01/20/2023 09:13:48 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "01/20/2023 09:13:48 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "01/20/2023 09:13:48 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "01/20/2023 09:13:48 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "01/20/2023 09:13:48 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "01/20/2023 09:13:48 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "01/20/2023 09:13:48 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n"
     ]
    }
   ],
   "source": [
    "green_risk_api = RiskInfer('./risk_data_v5/config_offensive_risk_green.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735d0a05-53ef-4342-8ec4-ce0b858ab551",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_risk_api_base = RiskInfer('./risk_data_v5/config_offensive_risk_green_base.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a87e2a0-277f-44ad-a6e3-fd01b111c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_risk_api_base.reload('/data/albert.xht/xiaodao/risk_classification/multitask_raw_filter_senti_query_risk_v12_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_v17_base_3090/multitask_cls.pth.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccfff6f0-890f-43d1-8957-adbbac958059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# green_risk_api.reload('/data/albert.xht/xiaodao/risk_classification/multitask_raw_filter_senti_query_risk_v12_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_v6/multitask_cls.pth.6')\n",
    "# green_risk_api.reload('/data/albert.xht/xiaodao/risk_classification/multitask_raw_filter_senti_query_risk_v12_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_v14/multitask_cls.pth.9')\n",
    "\n",
    "# green_risk_api.reload('/data/albert.xht/xiaodao/risk_classification/multitask_raw_filter_senti_query_risk_v12_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_v18_base_distill/multitask_cls.pth.9')\n",
    "green_risk_api.reload('/data/albert.xht/xiaodao/risk_classification/multitask_raw_filter_senti_query_risk_v12_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_v18/multitask_cls.pth.9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5ddefb3-9f17-446a-a972-fe3d8f2f11a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def eval_all(data, model, key):\n",
    "    pred = []\n",
    "    gold = []\n",
    "    pred_score = []\n",
    "    for item in tqdm(data):\n",
    "        gold.append(item['label'][0])\n",
    "        if isinstance(item['text'], list):\n",
    "            text = \"\\n\".join(item['text'])\n",
    "        else:\n",
    "            text = item['text']\n",
    "        text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", text)   # 合并正文中过多的空格\n",
    "\n",
    "        result = model.predict(text)\n",
    "        score = sorted(result[key], key=lambda u:u[1], reverse=True)\n",
    "        pred.append(score[0][0])\n",
    "        pred_score.append(result[key])\n",
    "    print(classification_report(gold, pred, digits=4))\n",
    "    return pred, gold, pred_score\n",
    "    \n",
    "\n",
    "def evaluation_ece(pred_score, gold):\n",
    "    pred_score_l = []\n",
    "    mapping_dict = {}\n",
    "    for item in pred_score:\n",
    "        pred_score_l.append([])\n",
    "        for idx, p in enumerate(item):\n",
    "            if p[0] not in mapping_dict:\n",
    "                mapping_dict[p[0]] = idx\n",
    "            pred_score_l[-1].append(p[1])\n",
    "    pred_score_l = torch.tensor(pred_score_l)\n",
    "    gold_l = torch.tensor([mapping_dict[item] for item in gold])\n",
    "\n",
    "    ece_fn = ECE(n_bins=15)\n",
    "    print(ece_fn(pred_score_l, gold_l, mode='probs'), '==ece==')\n",
    "# pred, gold, pred_score = eval_all(offensive_test, risk_api, 'offensive')\n",
    "# evaluation_ece(pred_score, gold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0af67ed-21cd-4e47-bde1-ba44b75551f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5304/5304 [00:48<00:00, 109.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          冒犯     0.7169    0.8803    0.7903      2106\n",
      "          正常     0.9073    0.7711    0.8337      3198\n",
      "\n",
      "    accuracy                         0.8145      5304\n",
      "   macro avg     0.8121    0.8257    0.8120      5304\n",
      "weighted avg     0.8317    0.8145    0.8164      5304\n",
      "\n",
      "tensor([0.0971]) ==ece==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "offensive = []\n",
    "with open('/data/albert.xht/sentiment/test/offensive_cold.json') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        offensive.append(content)\n",
    "        \n",
    "pred, gold, pred_score = eval_all(offensive, green_risk_api, 'offensive')\n",
    "evaluation_ece(pred_score, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "fcd45df8-0bc8-4c01-811a-371c0ace9960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 6362/20641 [01:44<03:55, 60.62it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23165/1768068845.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#         risk_query.append(p)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mrisk_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrisk_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreen_risk_api\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'query_risk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mevaluation_ece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23165/2637395463.py\u001b[0m in \u001b[0;36meval_all\u001b[0;34m(data, model, key)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 合并正文中过多的空格\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23165/1460835750.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             [logits_list, \n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                 attention_mask, token_type_ids, transformer_mode='cls')\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mschema_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23165/1460835750.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, input_mask, segment_ids, transformer_mode, dt_idx)\u001b[0m\n\u001b[1;32m    106\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                     \u001b[0mce_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                     \u001b[0moutputs_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mce_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/xiaoda/query_topic/nets/them_classifier.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mlast_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mlast_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "risk_query = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/offensive_select_labeled.txt.paraphrase') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        # for d in content['paraphrase']:\n",
    "        #     p = {\n",
    "        #         'text':d[0],\n",
    "        #         'label':content['label']\n",
    "        #     }\n",
    "        #     if d:\n",
    "        #         risk_query.append(p)\n",
    "        risk_query.append(json.loads(line.strip()))\n",
    "pred, gold, pred_score = eval_all(risk_query, green_risk_api, 'query_risk')\n",
    "evaluation_ece(pred_score, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe5a04-2a44-4735-af20-513ab787e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "an = []\n",
    "for pred_, gold_, text in zip(pred, gold, risk_query):\n",
    "    if pred_ != gold_:\n",
    "        an.append((pred_, gold_, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e74fbec0-df93-48b1-b924-21d5c07abc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20641/20641 [03:07<00:00, 110.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          正常     0.6708    0.6396    0.6548      5514\n",
      "          风险     0.8708    0.8856    0.8781     15127\n",
      "\n",
      "    accuracy                         0.8199     20641\n",
      "   macro avg     0.7708    0.7626    0.7665     20641\n",
      "weighted avg     0.8174    0.8199    0.8185     20641\n",
      "\n",
      "tensor([0.0514]) ==ece==\n"
     ]
    }
   ],
   "source": [
    "risk_query = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/offensive_select_labeled.txt.paraphrase') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        # for d in content['paraphrase']:\n",
    "        #     p = {\n",
    "        #         'text':d[0],\n",
    "        #         'label':content['label']\n",
    "        #     }\n",
    "        #     if d:\n",
    "        #         risk_query.append(p)\n",
    "        risk_query.append(json.loads(line.strip()))\n",
    "pred, gold, pred_score = eval_all(risk_query, green_risk_api, 'query_risk')\n",
    "evaluation_ece(pred_score, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83eba20d-f040-4f5c-a26f-3d3aaa02a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json, re\n",
    "\n",
    "def risk_predict_batch(risk_api, text):\n",
    "    if isinstance(text, list):\n",
    "        text_list = text\n",
    "    else:\n",
    "        text_list = [text]\n",
    "    result_list = risk_api.predict_batch(text_list)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af8e6fa8-c7fc-41c9-9cc5-5de8a62cadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(risk_api, input_path, output_path):\n",
    "    queue = []\n",
    "    t = []\n",
    "    from collections import Counter\n",
    "    pppp = Counter()\n",
    "    with open(output_path, 'w') as fwobj:\n",
    "        with open(input_path) as frobj:\n",
    "            for idx, line in tqdm(enumerate(frobj)):\n",
    "                if idx == 0:\n",
    "                    continue\n",
    "                content = json.loads(line.strip())\n",
    "                text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", content['text'])   # 合并正文中过多的空格\n",
    "                # if content['label'] not in ['black']:\n",
    "                #     continue\n",
    "                if len(text) >= 164:\n",
    "                    text = text[:164]\n",
    "                queue.append(text)\n",
    "                t.append(content)\n",
    "                if np.mod(len(queue), 128) == 0 and queue:\n",
    "                    probs = risk_predict_batch(risk_api, queue)\n",
    "                    for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                        tt['score_list'] = prob_dict\n",
    "                        fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "                    queue = []\n",
    "                    t = []\n",
    "            if queue:\n",
    "                probs = risk_predict_batch(risk_api, queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tt['score_list'] = prob_dict\n",
    "                    fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d89d0db-c716-4c48-af0a-e545105da35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19655it [00:40, 488.10it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8.other'\n",
    "output_path = '/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8.other.green'\n",
    "\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1d52cef2-e2e6-4488-9759-65850f671b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "242576it [06:15, 646.82it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_path = '/data/albert.xht/sentiment/green_teenager.json'\n",
    "output_path = '/data/albert.xht/sentiment/green_teenager.json.offensive_query_risk.green'\n",
    "\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400452e-35a6-452a-832d-6ef0f270d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/data/albert.xht/sentiment/green_teenager.json'\n",
    "output_path = '/data/albert.xht/sentiment/green_teenager.json.offensive_query_risk.green'\n",
    "\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d6b02c86-b652-4018-a445-6bc5be6eb2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44565it [00:52, 845.15it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final.keyword'\n",
    "output_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final.keyword.green'\n",
    "\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "daffe887-6a3f-48aa-91c6-f57cd195cc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2071401it [40:15, 857.44it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final'\n",
    "output_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final.green.v18'\n",
    "\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49434f70-8db7-4351-a28a-6fec0e45697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/baike2018qa/baike_qa_train.json') as frobj:\n",
    "    data_list = []\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['category'] in ['娱乐-博彩']:\n",
    "            data_list.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "665ff045-8cfa-43b1-9747-2607f53b5b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46124"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1db57781-5008-42a1-bae3-ae496effbb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.4775e-05, 3.7232e-02, 9.6271e-01]], device='cuda:2') {4: '', 6: '', 9: '', 10: ''}\n",
      "tensor([[9.6828e-01, 3.5527e-06, 3.1714e-02]], device='cuda:2') {4: '', 6: '', 9: '', 10: ''}\n",
      "tensor([[0.0053, 0.9947]], device='cuda:2') {4: '', 6: '', 9: '', 10: ''}\n",
      "tensor([[0.0067, 0.9933]], device='cuda:2') {4: '', 6: '', 9: '', 10: ''}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "green_risk_api.predict('吃冰毒', allowed_schema_type_ids={\n",
    "    4:'',6:'',9:'', 10:''\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "8e506834-4d5b-464b-bc6c-c726b39ea278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "728266it [19:20, 627.32it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/data/albert.xht/xiaoda/sentiment/green_porn/green_porn.json.disu'\n",
    "output_path = '/data/albert.xht/xiaoda/sentiment/green_porn/green_porn.json.disu.v18'\n",
    "\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e09a3-bc44-47ae-8ad9-272383817a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/data/albert.xht/xiaoda/sentiment/green_porn/green_porn.json.disu.v18'\n",
    "with open(output_path) as frobj:\n",
    "    for line in tqdm(frobj):\n",
    "        content = json.loads(line.strip())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "3f1f776c-8548-4465-9763-2f267349c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25663it [00:38, 659.46it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/data/albert.xht/sentiment/offensive_cold.json'\n",
    "output_path = '/data/albert.xht/sentiment/offensive_cold.json.v18'\n",
    "\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "20b5d91c-d17e-4afd-bed2-57843bf29d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42195it [00:49, 854.22it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/smal_white_positive.json.filter.offensive_query_risk'\n",
    "output_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/smal_white_positive.json.filter.offensive_query_risk.v18'\n",
    "\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "9751e0ec-7d60-4d34-9bdc-52f4d127b565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2071400it [03:54, 8827.78it/s]\n"
     ]
    }
   ],
   "source": [
    "output_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final.green.v18'\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final.green.v18.white', 'w') as fwobj:\n",
    "    with open(output_path) as frobj:\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            if content['score_list']['query_risk'][0][1] < 0.3 and  (content['score_list']['senti_query'][0][1] < 0.3 or content['score_list']['senti'][0][1] < 0.3)\\\n",
    "            and (content['score_list']['porn'][0][1] < 0.5 and content['score_list']['abusive'][0][1] < 0.5)\\\n",
    "            and (content['score_list']['offensive'][0][1] < 0.5 and content['score_list']['politics'][0][1] < 0.5\\\n",
    "                and content['score_list']['teenager'][0][1] < 0.5):\n",
    "                text = re.sub(u'[^\\u4e00-\\u9fa50-9a-zA-Z ]+', '\\n', content['text'].lower())\n",
    "                content['keywords'] = keyword_api.extract_keywords(text)\n",
    "                if content['keywords']:\n",
    "                    continue\n",
    "                d = {\n",
    "                    'text':content['text'],\n",
    "                    'label':['正常'],\n",
    "                    'source':'topic_query_risk_gren_v18',\n",
    "                    'topic':content['label'],\n",
    "                    'score_list':content['score_list']\n",
    "                }\n",
    "                fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "            elif content['score_list']['query_risk'][0][1] < 0.3 and  (content['score_list']['senti_query'][0][1] > 0.5 or content['score_list']['senti'][0][1] > 0.5)\\\n",
    "                and (content['score_list']['porn'][0][1] < 0.5 and content['score_list']['abusive'][0][1] < 0.5)\\\n",
    "                and (content['score_list']['offensive'][0][1] < 0.5 and content['score_list']['politics'][0][1] < 0.5\\\n",
    "                    and content['score_list']['teenager'][0][1] < 0.5):\n",
    "                text = re.sub(u'[^\\u4e00-\\u9fa50-9a-zA-Z ]+', '\\n', content['text'].lower())\n",
    "                content['keywords'] = keyword_api.extract_keywords(text)\n",
    "                if content['keywords']:\n",
    "                    continue\n",
    "                d = {\n",
    "                    'text':content['text'],\n",
    "                    'label':['正常'],\n",
    "                    'source':'topic_query_risk_gren_v18',\n",
    "                    'topic':content['label'],\n",
    "                    'score_list':content['score_list']\n",
    "                }\n",
    "                fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c758fc2-6ca1-4d42-af48-72d710c19150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{} 0.008749961853027344\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print(green_risk_api.predict('我想拉屎', {\n",
    "    4:'', 6:'', 9:'', 10:''\n",
    "}), time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "63ef4711-fcdc-4fd3-9c4f-ec364bf760bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyword_processor import KeywordProcesser\n",
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/risk_event.txt') as frobj:\n",
    "    for line in frobj:\n",
    "        content = line.strip()\n",
    "    keyword_list = content.split('||')\n",
    "\n",
    "keyword_api = KeywordProcesser(keywords=keyword_list)\n",
    "keyword_api.add_keyword('性关系')\n",
    "keyword_api.add_keyword('手淫')\n",
    "keyword_api.add_keyword('性生活')\n",
    "keyword_api.add_keyword('性能力')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "36cfa521-e26d-45a1-b630-ce09d0c1ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/smal_white_positive.json.filter.offensive_query_risk.v18.white', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/smal_white_positive.json.filter.offensive_query_risk.v18') as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            if content['label'][0] in ['正常']:\n",
    "                if content['score_list']['query_risk'][0][1] < 0.5 and  (content['score_list']['senti_query'][0][1] < 0.3 or content['score_list']['senti'][0][1] < 0.3)\\\n",
    "            and (content['score_list']['porn'][0][1] < 0.5 and content['score_list']['abusive'][0][1] < 0.5)\\\n",
    "            and (content['score_list']['offensive'][0][1] < 0.5 and content['score_list']['politics'][0][1] < 0.5\\\n",
    "                and content['score_list']['teenager'][0][1] < 0.5):\n",
    "                    text = re.sub(u'[^\\u4e00-\\u9fa50-9a-zA-Z ]+', '\\n', content['text'].lower())\n",
    "                    content['keywords'] = keyword_api.extract_keywords(text)\n",
    "                    if content['keywords']:\n",
    "                        continue\n",
    "                    d = {\n",
    "                        'text':content['text'],\n",
    "                        'label':['正常'],\n",
    "                        'source':'offensive_query_risk_gren_v18'\n",
    "                    }\n",
    "                    fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "                elif content['score_list']['query_risk'][0][1] < 0.5 and  (content['score_list']['senti_query'][0][1] > 0.5 or content['score_list']['senti'][0][1] > 0.5)\\\n",
    "                and (content['score_list']['porn'][0][1] < 0.5 and content['score_list']['abusive'][0][1] < 0.5)\\\n",
    "                and (content['score_list']['offensive'][0][1] < 0.5 and content['score_list']['politics'][0][1] < 0.5\\\n",
    "                    and content['score_list']['teenager'][0][1] < 0.5):\n",
    "                    text = re.sub(u'[^\\u4e00-\\u9fa50-9a-zA-Z ]+', '\\n', content['text'].lower())\n",
    "                    content['keywords'] = keyword_api.extract_keywords(text)\n",
    "                    if content['keywords']:\n",
    "                        continue\n",
    "                    d = {\n",
    "                        'text':content['text'],\n",
    "                        'label':['正常'],\n",
    "                        'source':'offensive_query_risk_gren_v18'\n",
    "                    }\n",
    "                    fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "dff65e25-f9a4-4061-a3c0-485de13a69cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/sentiment/offensive_cold.json.v18.risk', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/sentiment/offensive_cold.json.v18') as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            if content['label'][0] in ['正常']:\n",
    "                if content['score_list']['query_risk'][0][1] < 0.5 and  (content['score_list']['senti_query'][0][1] < 0.3 or content['score_list']['senti'][0][1] < 0.3)\\\n",
    "            and (content['score_list']['porn'][0][1] < 0.5 and content['score_list']['abusive'][0][1] < 0.5)\\\n",
    "            and (content['score_list']['offensive'][0][1] < 0.5 and content['score_list']['politics'][0][1] < 0.5\\\n",
    "                and content['score_list']['teenager'][0][1] < 0.5):\n",
    "                    d = {\n",
    "                        'text':content['text'],\n",
    "                        'label':['正常'],\n",
    "                        'source':'offensive_query_risk_gren_v18'\n",
    "                    }\n",
    "                    fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "                elif content['score_list']['query_risk'][0][1] < 0.5 and  (content['score_list']['senti_query'][0][1] > 0.5 or content['score_list']['senti'][0][1] > 0.5)\\\n",
    "                and (content['score_list']['porn'][0][1] < 0.5 and content['score_list']['abusive'][0][1] < 0.5)\\\n",
    "                and (content['score_list']['offensive'][0][1] < 0.5 and content['score_list']['politics'][0][1] < 0.5\\\n",
    "                    and content['score_list']['teenager'][0][1] < 0.5):\n",
    "                    d = {\n",
    "                        'text':content['text'],\n",
    "                        'label':['正常'],\n",
    "                        'source':'offensive_query_risk_gren_v18'\n",
    "                    }\n",
    "                    fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "05a0a16e-0cc2-45e2-8e0f-72ea2a78c70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2071401it [39:16, 879.17it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final'\n",
    "output_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final.green.v17'\n",
    "\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9556e5c6-a5ed-4916-9884-12b1360f0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final'\n",
    "output_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final.green.v17'\n",
    "\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "86fed580-a5b9-40de-825a-baa09de60014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2071400it [01:05, 31510.55it/s]\n"
     ]
    }
   ],
   "source": [
    "white = []\n",
    "black = []\n",
    "query_risk_black = []\n",
    "query_risk_black_v1 = []\n",
    "output_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final.green.v18'\n",
    "with open(output_path) as frobj:\n",
    "    for line in tqdm(frobj):\n",
    "        content = json.loads(line.strip())\n",
    "        if content['score_list']['bias'][0][1] > 0.5 and (content['score_list']['senti_query'][0][1] < 0.1 or content['score_list']['senti'][0][1] < 0.1) \\\n",
    "        and (content['score_list']['ciron'][0][1] < 0.5 and content['score_list']['offensive'][0][1] < 0.5) \\\n",
    "        and (content['score_list']['porn'][0][1] < 0.5):\n",
    "            white.append(content)\n",
    "        elif  content['score_list']['bias'][0][1] > 0.8 and (content['score_list']['senti_query'][0][1] > 0.8 or content['score_list']['senti'][0][1] > 0.8):\n",
    "            black.append(content)\n",
    "        if content['score_list']['query_risk'][0][1] > 0.5 and  (content['score_list']['senti_query'][0][1] < 0.3 or content['score_list']['senti'][0][1] < 0.3)\\\n",
    "        and (content['score_list']['porn'][0][1] < 0.5 and content['score_list']['abusive'][0][1] < 0.5)\\\n",
    "        and (content['score_list']['offensive'][0][1] < 0.5 and content['score_list']['politics'][0][1] < 0.5):\n",
    "            query_risk_black.append(content)\n",
    "        if content['score_list']['query_risk'][0][1] > 0.5 and  (content['score_list']['senti_query'][0][1] > 0.5 or content['score_list']['senti'][0][1] > 0.5)\\\n",
    "        and (content['score_list']['porn'][0][1] < 0.5 and content['score_list']['abusive'][0][1] < 0.5)\\\n",
    "        and (content['score_list']['offensive'][0][1] < 0.5 and content['score_list']['politics'][0][1] < 0.5):\n",
    "            query_risk_black_v1.append(content)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "683f6a59-97a8-429c-8abd-be00cc43f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.green.v18', 'w') as fwobj:\n",
    "    for d in query_risk_black+query_risk_black_v1:\n",
    "        # if d['label'][0] in ['恐怖主义', '灵异灵修', '死亡', 'LGBT', '战争', 'BDSM', '性生活', '心理健康', '健康', '国家', '历史', '法律', 'LGBT', '军事', '时事政治', '社会', '宗教', '毒品', '博彩', '色情']:\n",
    "        #     continue\n",
    "        # # if '杀死' in d['text'] or '生不如死' in d['text'] or '色情' in d['text'] or '一夜情' in d['text'] or '出轨' in d['text'] or '代孕' in d['text'] or '敲诈' in d['text'] or '勒索' in d['text'] or '邪教' in d['text'] or '找小姐' in d['text'] or '赌博' in d['text'] or '约炮' in d['text'] or '强奸' in d['text'] or '暴力' in d['text'] or '嫖娼' in d['text'] or '淫秽' in d['text'] or '口交' in d['text'] or '安眠药' in d['text'] or '做爱' in d['text'] or '报复' in d['text'] or '侵略' in d['text'] or '攻打' in d['text'] or '砍刀' in d['text'] or '原子弹' in d['text'] or '闯红灯' in d['text'] or '跳楼' in d['text'] or '轻生' in d['text'] or '自杀' in d['text']:\n",
    "        # #     continue\n",
    "        # keyword_resp = keyword.extract_keywords(d['text'])\n",
    "        # flag = False\n",
    "        # if keyword_resp and d['label'][0] not in ['游戏', '动漫', '电脑/网络', '电子数码', '体育/运动']:\n",
    "        #     continue\n",
    "        # if '如何评价' in d['text']:\n",
    "        #     continue\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'topic':d['label'],\n",
    "            'label':['风险']\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "601d692e-0bf9-407c-9d8a-264b95155a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = [\n",
    "    '找小姐',\n",
    "    '赌博',\n",
    "    '约炮',\n",
    "    '强奸',\n",
    "    '暴力',\n",
    "    '嫖娼',\n",
    "    '淫秽',\n",
    "    '口交',\n",
    "    '安眠药',\n",
    "    '做爱',\n",
    "    '报复',\n",
    "    '报仇',\n",
    "    '侵略',\n",
    "    '原子弹',\n",
    "    '跳楼',\n",
    "    '闯红灯',\n",
    "    '自杀',\n",
    "    '轻生',\n",
    "    '大麻',\n",
    "    '冰毒',\n",
    "    '邪教',\n",
    "    '敲诈',\n",
    "    '勒索',\n",
    "    '出轨',\n",
    "    '代孕',\n",
    "    '色情',\n",
    "    '一夜情',\n",
    "    '生不如死',\n",
    "    '杀死',\n",
    "    '诈骗',\n",
    "    '骗钱',\n",
    "    '被骗',\n",
    "    '砍刀',\n",
    "    '伊斯兰教',\n",
    "    '屠杀',\n",
    "    '蹂躏',\n",
    "    '强暴',\n",
    "    '虐待',\n",
    "    '家暴',\n",
    "    '炸弹',\n",
    "    '岛国片',\n",
    "    '战争',\n",
    "    '暴打',\n",
    "    '残忍',\n",
    "    '处死',\n",
    "    '911',\n",
    "    '体罚',\n",
    "    '炮友',\n",
    "    '核武',\n",
    "    '吴亦凡',\n",
    "    '毁灭',\n",
    "    '打架',\n",
    "    '小三',\n",
    "    '犯罪',\n",
    "    '性侵',\n",
    "    '强奸',\n",
    "    '歧视',\n",
    "    '羞辱',\n",
    "    '淫妻癖',\n",
    "    '嫖妓',\n",
    "    '敲诈',\n",
    "    '毒品',\n",
    "    '烧死',\n",
    "    '爱国',\n",
    "    '暴徒',\n",
    "    '打伤',\n",
    "    '打死',\n",
    "    '歧视',\n",
    "    '杀害',\n",
    "    '奥巴马',\n",
    "    'AV',\n",
    "    '黑人',\n",
    "    '诱奸',\n",
    "    '迷奸',\n",
    "    '病态',\n",
    "    '同性恋',\n",
    "    '打到跪地求饶',\n",
    "    '陪睡',\n",
    "    '血腥',\n",
    "    '杀人',\n",
    "    '行窃',\n",
    "    '黑社会',\n",
    "    '暴力执法',\n",
    "    '残杀',\n",
    "    '不要脸',\n",
    "    '羞辱',\n",
    "    '愚民政策',\n",
    "    '性骚扰',\n",
    "    '烧死',\n",
    "    '玩弄女性',\n",
    "    '拍裸照',\n",
    "    '威胁',\n",
    "    '校园暴力',\n",
    "    '迫害',\n",
    "    '袭警',\n",
    "    '腐败',\n",
    "    'A片',\n",
    "    '侵略',\n",
    "    '核战争',\n",
    "    '恶毒',\n",
    "    '挂卖',\n",
    "    '鸦片',\n",
    "    '猥亵',\n",
    "    '打一架',\n",
    "    '打架',\n",
    "    '斗殴',\n",
    "    '械斗',\n",
    "    '殉情',\n",
    "    '抽人',\n",
    "    '砸玻璃',\n",
    "    '砸东西',\n",
    "    '最恶心',\n",
    "    '堕胎',\n",
    "    '虐杀',\n",
    "    '怒骂',\n",
    "    '虐狗',\n",
    "    '虐猫',\n",
    "    '虐待',\n",
    "    '结束生命',\n",
    "    '偷欢',\n",
    "    '穿刺',\n",
    "    'av',\n",
    "    '想杀'\n",
    "]\n",
    "\n",
    "from keyword_processor import KeywordProcesser\n",
    "keyword_api = KeywordProcesser(keywords=keyword)\n",
    "\n",
    "\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.green', 'w') as fwobj:\n",
    "    for d in query_risk_black+query_risk_black_v1:\n",
    "        if d['label'][0] in ['恐怖主义', '灵异灵修', '死亡', 'LGBT', '战争', 'BDSM', '性生活', '心理健康', '健康', '国家', '历史', '法律', 'LGBT', '军事', '时事政治', '社会', '宗教', '毒品', '博彩', '色情']:\n",
    "            continue\n",
    "        # if '杀死' in d['text'] or '生不如死' in d['text'] or '色情' in d['text'] or '一夜情' in d['text'] or '出轨' in d['text'] or '代孕' in d['text'] or '敲诈' in d['text'] or '勒索' in d['text'] or '邪教' in d['text'] or '找小姐' in d['text'] or '赌博' in d['text'] or '约炮' in d['text'] or '强奸' in d['text'] or '暴力' in d['text'] or '嫖娼' in d['text'] or '淫秽' in d['text'] or '口交' in d['text'] or '安眠药' in d['text'] or '做爱' in d['text'] or '报复' in d['text'] or '侵略' in d['text'] or '攻打' in d['text'] or '砍刀' in d['text'] or '原子弹' in d['text'] or '闯红灯' in d['text'] or '跳楼' in d['text'] or '轻生' in d['text'] or '自杀' in d['text']:\n",
    "        #     continue\n",
    "        keyword_resp = keyword_api.extract_keywords(d['text'])\n",
    "        flag = False\n",
    "        if keyword_resp and d['label'][0] not in ['游戏', '动漫', '电脑/网络', '电子数码', '体育/运动']:\n",
    "            continue\n",
    "        if '如何评价' in d['text']:\n",
    "            continue\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'topic':d['label'],\n",
    "            'label':['正常']\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ed6f9dd0-4075-4eea-911a-aa321300da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, jieba_fast\n",
    "termdict = {}\n",
    "with open('/data/albert.xht/TermTree.V1-1.0.json', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/TermTree.V1-1.0', 'r') as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line)\n",
    "            content['text'] = content['term']\n",
    "            fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a4871205-e86c-4f5f-a068-666b06b5bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyword_processor import KeywordProcesser\n",
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/risk_event.txt') as frobj:\n",
    "    for line in frobj:\n",
    "        content = line.strip()\n",
    "    keyword_list = content.split('||')\n",
    "\n",
    "keyword_api = KeywordProcesser(keywords=keyword_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "96f4c56d-a6bf-4ac2-8181-f79b8f86f8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2071401it [03:19, 10407.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import jieba_fast as jieba\n",
    "for word in keyword_list:\n",
    "    jieba.add_word(word)\n",
    "\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final.keyword', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final') as frobj:\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            text = re.sub(u'[^\\u4e00-\\u9fa50-9a-zA-Z ]+', '\\n', content['text'].lower())\n",
    "            content['keywords'] = keyword_api.extract_keywords(text)\n",
    "            words_list = list(jieba.cut(text))\n",
    "            words_set = {}\n",
    "            for word in words_list:\n",
    "                words_set[word] = ''\n",
    "            if content['keywords']:\n",
    "                keyword_list = []\n",
    "                for keyword_ in content['keywords']:\n",
    "                    if keyword_[-1] in words_set:\n",
    "                        keyword_list.append(keyword_)\n",
    "                if keyword_list:\n",
    "                    if content['label'][0] not in ['游戏', '电脑/网络', '小说', '网络安全', '动漫']:\n",
    "                        content['topic'] = content['label']\n",
    "                        content['label'] = ['风险']\n",
    "                        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "                    elif content['label'][0] in ['游戏', '电脑/网络', '小说', '网络安全', '动漫']:\n",
    "                        content['topic'] = content['label']\n",
    "                        content['label'] = ['正常']\n",
    "                        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d34bde97-d069-4082-8ede-87572856a877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " keyword_api.extract_keywords('我讨厌你')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "938531da-96c5-47f2-9170-7c2bdae4e8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1019179it [17:41, 960.55it/s] \n"
     ]
    }
   ],
   "source": [
    "input_path = '/data/albert.xht/TermTree.V1-1.0.json'\n",
    "output_path = '/data/albert.xht/TermTree.V1-1.0.json.green.v17'\n",
    "\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "bb7a5c08-0963-4aa4-ba52-76de20df851f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'senti_query': [['负向', 0.7054874300956726],\n",
       "  ['中性', 0.23167049884796143],\n",
       "  ['正向', 0.06284210085868835]],\n",
       " 'senti': [['负向', 0.4684368669986725], ['正向', 0.5315631628036499]],\n",
       " 'bias': [['偏见', 0.10638052970170975], ['正常', 0.8936194777488708]],\n",
       " 'ciron': [['讽刺', 0.0028106968384236097], ['正常', 0.9971893429756165]],\n",
       " 'intent': [['主观评价/比较/判断', 0.6584299206733704],\n",
       "  ['寻求建议/帮助', 0.02026059664785862],\n",
       "  ['其它', 0.32130950689315796]],\n",
       " 'offensive': [['冒犯', 0.0009839484700933099], ['正常', 0.9990160465240479]],\n",
       " 'query_risk': [['风险', 0.0022602395620197058],\n",
       "  ['个人信息', 1.4249596347326587e-07],\n",
       "  ['正常', 0.9977396726608276]],\n",
       " 'teenager': [['不良', 0.09984611719846725], ['正常', 0.9001538753509521]],\n",
       " 'politics': [['涉政', 0.0002020606625592336], ['正常', 0.9997979998588562]],\n",
       " 'porn': [['色情', 0.0021054197568446398], ['正常', 0.9978945851325989]],\n",
       " 'abusive': [['辱骂', 0.00041765838977880776], ['正常', 0.9995823502540588]]}"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_risk_api_base.predict('我承认我馋妹妹身子了 我的天哪')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cacc85d1-c5e3-4c10-ac84-e5b8db75e116",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'green_risk_api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_94100/1268439750.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgreen_risk_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"\"手腕上有18道伤口，今晚过后又会多两道，我在等，等熬不下去的那一天逃离这个世界\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'green_risk_api' is not defined"
     ]
    }
   ],
   "source": [
    "green_risk_api.predict(\"\"\"\"手腕上有18道伤口，今晚过后又会多两道，我在等，等熬不下去的那一天逃离这个世界\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a405b4b0-a2ee-4dff-a95c-b94233ca2d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7873100983020554,\n",
       " 0.2975871313672922,\n",
       " 0.7087087087087087,\n",
       " 0.23771224307417338)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "881/1119, 333/1119, 236/333, 266/1119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8b99ef89-04e2-4876-959f-99504a45b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.green.manual', 'w') as fwobj:\n",
    "    for word in keyword:\n",
    "        d = {\n",
    "            'text':'如何'+word,\n",
    "            'label':['风险']\n",
    "        }\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "        d = {\n",
    "            'text':'想'+word,\n",
    "            'label':['风险']\n",
    "        }\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "        d = {\n",
    "            'text':'可以'+word,\n",
    "            'label':['风险']\n",
    "        }\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "53b7f1a1-b518-4c33-835f-f540ca6a789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.green.v1', 'w') as fwobj:\n",
    "    for d in query_risk_black_v1:\n",
    "        if d['label'][0] in ['BDSM', '性生活', '心理健康', '健康', '国家', '历史', '法律', 'LGBT', '军事', '时事政治', '社会', '宗教', '毒品', '博彩', '色情']:\n",
    "            continue\n",
    "        if '原子弹' in d['text'] or '闯红灯' in d['text'] or '跳楼' in d['text'] or '轻生' in d['text'] or '自杀' in d['text']:\n",
    "            continue\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'topic':d['label'],\n",
    "            'label':['正常']\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cd0dc1a8-b646-4e30-ba54-184d73a82963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'senti_query': [['负向', 0.017739232629537582],\n",
       "  ['中性', 0.9809319376945496],\n",
       "  ['正向', 0.0013288870686665177]],\n",
       " 'senti': [['负向', 0.9829708337783813], ['正向', 0.01702919416129589]],\n",
       " 'bias': [['偏见', 0.017038287594914436], ['正常', 0.9829617142677307]],\n",
       " 'ciron': [['讽刺', 0.002870894968509674], ['正常', 0.9971291422843933]],\n",
       " 'intent': [['主观评价/比较/判断', 0.9999998807907104],\n",
       "  ['寻求建议/帮助', 2.48466243135681e-08],\n",
       "  ['其它', 1.3022548728258698e-07]],\n",
       " 'offensive': [['冒犯', 0.24652498960494995], ['正常', 0.7534750699996948]],\n",
       " 'query_risk': [['风险', 0.015497354790568352],\n",
       "  ['个人信息', 2.2719566459272755e-06],\n",
       "  ['正常', 0.9845004081726074]],\n",
       " 'teenager': [['不良', 0.004878206178545952], ['正常', 0.9951217770576477]],\n",
       " 'politics': [['涉政', 0.0005057504749856889], ['正常', 0.999494194984436]],\n",
       " 'porn': [['色情', 4.695681855082512e-05], ['正常', 0.999953031539917]],\n",
       " 'abusive': [['辱骂', 0.00043745216680690646], ['正常', 0.9995625615119934]]}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_risk_api_base.predict('你是不是一个鬼')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d2f5b0-2009-41d9-b8e3-8f9136e2feca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b927d-61d4-45cc-aa89-c7fe16858a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/data/albert.xht/sentiment/green_teenager.json'\n",
    "output_path = '/data/albert.xht/sentiment/green_teenager.json.offensive_query_risk.green'\n",
    "\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f8c93b60-ee37-46f9-9a27-63c7e9369ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/sentiment/green_teenager.json.offensive_query_risk.filter.green', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/sentiment/green_teenager.json.offensive_query_risk.green') as frobj:\n",
    "        white = []\n",
    "        black = []\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            if content['score_list']['query_risk'][0][1]< 0.2 and content['score_list']['offensive'][0][1] < 0.3 and  content['label'][0] in ['正常']:\n",
    "                white.append(content)\n",
    "            elif content['score_list']['query_risk'][0][1]> 0.9 and content['score_list']['offensive'][0][1] > 0.9:\n",
    "                black.append(content)\n",
    "            elif content['score_list']['query_risk'][0][1]> 0.8 and content['score_list']['offensive'][0][1] > 0.8 and content['label'][0] in ['不良']:\n",
    "                black.append(content)\n",
    "            elif content['score_list']['query_risk'][0][1] > 0.6 and content['score_list']['offensive'][0][1] > 0.6 and  content['label'][0] in ['不良']:                                                                                                              \n",
    "                black.append(content)\n",
    "            elif content['score_list']['porn'][0][1] > 0.8:\n",
    "                black.append(content)\n",
    "            elif content['score_list']['politics'][0][1] > 0.8:\n",
    "                black.append(content)\n",
    "            elif content['score_list']['abusive'][0][1] > 0.8:\n",
    "                black.append(content)\n",
    "            elif content['score_list']['offensive'][0][1] > 0.9:\n",
    "                black.append(content)\n",
    "            elif content['ori_label'] in ['博彩相关内容', '博彩广告、宣传']:\n",
    "                black.append(content)\n",
    "    for d in white:\n",
    "        tmp = {\n",
    "            'text':d['text'],\n",
    "            'label':['正常'],\n",
    "            'source':'green_teenager_filter'\n",
    "        }\n",
    "        fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')\n",
    "    for d in black:\n",
    "        tmp = {\n",
    "            'text':d['text'],\n",
    "            'label':['风险'],\n",
    "            'source':'green_teenager_filter'\n",
    "        }\n",
    "        fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c385712-3af1-4acc-a844-7a1319319129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/smal_white_positive.json.filter'\n",
    "output_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/smal_white_positive.json.filter.offensive_query_risk'\n",
    "\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e768eb8e-e319-4ab7-8a85-8d66cd2135e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35694it [00:57, 624.96it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/data/albert.xht/xiaoda/sentiment/senti/senti_ocemotion.json'\n",
    "output_path = '/data/albert.xht/xiaoda/sentiment/senti/senti_ocemotion.json.green'\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aeb00c9e-0247-4c0b-b66f-5f0244c421bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113317it [02:12, 853.19it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/data/albert.xht/xiaoda/sentiment/green_politics/green_politics.json.topic'\n",
    "output_path = '/data/albert.xht/xiaoda/sentiment/green_politics/green_politics.json.topic.green'\n",
    "\n",
    "predict_data(green_risk_api, input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "db0fad80-d426-438e-b960-4a534e24f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "from collections import Counter\n",
    "k = {}\n",
    "with open(output_path) as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['score_list']['politics'][0][1] > 0.5:\n",
    "            if content['topic'] not in k:\n",
    "                k[content['topic']] = []\n",
    "            k[content['topic']].append(content)\n",
    "# for key in k:\n",
    "#     print(key, len(k[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cb561d3-a8cd-4156-a2da-f147a49d253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39650/39650 [06:05<00:00, 108.52it/s]\n"
     ]
    }
   ],
   "source": [
    "violence_query = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/Gender-Base-Violence-main/GBV/gbv/Train.csv.json.translate') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        text = ''.join(json.loads(line.strip())['zh'])\n",
    "        text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", text)   # 合并正文中过多的空格\n",
    "        violence_query.append(text)\n",
    "violence_result = []\n",
    "for d in tqdm(violence_query):\n",
    "    result = green_risk_api.predict(d)\n",
    "    violence_result.append((d, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cec75a4-866a-45f5-8b5b-85916362db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dddd = []\n",
    "dddd_1 = []\n",
    "for p in violence_result:\n",
    "    if p[1]['query_risk'][0][1] > 0.5:\n",
    "        dddd.append(p)\n",
    "    else:\n",
    "        dddd_1.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "d727de47-e043-446b-a6e9-44b77ab0ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5128/5128 [01:19<00:00, 64.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/itag_labl_data_deal_20230112.xlsx')\n",
    "gold, pred = [], []\n",
    "result_list = []\n",
    "data_dict = {}\n",
    "for idx in tqdm(range(df.shape[0])):\n",
    "    content = df.loc[idx]\n",
    "    \n",
    "    score_list = green_risk_api_base.predict(content['query'])\n",
    "    if content['query'] not in data_dict:\n",
    "        data_dict[content['query']] = ''\n",
    "    else:\n",
    "        continue\n",
    "    if content['human_risk'] in [-1]:\n",
    "        continue\n",
    "    result_list.append((content, score_list))\n",
    "    if score_list['query_risk'][0][1] > 0.5 :\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)\n",
    "    \n",
    "    gold.append(int(content['human_risk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5eb05b74-af80-4b67-bcab-f36027e842d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = []\n",
    "for p1, p2, p3 in zip(pred, gold, result_list):\n",
    "    if p1 != p2:\n",
    "        left.append((p1, p2, p3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "ee58727b-d47f-416d-bd01-83337bdcb429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9710    0.9253    0.9476      4378\n",
      "           1     0.6002    0.8023    0.6867       612\n",
      "\n",
      "    accuracy                         0.9102      4990\n",
      "   macro avg     0.7856    0.8638    0.8172      4990\n",
      "weighted avg     0.9255    0.9102    0.9156      4990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(gold, pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e2dcb530-b750-4438-8881-9a128a4c3f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'senti_query': [['负向', 0.8237579464912415],\n",
       "  ['中性', 0.12852226197719574],\n",
       "  ['正向', 0.04771984741091728]],\n",
       " 'senti': [['负向', 0.9493701457977295], ['正向', 0.05062990263104439]],\n",
       " 'bias': [['偏见', 0.08553604781627655], ['正常', 0.9144639372825623]],\n",
       " 'ciron': [['讽刺', 0.06932748854160309], ['正常', 0.9306724667549133]],\n",
       " 'intent': [['主观评价/比较/判断', 0.9464393258094788],\n",
       "  ['寻求建议/帮助', 0.00016714385128580034],\n",
       "  ['其它', 0.05339352786540985]],\n",
       " 'offensive': [['冒犯', 0.5559340119361877], ['正常', 0.44406598806381226]],\n",
       " 'query_risk': [['风险', 0.6405723094940186],\n",
       "  ['个人信息', 3.481139856376103e-07],\n",
       "  ['正常', 0.3594273328781128]],\n",
       " 'teenager': [['不良', 0.12507572770118713], ['正常', 0.8749243021011353]],\n",
       " 'politics': [['涉政', 0.002126638311892748], ['正常', 0.9978733062744141]],\n",
       " 'porn': [['色情', 0.0002676665026228875], ['正常', 0.9997323155403137]],\n",
       " 'abusive': [['辱骂', 0.007041711825877428], ['正常', 0.9929583072662354]]}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_risk_api.predict('饭前打孩子一顿还是饭后呢')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e1a294f-2ced-463a-a7c2-13f7df294828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "offensive = []\n",
    "with open('/data/albert.xht/sentiment/dev/offensive_cold.json') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        offensive.append(content)\n",
    "        \n",
    "offensive_test = []\n",
    "with open('/data/albert.xht/sentiment/test/offensive_cold.json') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        offensive_test.append(content)\n",
    "\n",
    "        \n",
    "cdia_bias = []\n",
    "with open('/data/albert.xht/sentiment/dev/cdial_bias.json') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        cdia_bias.append(content)\n",
    "        \n",
    "senti_copr = []\n",
    "with open('/data/albert.xht/sentiment/dev/senti_copr.json') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        senti_copr.append(content)\n",
    "        \n",
    "ciron = []\n",
    "with open('/data/albert.xht/sentiment/dev/chinese_ciron.json') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        ciron.append(content)\n",
    "\n",
    "senti_smp = []\n",
    "with open('/data/albert.xht/sentiment/dev/senti_smp_usual.json') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        senti_smp.append(content)\n",
    "        \n",
    "senti_smpecisa = []\n",
    "with open('/data/albert.xht/sentiment/dev/senti_smpecisa.json') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        senti_smpecisa.append(content)\n",
    "\n",
    "        \n",
    "senti_query = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_valid.json.filter.0.7') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        senti_query.append(content)\n",
    "\n",
    "def evaluation(risk_api, model_path):\n",
    "    risk_api.reload(model_path)\n",
    "    print('===offensive===')\n",
    "    pred, gold, pred_score = eval_all(offensive_test, risk_api, 'offensive')\n",
    "    evaluation_ece(pred_score, gold)\n",
    "    print('===cdia-bias===')\n",
    "    pred, gold, pred_score = eval_all(cdia_bias, risk_api, 'bias')\n",
    "    evaluation_ece(pred_score, gold)\n",
    "    print('===ciron===')\n",
    "    pred, gold, pred_score = eval_all(ciron, risk_api, 'ciron')\n",
    "    evaluation_ece(pred_score, gold)\n",
    "    print('===chsenti===')\n",
    "    pred, gold, pred_score = eval_all(senti_copr, risk_api, 'senti')\n",
    "    evaluation_ece(pred_score, gold)\n",
    "    print('===senti_smpecisa===')\n",
    "    pred, gold, pred_score = eval_all(senti_smpecisa, risk_api, 'senti')\n",
    "    evaluation_ece(pred_score, gold)\n",
    "    print('===senti_smp===')\n",
    "    pred, gold, pred_score = eval_all(senti_smp, risk_api, 'senti')\n",
    "    evaluation_ece(pred_score, gold)\n",
    "    print('===senti_query===')\n",
    "    pred, gold, pred_score = eval_all(senti_query, risk_api, 'senti')\n",
    "    evaluation_ece(pred_score, gold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa4a76a2-9ca8-4670-8add-4710c8cc2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation(green_risk_api,\n",
    "#            '/data/albert.xht/xiaodao/risk_classification/multitask_raw_filter_senti_query_risk_v12_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_v11/multitask_cls.pth.4'\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c48ae-86fd-43ba-b206-ac431c3258d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_abusive = []\n",
    "# risk_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_filter_senti_query_risk_v11_offensive_v2/multitask_cls.pth.9')\n",
    "\n",
    "with open('/data/albert.xht/xiaoda/sentiment/senti/', 'r') as frobj:\n",
    "    queue = []\n",
    "    t = []\n",
    "    from collections import Counter\n",
    "    pppp = Counter()\n",
    "    for idx, line in tqdm(enumerate(frobj)):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        content = json.loads(line.strip())\n",
    "        text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", content['text'])   # 合并正文中过多的空格\n",
    "        # if content['label'] not in ['black']:\n",
    "        #     continue\n",
    "        if len(text) >= 164:\n",
    "            text = text[:164]\n",
    "        queue.append(text)\n",
    "        t.append(content)\n",
    "        if np.mod(len(queue), 128) == 0 and queue:\n",
    "            probs = risk_predict_batch(green_risk_api, queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                content = {\n",
    "                    'text':text,\n",
    "                    'topic':tt['label'],\n",
    "                    'score_list':prob_dict\n",
    "                }\n",
    "                politics_abusive.append(content)\n",
    "            queue = []\n",
    "            t = []\n",
    "    if queue:\n",
    "        probs = risk_predict_batch(green_risk_api, queue)\n",
    "        for prob_dict, text, tt in zip(probs, queue, t):\n",
    "            content = {\n",
    "                'text':text,\n",
    "                'topic':tt['label'],\n",
    "                'score_list':prob_dict\n",
    "            }\n",
    "            politics_abusive.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35c87f80-009d-44c6-9851-3b506e32b052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "451726it [12:53, 583.69it/s]\n"
     ]
    }
   ],
   "source": [
    "politics_abusive = []\n",
    "# risk_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_filter_senti_query_risk_v11_offensive_v2/multitask_cls.pth.9')\n",
    "\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_politics/green_politics.json', 'r') as frobj:\n",
    "    queue = []\n",
    "    t = []\n",
    "    from collections import Counter\n",
    "    pppp = Counter()\n",
    "    for idx, line in tqdm(enumerate(frobj)):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        content = json.loads(line.strip())\n",
    "        text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", content['text'])   # 合并正文中过多的空格\n",
    "        # if content['label'] not in ['black']:\n",
    "        #     continue\n",
    "        if len(text) >= 164:\n",
    "            text = text[:164]\n",
    "        queue.append(text)\n",
    "        t.append(content)\n",
    "        if np.mod(len(queue), 128) == 0 and queue:\n",
    "            probs = risk_predict_batch(green_risk_api, queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                content = {\n",
    "                    'text':text,\n",
    "                    'topic':tt['label'],\n",
    "                    'score_list':prob_dict\n",
    "                }\n",
    "                politics_abusive.append(content)\n",
    "            queue = []\n",
    "            t = []\n",
    "    if queue:\n",
    "        probs = risk_predict_batch(green_risk_api, queue)\n",
    "        for prob_dict, text, tt in zip(probs, queue, t):\n",
    "            content = {\n",
    "                'text':text,\n",
    "                'topic':tt['label'],\n",
    "                'score_list':prob_dict\n",
    "            }\n",
    "            politics_abusive.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af35e88-e59b-4429-9e20-db6c1d0e8f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/xiaoda/sentiment/green_politics/green_poltics.json.green', 'w') as fwobj:\n",
    "    for d in politics_abusive:\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b35f0941-241a-4fd3-9bde-c8ee39bff7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/xiaoda/sentiment/green_politics/green_poltics.json.green', 'w') as fwobj:\n",
    "    for d in politics_abusive:\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70880334-2ce3-4518-916d-55522bb1a3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1105924it [29:34, 623.16it/s]\n"
     ]
    }
   ],
   "source": [
    "abusive_list = []\n",
    "import re\n",
    "# risk_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_filter_senti_query_risk_v11_offensive_v2/multitask_cls.pth.9')\n",
    "\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_abusive/green_abusive.json', 'r') as frobj:\n",
    "    queue = []\n",
    "    t = []\n",
    "    from collections import Counter\n",
    "    pppp = Counter()\n",
    "    for idx, line in tqdm(enumerate(frobj)):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        content = json.loads(line.strip())\n",
    "        text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", content['text'])   # 合并正文中过多的空格\n",
    "        # if content['label'] not in ['black']:\n",
    "        #     continue\n",
    "        if len(text) >= 164:\n",
    "            text = text[:164]\n",
    "        queue.append(text)\n",
    "        t.append(content)\n",
    "        if np.mod(len(queue), 128) == 0 and queue:\n",
    "            probs = risk_predict_batch(green_risk_api, queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                content = {\n",
    "                    'text':text,\n",
    "                    'topic':tt['label'],\n",
    "                    'score_list':prob_dict\n",
    "                }\n",
    "                abusive_list.append(content)\n",
    "            queue = []\n",
    "            t = []\n",
    "    if queue:\n",
    "        probs = risk_predict_batch(green_risk_api, queue)\n",
    "        for prob_dict, text, tt in zip(probs, queue, t):\n",
    "            content = {\n",
    "                'text':text,\n",
    "                'topic':tt['label'],\n",
    "                'score_list':prob_dict\n",
    "            }\n",
    "            abusive_list.append(content)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e77a435-57df-4e49-b74e-6b59da449a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/xiaoda/sentiment/green_abusive/green_abusive.json.green', 'w') as fwobj:\n",
    "    for d in abusive_list:\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17d2649b-3d2e-46d8-a56f-70ee3bb700cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2758518it [1:13:02, 629.39it/s]\n"
     ]
    }
   ],
   "source": [
    "porn_list = []\n",
    "# risk_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_filter_senti_query_risk_v11_offensive_v2/multitask_cls.pth.9')\n",
    "\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_porn/green_porn.json', 'r') as frobj:\n",
    "    queue = []\n",
    "    t = []\n",
    "    from collections import Counter\n",
    "    pppp = Counter()\n",
    "    for idx, line in tqdm(enumerate(frobj)):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        content = json.loads(line.strip())\n",
    "        text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", content['text'])   # 合并正文中过多的空格\n",
    "        # if content['label'] not in ['black']:\n",
    "        #     continue\n",
    "        if len(text) >= 164:\n",
    "            text = text[:164]\n",
    "        queue.append(text)\n",
    "        t.append(content)\n",
    "        if np.mod(len(queue), 128) == 0 and queue:\n",
    "            probs = risk_predict_batch(green_risk_api, queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                content = {\n",
    "                    'text':text,\n",
    "                    'topic':tt['label'],\n",
    "                    'score_list':prob_dict\n",
    "                }\n",
    "                porn_list.append(content)\n",
    "            queue = []\n",
    "            t = []\n",
    "    if queue:\n",
    "        probs = risk_predict_batch(green_risk_api, queue)\n",
    "        for prob_dict, text, tt in zip(probs, queue, t):\n",
    "            content = {\n",
    "                'text':text,\n",
    "                'topic':tt['label'],\n",
    "                'score_list':prob_dict\n",
    "            }\n",
    "            porn_list.append(content)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cc205d6-0bba-4762-a6d7-c686845cffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/xiaoda/sentiment/green_porn/green_porn.json.green', 'w') as fwobj:\n",
    "    for d in porn_list:\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6557e36-5642-4b89-a115-84d52127b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "porn_list = []\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_porn/green_porn.json.green', 'r') as frobj:\n",
    "    for line in frobj:\n",
    "        porn_list.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "a8911799-f013-48df-8c24-15259bbe5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.extend(['/root/deepIE'])\n",
    "\n",
    "from utils.keyword_processor import KeywordProcesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6bccee04-b022-45a0-b408-9a8adcddc473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "_DocSpan = namedtuple(  # pylint: disable=invalid-name\n",
    "        \"DocSpan\", [\"start\", \"length\"])\n",
    "\n",
    "def slide_window(all_doc_tokens, max_length, doc_stride, offset=32):\n",
    "    doc_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < len(all_doc_tokens):\n",
    "        length = len(all_doc_tokens) - start_offset\n",
    "        if length > max_length - offset:\n",
    "            length = max_length - offset\n",
    "        doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "        if start_offset + length == len(all_doc_tokens):\n",
    "            break\n",
    "        start_offset += min(length, doc_stride)\n",
    "    return doc_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba3604ce-6af2-479f-a0aa-b41d550fdeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "poltics_new = []\n",
    "white = []\n",
    "for d in politics_abusive:\n",
    "    # if (d['score_list']['abusive'][0][1] < 0.5 and d['score_list']['porn'][0][1] < 0.5) and d['topic'][0] in ['涉政']:\n",
    "    if  d['topic'][0] in ['涉政']:\n",
    "        poltics_new.append(d)\n",
    "for d in politics_abusive:\n",
    "    if d['topic'][0] in ['正常']:\n",
    "        poltics_new.append(d)\n",
    "        white.append(d)\n",
    "        \n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_politics/green_politics.json.politics', 'w') as fwobj:\n",
    "    for d in poltics_new:\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'label':d['topic'],\n",
    "            'topic':d['topic']\n",
    "        }\n",
    "        if d['topic'][0] in ['正常']:\n",
    "            fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "            text = d['text']\n",
    "            spans = slide_window(text[16:], 16, 8, 0)\n",
    "            for span in spans:\n",
    "                span_text = text[:16]+text[span.start+16:span.start+span.length+16]\n",
    "                content = {\n",
    "                    'text':span_text,\n",
    "                    'label':d['topic'],\n",
    "                    'topic':d['topic']\n",
    "                }\n",
    "                fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "        else:\n",
    "            fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d8a6c263-109a-457c-a8df-285ff073b489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221325\n"
     ]
    }
   ],
   "source": [
    "porn_politics = []\n",
    "for d in porn_list:\n",
    "    # if (d['score_list']['abusive'][0][1] < 0.5 and d['score_list']['porn'][0][1] < 0.5) and d['score_list']['politics'][0][1] > 0.9:\n",
    "    # if (d['score_list']['abusive'][0][1] > 0.9 or d['score_list']['porn'][0][1] > 0.9) and d['score_list']['politics'][0][1] > 0.5 and d['score_list']['politics'][0][1] < 0.8:\n",
    "    if (d['score_list']['abusive'][0][1] > 0.9 or d['score_list']['porn'][0][1] > 0.9) and d['score_list']['politics'][0][1] > 0.3 and d['score_list']['politics'][0][1] < 0.8:\n",
    "        porn_politics.append(d)\n",
    "    if (d['score_list']['abusive'][0][1] < 0.3 and d['score_list']['porn'][0][1] < 0.3) and d['topic'][0] in ['色情']:\n",
    "        porn_politics.append(d)\n",
    "    # if (d['score_list']['abusive'][0][1] < 0.3 and d['score_list']['porn'][0][1] < 0.3) and d['score_list']['politics'][0][1] < 0.3:\n",
    "    #     porn_politics.append(d)\n",
    "\n",
    "print(len(porn_politics))\n",
    "\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_politics/green_politics.json.porn', 'w') as fwobj:\n",
    "    for d in porn_politics:\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'label':['正常'],\n",
    "            'topic':d['topic']\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c6998325-d6fc-4d25-b331-af7050bb48d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "porn_abusive = []\n",
    "for d in porn_list:\n",
    "    # if (d['score_list']['abusive'][0][1] > 0.9 and d['score_list']['porn'][0][1] > 0.5) :\n",
    "    #     porn_abusive.append(d)\n",
    "    # if (d['score_list']['abusive'][0][1] < 0.5 and d['score_list']['porn'][0][1] > 0.5) :\n",
    "    #     porn_abusive.append(d)\n",
    "    # if (d['score_list']['abusive'][0][1] > 0.5 and d['score_list']['porn'][0][1] < 0.5) :\n",
    "    #     porn_abusive.append(d)\n",
    "    # if (d['topic'][0] in ['色情'] and d['score_list']['porn'][0][1] < 0.5) :\n",
    "    #     porn_abusive.append(d)\n",
    "    # if (d['score_list']['abusive'][0][1] < 0.5 and d['score_list']['porn'][0][1] < 0.5) :\n",
    "    #     porn_abusive.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ccad6ba-4f24-47c9-bb31-e6ee92d46d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91527\n"
     ]
    }
   ],
   "source": [
    "abusive_politics = []\n",
    "for d in abusive_list:\n",
    "    # if (d['score_list']['abusive'][0][1] < 0.5 and d['score_list']['porn'][0][1] < 0.5) and d['score_list']['politics'][0][1] > 0.9:\n",
    "    # if (d['score_list']['abusive'][0][1] > 0.9 or d['score_list']['porn'][0][1] > 0.9) and d['score_list']['politics'][0][1] > 0.5 and d['score_list']['politics'][0][1] < 0.8:\n",
    "    if (d['score_list']['abusive'][0][1] > 0.9 or d['score_list']['porn'][0][1] > 0.9) and d['score_list']['politics'][0][1] > 0.3 and d['score_list']['politics'][0][1] < 0.8:\n",
    "        abusive_politics.append(d)\n",
    "    if (d['score_list']['abusive'][0][1] < 0.3 and d['score_list']['porn'][0][1] < 0.3) and d['topic'][0] in ['辱骂']:\n",
    "        abusive_politics.append(d)\n",
    "\n",
    "print(len(abusive_politics))\n",
    "\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_politics/green_politics.json.abusive', 'w') as fwobj:\n",
    "    for d in abusive_politics:\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'label':['正常'],\n",
    "            'topic':d['topic']\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "564c5ac5-123c-455a-8042-0a124ebf8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json, re\n",
    "\n",
    "def risk_predict_batch(risk_api, text):\n",
    "    if isinstance(text, list):\n",
    "        text_list = text\n",
    "    else:\n",
    "        text_list = [text]\n",
    "    result_list = risk_api.predict_batch(text_list)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3763600-1aa4-4910-865e-b849b5dabf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1128122it [17:42, 1062.09it/s]\n"
     ]
    }
   ],
   "source": [
    "abusive = []\n",
    "risk_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_filter_senti_query_risk_v11_offensive_v2/multitask_cls.pth.9')\n",
    "\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_abusive/short_text_benchmark_for_cro_albert_ruma.json.txt', 'r') as frobj:\n",
    "    queue = []\n",
    "    t = []\n",
    "    from collections import Counter\n",
    "    pppp = Counter()\n",
    "    for idx, line in tqdm(enumerate(frobj)):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        content = json.loads(line.strip())\n",
    "        text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", content['text'])   # 合并正文中过多的空格\n",
    "        # if content['label'] not in ['black']:\n",
    "        #     continue\n",
    "        if len(text) >= 164:\n",
    "            text = text[:164]\n",
    "        pppp[content['label']] += 1\n",
    "        queue.append(text)\n",
    "        t.append(content)\n",
    "        if np.mod(len(queue), 128) == 0 and queue:\n",
    "            probs = risk_predict_batch(risk_api, queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                content = {\n",
    "                    'text':text,\n",
    "                    'topic':tt['label'],\n",
    "                    'score_list':prob_dict\n",
    "                }\n",
    "                abusive.append(content)\n",
    "            queue = []\n",
    "            t = []\n",
    "    if queue:\n",
    "        probs = risk_predict_batch(risk_api, queue)\n",
    "        for prob_dict, text, tt in zip(probs, queue, t):\n",
    "            content = {\n",
    "                'text':text,\n",
    "                'topic':tt['label'],\n",
    "                'score_list':prob_dict\n",
    "            }\n",
    "            abusive.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47e571e-0574-45d4-8837-00d677e7357f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1128121/1128121 [00:17<00:00, 65633.13it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/data/albert.xht/xiaoda/sentiment/green_abusive/short_text_benchmark_for_cro_albert_ruma.json.txt.offensive', 'w') as fwobj:\n",
    "    for d in tqdm(abusive):\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8930606f-2507-4eda-bdf8-f4f705e6292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1128121it [00:25, 44229.65it/s]\n"
     ]
    }
   ],
   "source": [
    "abusive = []\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_abusive/short_text_benchmark_for_cro_albert_ruma.json.txt.offensive') as frobj:\n",
    "    for line in tqdm(frobj):\n",
    "        abusive.append(json.loads(line.strip()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "733dd2e6-2440-4c06-964e-98ea3b1d6aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1128121/1128121 [00:03<00:00, 370510.78it/s]\n",
      "100%|██████████| 1105924/1105924 [00:07<00:00, 139382.76it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mapping = {\n",
    "    '辱骂':'辱骂',\n",
    "    '辱骂-白样本':'正常'\n",
    "}\n",
    "\n",
    "import random\n",
    "random.shuffle(abusive)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_abusive/green_abusive.json', 'w') as fwobj:\n",
    "    black = []\n",
    "    left = []\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "    data_dict = {}\n",
    "    for d in tqdm(abusive):\n",
    "        p = {\n",
    "                'text':d['text'],\n",
    "                'label':[mapping[d['topic']]]\n",
    "            }\n",
    "        if d['text'] not in data_dict:\n",
    "            data_dict[d['text']] = set()\n",
    "        data_dict[d['text']].add(p['label'][0])\n",
    "    for key in tqdm(data_dict):\n",
    "        label = data_dict[key]\n",
    "        if len(label) == 1:\n",
    "            p = {\n",
    "                'text':key,\n",
    "                'label':list(label)\n",
    "            }\n",
    "        fwobj.write(json.dumps(p, ensure_ascii=False)+'\\n')\n",
    "    #     if d['score_list']['offensive'][0][1] > 0.9 or d['score_list']['query_risk'][0][1] > 0.9:\n",
    "    #         if d['topic'] in ['辱骂']:\n",
    "    #             p = {\n",
    "    #                 'text':d['text'],\n",
    "    #                 'label':[mapping[d['topic']]],\n",
    "    #                 'source':'risk-offensive'\n",
    "    #             }\n",
    "    #             fwobj.write(json.dumps(p, ensure_ascii=False)+'\\n')\n",
    "    #             count1 += 1\n",
    "    #         elif d['topic'] in ['辱骂-白样本', '辱骂-白样本']:\n",
    "    #             p = {\n",
    "    #                 'text':d['text'],\n",
    "    #                 'label':[mapping[d['topic']]],\n",
    "    #                 'source':'risk-offensive'\n",
    "    #             }\n",
    "    #             fwobj.write(json.dumps(p, ensure_ascii=False)+'\\n')\n",
    "    #             count2 += 1\n",
    "    #         if count1+count2 >= 200000:\n",
    "    #             break\n",
    "    # count = 0\n",
    "    # for d in tqdm(abusive):\n",
    "    #     if d['score_list']['offensive'][0][1] < 0.1 and d['score_list']['query_risk'][0][1] < 0.1:\n",
    "    #         if d['topic'] in ['辱骂']:\n",
    "    #             p = {\n",
    "    #                 'text':d['text'],\n",
    "    #                 'label':[mapping[d['topic']]],\n",
    "    #                 'source':'abusive'\n",
    "    #             }\n",
    "    #             fwobj.write(json.dumps(p, ensure_ascii=False)+'\\n')\n",
    "    #             count3 += 1\n",
    "    #         elif d['topic'] in ['辱骂-白样本', '辱骂-白样本']:\n",
    "    #             p = {\n",
    "    #                 'text':d['text'],\n",
    "    #                 'label':[mapping[d['topic']]],\n",
    "    #                 'source':'abusive'\n",
    "    #             }\n",
    "    #             fwobj.write(json.dumps(p, ensure_ascii=False)+'\\n')\n",
    "    #             count4 += 1\n",
    "    #         if count3+count4 >= 200000:\n",
    "    #             break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7106bc53-934e-4be8-b8d0-0a7f6c69fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "_DocSpan = namedtuple(  # pylint: disable=invalid-name\n",
    "        \"DocSpan\", [\"start\", \"length\"])\n",
    "\n",
    "def slide_window(all_doc_tokens, max_length, doc_stride, offset=0):\n",
    "    doc_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < len(all_doc_tokens):\n",
    "        length = len(all_doc_tokens) - start_offset\n",
    "        if length > max_length - offset:\n",
    "            length = max_length - offset\n",
    "        doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "        if start_offset + length == len(all_doc_tokens):\n",
    "            break\n",
    "        start_offset += min(length, doc_stride)\n",
    "    return doc_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bd1ef8b5-2df9-44ff-92ae-261dd659f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/xiaoda/sentiment/green_politics/green_politics.json.add', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/xiaoda/sentiment/green_politics/green_politics.json') as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "    import random\n",
    "    random.shuffle(white)\n",
    "    for d in white[:50000]:\n",
    "        d['label'] = ['正常']\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "efd4b4ce-0783-4a2c-8456-929cccd5e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/xiaoda/sentiment/query_risk_v12/query_risk_final.json.merge.add', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/xiaoda/sentiment/query_risk_v12/query_risk_final.json.merge', 'r') as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "    for d in politics+porn+abusive_list:\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "    \n",
    "    import random\n",
    "    random.shuffle(politics_black)\n",
    "    random.shuffle(porn_black)\n",
    "    random.shuffle(abusive_black)\n",
    "    for d in politics_black[:int(len(politics_black)*0.5)]:\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "    for d in porn_black[:int(len(porn_black)*0.5)]:\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "    for d in abusive_black[:int(len(abusive_black)*0.5)]:\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "90682e3e-fd46-432e-9699-f04e513b998c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39431"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(politics_black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fc822095-c6dc-40c7-a37e-ad84cbb097db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "politics = []\n",
    "politics_black = []\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_politics/green_politics.json') as frobj:\n",
    "    for line in frobj:\n",
    "        d = json.loads(line.strip())\n",
    "        if d['label'][0] in ['涉政'] and d['source'] in ['politics']:\n",
    "            d['label'] = ['风险']\n",
    "            politics.append(d)\n",
    "        if d['label'][0] in ['涉政'] and d['source'] in ['risk-offensive']:\n",
    "            d['label'] = ['风险']\n",
    "            politics_black.append(d)\n",
    "        if d['label'][0] in ['正常'] and d['source'] in ['politics']:\n",
    "            d['label'] = ['正常']\n",
    "            politics.append(d)\n",
    "print(len(politics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cc146fe4-ef9f-4fed-b6bc-5117fc41554a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 150969\n"
     ]
    }
   ],
   "source": [
    "porn = []\n",
    "white = []\n",
    "porn_black = []\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_porn/green_porn.json') as frobj:\n",
    "    for line in frobj:\n",
    "        d = json.loads(line.strip())\n",
    "        if d['label'][0] in ['色情'] and d['source'] in ['porn']:\n",
    "            d['label'] = ['风险']\n",
    "            porn.append(d)\n",
    "        elif d['label'][0] in ['色情'] and d['source'] in ['risk-offensive']:\n",
    "            d['label'] = ['风险']\n",
    "            porn_black.append(d)\n",
    "        elif d['label'][0] in ['正常'] and d['source'] in ['porn']:\n",
    "            d['label'] = ['正常']\n",
    "            porn.append(d)\n",
    "            white.append(d)\n",
    "print(len(porn), len(white))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "05064185-efe7-4e6d-b43d-a74fe1833cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29701\n"
     ]
    }
   ],
   "source": [
    "abusive_list = []\n",
    "abusive_black = []\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_abusive/green_abusive.json') as frobj:\n",
    "    for line in frobj:\n",
    "        d = json.loads(line.strip())\n",
    "        if d['label'][0] in ['辱骂'] and d['source'] in ['abusive']:\n",
    "            d['label'] = ['风险']\n",
    "            abusive_list.append(d)\n",
    "        if d['label'][0] in ['辱骂'] and d['source'] in ['risk-offensive']:\n",
    "            d['label'] = ['风险']\n",
    "            abusive_black.append(d)\n",
    "        if d['label'][0] in ['辱骂'] and d['source'] in ['abusive']:\n",
    "            d['label'] = ['正常']\n",
    "            abusive_list.append(d)\n",
    "            white.append(d)\n",
    "print(len(abusive_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a85d99ca-79d4-43d0-9a51-be3372cbbd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150786"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb774b71-20c6-42be-af68-98bdbcece541",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/xiaoda/sentiment/green_abusive/green_abusive_label.txt', 'w') as fwobj:\n",
    "    for label in ['辱骂', '正常']:\n",
    "        fwobj.write(label+'\\n')\n",
    "        \n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_porn/green_porn_label.txt', 'w') as fwobj:\n",
    "    for label in ['色情', '正常']:\n",
    "        fwobj.write(label+'\\n')\n",
    "        \n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_politics/green_politics_label.txt', 'w') as fwobj:\n",
    "    for label in ['涉政', '正常']:\n",
    "        fwobj.write(label+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "561e4b40-48fa-4ab4-b9b0-afcc0423299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1128121it [00:13, 85335.15it/s]\n"
     ]
    }
   ],
   "source": [
    "abusive = []\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_abusive/short_text_benchmark_for_cro_albert_ruma.json.txt.offensive') as frobj:\n",
    "    for line in tqdm(frobj):\n",
    "        abusive.append(json.loads(line.strip()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ae3b45ca-ecb3-4927-9400-3678c8c35c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2805958it [01:11, 39464.73it/s]\n"
     ]
    }
   ],
   "source": [
    "porn = []\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_porn/short_text_benchmark_for_cro_albert_seqing.json.txt.offensive') as frobj:\n",
    "    for line in tqdm(frobj):\n",
    "        porn.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7d6b9610-4c54-4530-b9dc-165120f85ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "456219it [00:05, 79169.32it/s]\n"
     ]
    }
   ],
   "source": [
    "politics = []\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_politics/short_text_benchmark_for_cro_albert_politics.json.txt.offensive') as frobj:\n",
    "    for line in tqdm(frobj):\n",
    "        politics.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7d936600-b7e1-4fba-a8d8-b2fd7d7083d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "black = {\n",
    "    'abusive':{1:[], 2:[], 3:[], 4:[]},\n",
    "    'porn':{1:[], 2:[], 3:[], 4:[]},\n",
    "    'politics':{1:[], 2:[], 3:[], 4:[]}\n",
    "}\n",
    "white = {\n",
    "    'abusive':{1:[], 2:[], 3:[], 4:[]},\n",
    "    'porn':{1:[], 2:[], 3:[], 4:[]},\n",
    "    'politics':{1:[], 2:[], 3:[], 4:[]}\n",
    "}\n",
    "abusive_mapping = {\n",
    "    '辱骂':'辱骂',\n",
    "    '辱骂-白样本':'正常'\n",
    "}\n",
    "\n",
    "porn_mapping = {\n",
    "    '色情':'色情',\n",
    "    '色情-白样本':'正常',\n",
    "    '色情-正常':'正常'\n",
    "}\n",
    "\n",
    "politics_mapping = {\n",
    "    'black':'涉政',\n",
    "    'white':'正常'\n",
    "}\n",
    "\n",
    "def filter_fn(d, stage):\n",
    "    if stage == 1:\n",
    "        cond1 = d['score_list']['offensive'][0][1] < 0.5 and d['score_list']['offensive'][0][1] > 0.3\n",
    "        cond2 = d['score_list']['query_risk'][0][1] < 0.5 and d['score_list']['query_risk'][0][1] > 0.3\n",
    "        if cond1 and cond2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    if stage == 2:\n",
    "        cond1 = d['score_list']['offensive'][0][1] > 0.5 and d['score_list']['offensive'][0][1] < 0.9\n",
    "        cond2 = d['score_list']['query_risk'][0][1] > 0.5 and d['score_list']['query_risk'][0][1] < 0.9\n",
    "        if cond1 and cond2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    if stage ==3:\n",
    "        cond1 = d['score_list']['offensive'][0][1] > 0.9\n",
    "        cond2 = d['score_list']['query_risk'][0][1] > 0.9\n",
    "        if cond1 and cond2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    if stage == 4:\n",
    "        cond1 = d['score_list']['offensive'][0][1] < 0.2\n",
    "        cond2 = d['score_list']['query_risk'][0][1] < 0.2\n",
    "        if cond1 and cond2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "for d in abusive:\n",
    "    if abusive_mapping[d['topic']] in ['辱骂']:\n",
    "        for stage in [1, 2, 3, 4]:\n",
    "            if filter_fn(d, stage):\n",
    "                d['label'] = abusive_mapping[d['topic']]\n",
    "                black['abusive'][stage].append(d)\n",
    "    if abusive_mapping[d['topic']] in ['正常']:\n",
    "        for stage in [1, 2, 3, 4]:\n",
    "            if filter_fn(d, stage):\n",
    "                d['label'] = abusive_mapping[d['topic']]\n",
    "                white['abusive'][stage].append(d)\n",
    "                \n",
    "for d in porn:\n",
    "    if porn_mapping[d['topic']] in ['色情']:\n",
    "        for stage in [1, 2, 3, 4]:\n",
    "            if filter_fn(d, stage):\n",
    "                d['label'] = porn_mapping[d['topic']]\n",
    "                black['porn'][stage].append(d)\n",
    "    if porn_mapping[d['topic']] in ['正常']:\n",
    "        for stage in [1, 2, 3, 4]:\n",
    "            if filter_fn(d, stage):\n",
    "                d['label'] = porn_mapping[d['topic']]\n",
    "                white['porn'][stage].append(d)\n",
    "                \n",
    "for d in politics:\n",
    "    if politics_mapping[d['topic']] in ['涉政']:\n",
    "        for stage in [1, 2, 3, 4]:\n",
    "            if filter_fn(d, stage):                    \n",
    "                d['label'] = politics_mapping[d['topic']]\n",
    "                black['politics'][stage].append(d)\n",
    "    if politics_mapping[d['topic']] in ['正常']:\n",
    "        for stage in [1, 2, 3, 4]:\n",
    "            if filter_fn(d, stage):\n",
    "                d['label'] = politics_mapping[d['topic']]\n",
    "                white['politics'][stage].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "764d0902-1d00-4be3-b3c1-3bc6685efe4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '对的给你点赞我还没购买呢就问它龟儿子一句三通一达都上班了咋还发不了货呢龟儿子怼老子去找三通一达买日它妈就跟只有它一家在卖一样就这副德行还想做生意拉黑老子一样骂它看到它商品有评论老子就来骂',\n",
       "  'topic': '色情-白样本',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9548986554145813],\n",
       "    ['正常', 0.04510140046477318]],\n",
       "   'query_risk': [['风险', 0.9991697072982788],\n",
       "    ['个人信息', 5.0525082770036533e-05],\n",
       "    ['正常', 0.0007798463338986039]]},\n",
       "  'label': '正常'},\n",
       " {'text': '2019年刷单不给钱死骗子',\n",
       "  'topic': '色情-白样本',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9943484663963318],\n",
       "    ['正常', 0.00565155828371644]],\n",
       "   'query_risk': [['风险', 0.9631624221801758],\n",
       "    ['个人信息', 5.80549931328278e-06],\n",
       "    ['正常', 0.03683171793818474]]},\n",
       "  'label': '正常'},\n",
       " {'text': 'sb关你屌事',\n",
       "  'topic': '色情-白样本',\n",
       "  'score_list': {'offensive': [['冒犯', 0.991685688495636],\n",
       "    ['正常', 0.00831428449600935]],\n",
       "   'query_risk': [['风险', 0.9979343414306641],\n",
       "    ['个人信息', 1.1481271940283477e-05],\n",
       "    ['正常', 0.0020541255362331867]]},\n",
       "  'label': '正常'},\n",
       " {'text': 'sb客服死全家',\n",
       "  'topic': '色情-白样本',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9836064577102661],\n",
       "    ['正常', 0.01639355719089508]],\n",
       "   'query_risk': [['风险', 0.9980872273445129],\n",
       "    ['个人信息', 1.2648452866415028e-05],\n",
       "    ['正常', 0.001900123548693955]]},\n",
       "  'label': '正常'},\n",
       " {'text': 'tmd想投诉这个黑店',\n",
       "  'topic': '色情-白样本',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9956533908843994],\n",
       "    ['正常', 0.004346581641584635]],\n",
       "   'query_risk': [['风险', 0.9993441700935364],\n",
       "    ['个人信息', 2.2116160835139453e-05],\n",
       "    ['正常', 0.000633683055639267]]},\n",
       "  'label': '正常'},\n",
       " {'text': '辣眼睛男子身穿女士内衣潜入女浴室偷窥被抓网友爆料',\n",
       "  'topic': '色情-白样本',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9502805471420288],\n",
       "    ['正常', 0.04971948638558388]],\n",
       "   'query_risk': [['风险', 0.9864383935928345],\n",
       "    ['个人信息', 9.980312825064175e-06],\n",
       "    ['正常', 0.013551575131714344]]},\n",
       "  'label': '正常'},\n",
       " {'text': '一脸猥琐',\n",
       "  'topic': '色情-白样本',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9822264909744263],\n",
       "    ['正常', 0.017773563042283058]],\n",
       "   'query_risk': [['风险', 0.9314595460891724],\n",
       "    ['个人信息', 9.810771189222578e-06],\n",
       "    ['正常', 0.06853070110082626]]},\n",
       "  'label': '正常'},\n",
       " {'text': '三个贱货',\n",
       "  'topic': '色情-白样本',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9886661767959595],\n",
       "    ['正常', 0.01133386418223381]],\n",
       "   'query_risk': [['风险', 0.9991815686225891],\n",
       "    ['个人信息', 1.352791969111422e-05],\n",
       "    ['正常', 0.0008047809242270887]]},\n",
       "  'label': '正常'},\n",
       " {'text': '不喜欢北京人你滚出北京呀在这瞎bb什么没有教养的泼妇吐',\n",
       "  'topic': '色情-白样本',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9957656860351562],\n",
       "    ['正常', 0.004234286490827799]],\n",
       "   'query_risk': [['风险', 0.9997045397758484],\n",
       "    ['个人信息', 0.00013982750533614308],\n",
       "    ['正常', 0.00015566327783744782]]},\n",
       "  'label': '正常'},\n",
       " {'text': '不是说他后悔了么怎么还敢惹中国我中国怕你呢么国庆节朴槿惠用三件事羞辱了中国国庆节朴槿惠用三件',\n",
       "  'topic': '色情-白样本',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9685829281806946],\n",
       "    ['正常', 0.03141703084111214]],\n",
       "   'query_risk': [['风险', 0.9994235038757324],\n",
       "    ['个人信息', 2.4831630071275868e-05],\n",
       "    ['正常', 0.0005517848185263574]]},\n",
       "  'label': '正常'}]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/data/albert.xht/xiaoda/sentiment/query_risk_v12/query_risk_final.json.merge.add', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/xiaoda/sentiment/query_risk_v12/query_risk_final.json.merge', 'r') as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "    \n",
    "    for d in white['porn'][1]:\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'label':[d['label']]\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "    \n",
    "    random.shuffle(white['porn'][4])\n",
    "    for d in white['porn'][4][:500000]:\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'label':[d['label']]\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "    \n",
    "    random.shuffle(black['porn'][4])\n",
    "    for d in black['porn'][4][:100000]:\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'label':[d['label']]\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "    random.shuffle(black['porn'][2])\n",
    "    for d in black['porn'][2][:100000]:\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'label':[d['label']]\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "    \n",
    "    random.shuffle(black['porn'][2])\n",
    "    for d in black['porn'][2][:100000]:\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'label':[d['label']]\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "    random.shuffle(black['politics'][4])\n",
    "    for d in black['politics'][4][:100000]:\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'label':[d['label']]\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "    random.shuffle(black['politics'][4])\n",
    "    for d in black['politics'][4][:100000]:\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'label':[d['label']]\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e07ca833-f43f-4412-8695-f88eb97dc617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5317"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(black['porn'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "aa23fc05-193e-4b4d-adf1-6ce7a194b79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '小骚货搞死你哦',\n",
       "  'topic': '色情',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9431781768798828],\n",
       "    ['正常', 0.05682184919714928]],\n",
       "   'query_risk': [['风险', 0.9965651631355286],\n",
       "    ['个人信息', 1.6847050574142486e-05],\n",
       "    ['正常', 0.003417979460209608]]},\n",
       "  'label': '色情'},\n",
       " {'text': '就你那个大几巴怕是早邪哦你怕是森窥哦还大几巴你行不行就是一个问题',\n",
       "  'topic': '色情',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9350871443748474],\n",
       "    ['正常', 0.06491286307573318]],\n",
       "   'query_risk': [['风险', 0.9823386073112488],\n",
       "    ['个人信息', 3.4572629374451935e-05],\n",
       "    ['正常', 0.017626861110329628]]},\n",
       "  'label': '色情'},\n",
       " {'text': '干爹干爹',\n",
       "  'topic': '色情',\n",
       "  'score_list': {'offensive': [['冒犯', 0.933490514755249],\n",
       "    ['正常', 0.06650949269533157]],\n",
       "   'query_risk': [['风险', 0.9553155899047852],\n",
       "    ['个人信息', 1.4558536349795759e-05],\n",
       "    ['正常', 0.04466981813311577]]},\n",
       "  'label': '色情'},\n",
       " {'text': '并且多数人还配合其强奸操难怪当年中国几乎亡于日本',\n",
       "  'topic': '色情',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9945043325424194],\n",
       "    ['正常', 0.005495647434145212]],\n",
       "   'query_risk': [['风险', 0.9997852444648743],\n",
       "    ['个人信息', 8.905443974072114e-05],\n",
       "    ['正常', 0.0001257533731404692]]},\n",
       "  'label': '色情'},\n",
       " {'text': '开什么车某些人也太精虫上脑了吧满脑子里都装着什么东西太loser了吧',\n",
       "  'topic': '色情',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9914496541023254],\n",
       "    ['正常', 0.008550368249416351]],\n",
       "   'query_risk': [['风险', 0.9962766766548157],\n",
       "    ['个人信息', 1.0923872650892008e-05],\n",
       "    ['正常', 0.0037123654037714005]]},\n",
       "  'label': '色情'},\n",
       " {'text': '我艹尼哥嘴',\n",
       "  'topic': '色情',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9207193851470947],\n",
       "    ['正常', 0.07928063720464706]],\n",
       "   'query_risk': [['风险', 0.9899368286132812],\n",
       "    ['个人信息', 1.364565014227992e-05],\n",
       "    ['正常', 0.010049431584775448]]},\n",
       "  'label': '色情'},\n",
       " {'text': '操他妈的婊子',\n",
       "  'topic': '色情',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9976796507835388],\n",
       "    ['正常', 0.0023204144090414047]],\n",
       "   'query_risk': [['风险', 0.9991286396980286],\n",
       "    ['个人信息', 1.611280458746478e-05],\n",
       "    ['正常', 0.0008552794461138546]]},\n",
       "  'label': '色情'},\n",
       " {'text': '操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈操你妈',\n",
       "  'topic': '色情',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9520869851112366],\n",
       "    ['正常', 0.04791303351521492]],\n",
       "   'query_risk': [['风险', 0.9993128776550293],\n",
       "    ['个人信息', 0.0003774068900384009],\n",
       "    ['正常', 0.0003097228182014078]]},\n",
       "  'label': '色情'},\n",
       " {'text': '操你妈日的没钱用了',\n",
       "  'topic': '色情',\n",
       "  'score_list': {'offensive': [['冒犯', 0.985346794128418],\n",
       "    ['正常', 0.014653213322162628]],\n",
       "   'query_risk': [['风险', 0.9997833371162415],\n",
       "    ['个人信息', 5.99386403337121e-05],\n",
       "    ['正常', 0.00015668000560253859]]},\n",
       "  'label': '色情'},\n",
       " {'text': '操你妈的山狗操你妈的山狗操你妈的山狗心塞塞山了我最爱的今井熊sp基本买的款都被山过今井熊是真爱啊我操你妈',\n",
       "  'topic': '色情',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9684181809425354],\n",
       "    ['正常', 0.0315818153321743]],\n",
       "   'query_risk': [['风险', 0.9996908903121948],\n",
       "    ['个人信息', 8.542079012840986e-05],\n",
       "    ['正常', 0.00022364809410646558]]},\n",
       "  'label': '色情'}]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black['porn'][3][20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "88e58b72-585a-4daf-9fa4-0afbcd900bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '用来打你玛',\n",
       "  'topic': '辱骂',\n",
       "  'score_list': {'offensive': [['冒犯', 0.7227365970611572],\n",
       "    ['正常', 0.2772634029388428]],\n",
       "   'query_risk': [['风险', 0.7774815559387207],\n",
       "    ['个人信息', 2.7271302315057255e-05],\n",
       "    ['正常', 0.22249111533164978]]}},\n",
       " {'text': '傻逼会的',\n",
       "  'topic': '辱骂',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9970690608024597],\n",
       "    ['正常', 0.0029309105593711138]],\n",
       "   'query_risk': [['风险', 0.9801044464111328],\n",
       "    ['个人信息', 1.1893325790879317e-05],\n",
       "    ['正常', 0.019883625209331512]]}},\n",
       " {'text': '上脸了太刺激我感觉我都要裂了这纯属就是骗人的代购的手段我还真的没信了呵呵太欺负人\\\\n这要是烂脸了谁负责',\n",
       "  'topic': '辱骂',\n",
       "  'score_list': {'offensive': [['冒犯', 0.979736864566803],\n",
       "    ['正常', 0.020263150334358215]],\n",
       "   'query_risk': [['风险', 0.987972617149353],\n",
       "    ['个人信息', 5.65334630664438e-05],\n",
       "    ['正常', 0.011970845982432365]]}},\n",
       " {'text': '一看你就是踏马的老板本人狗你麻痹',\n",
       "  'topic': '辱骂',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9497252702713013],\n",
       "    ['正常', 0.05027477815747261]],\n",
       "   'query_risk': [['风险', 0.9998016953468323],\n",
       "    ['个人信息', 5.620441152132116e-05],\n",
       "    ['正常', 0.000142022137879394]]}},\n",
       " {'text': '你个傻逼不会评论别jb评 让我骂你一顿舒服',\n",
       "  'topic': '辱骂',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9704589247703552],\n",
       "    ['正常', 0.02954104356467724]],\n",
       "   'query_risk': [['风险', 0.9974604845046997],\n",
       "    ['个人信息', 1.176420755655272e-05],\n",
       "    ['正常', 0.002527659758925438]]}},\n",
       " {'text': '骗子不得好死全家男的得癌症女的做妓女做婊子',\n",
       "  'topic': '辱骂',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9976295232772827],\n",
       "    ['正常', 0.0023705067578703165]],\n",
       "   'query_risk': [['风险', 0.9992768168449402],\n",
       "    ['个人信息', 2.496750676073134e-05],\n",
       "    ['正常', 0.0006981390179134905]]}},\n",
       " {'text': '死jb东西',\n",
       "  'topic': '辱骂',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9639133214950562],\n",
       "    ['正常', 0.03608671575784683]],\n",
       "   'query_risk': [['风险', 0.9938098788261414],\n",
       "    ['个人信息', 1.2828140825149603e-05],\n",
       "    ['正常', 0.006177353207021952]]}},\n",
       " {'text': '操你妈会说人话吗不卖就不卖嘴吃粑粑了',\n",
       "  'topic': '辱骂',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9510776996612549],\n",
       "    ['正常', 0.0489223450422287]],\n",
       "   'query_risk': [['风险', 0.9985440969467163],\n",
       "    ['个人信息', 1.1018200893886387e-05],\n",
       "    ['正常', 0.0014448629226535559]]}},\n",
       " {'text': '国语垃圾还不如看字幕',\n",
       "  'topic': '辱骂',\n",
       "  'score_list': {'offensive': [['冒犯', 0.998030960559845],\n",
       "    ['正常', 0.001968987751752138]],\n",
       "   'query_risk': [['风险', 0.9459164142608643],\n",
       "    ['个人信息', 1.7941461919690482e-05],\n",
       "    ['正常', 0.05406565964221954]]}},\n",
       " {'text': '贝心柔是个警察做事不过脑子害死同事就说新人什么都不懂吧这么长时间学不会既然学不会还留在警队干什么开了她得了恶心的人',\n",
       "  'topic': '辱骂',\n",
       "  'score_list': {'offensive': [['冒犯', 0.9979091286659241],\n",
       "    ['正常', 0.0020908715669065714]],\n",
       "   'query_risk': [['风险', 0.9993942975997925],\n",
       "    ['个人信息', 3.781007035286166e-05],\n",
       "    ['正常', 0.0005679345922544599]]}}]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58649afd-98c5-4958-a001-3a12d7c8e419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
