{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25482390-a9b7-46a1-aa0f-568521861d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys,os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efef73ac-9b7c-45fe-80a2-910fc1fd9308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.extend(['/root/xiaoda/query_topic/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c03a36f7-3f10-4c47-8902-c656f568e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/ondrejbohdal/meta-calibration/blob/main/Metrics/metrics.py\n",
    "\"\"\"\n",
    "\n",
    "class ECE(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(ECE, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, logits, labels, mode='logits'):\n",
    "        if mode == 'logits':\n",
    "            softmaxes = F.softmax(logits, dim=1)\n",
    "        else:\n",
    "            softmaxes = logits\n",
    "        # softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "        \n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ca7416-4776-4003-8cfb-179ffc0cf07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'基金': 0, '汽车': 1, '网络安全': 2, '情商': 3, '财务税务': 4, '旅游': 5, '建筑': 6, '炫富': 7, '家电': 8, '商业': 9, '性生活': 10, '风水': 11, '人物': 12, 'VPN': 13, '吸烟': 14, '市场营销': 15, '游戏': 16, '算命': 17, '编程': 18, '死亡': 19, '公司': 20, '国家': 21, '美食/烹饪': 22, '明星': 23, '城市': 24, '银行': 25, '期货': 26, '宗教': 27, '学习': 28, '电子数码': 29, '网络暴力': 30, 'LGBT': 31, '其他': 32, '故事': 33, '社会': 34, '二手': 35, '动漫': 36, '歧视': 37, '常识': 38, '星座': 39, '冷知识': 40, '职场职业': 41, '食品': 42, '心理健康': 43, '电子商务': 44, '道德伦理': 45, '商业/理财': 46, '赚钱': 47, '神话': 48, '校园生活': 49, '色情': 50, '婚姻': 51, '家居装修': 52, '生活': 53, '灵异灵修': 54, '股票': 55, '娱乐': 56, '女性': 57, '体验': 58, '广告': 59, '天气': 60, '女权': 61, '潜规则': 62, '人类': 63, '马克思主义': 64, '历史': 65, '音乐': 66, '毒品': 67, '摄影': 68, '金融': 69, '影视': 70, '语言': 71, '环境': 72, '高铁': 73, '人际交往': 74, '夜店': 75, '价值观': 76, '恋爱': 77, '相貌': 78, 'BDSM': 79, '恐怖主义': 80, '中医': 81, '性侵犯': 82, '阅读': 83, '时尚': 84, '体育/运动': 85, '资本主义': 86, '灾害意外': 87, '博彩': 88, '成长': 89, '校园暴力': 90, '移民': 91, '美容/塑身': 92, '经济': 93, '睡眠': 94, '抄袭': 95, '电脑/网络': 96, '两性': 97, '电影': 98, '思维': 99, '房地产': 100, '民族': 101, '购房置业': 102, '法律': 103, '购物': 104, '幽默滑稽': 105, '教育/科学': 106, '性骚扰': 107, '惊悚': 108, '动植物': 109, '战争': 110, '恶俗': 111, '航空航天': 112, '交通出行': 113, '家庭关系': 114, '文化/艺术': 115, '宠物': 116, '男性': 117, '爱国': 118, '育儿': 119, '保险': 120, '爱情': 121, '健康': 122, '军事': 123, '审美': 124, '交友': 125, '小说': 126, '青春期': 127, '创业投资': 128, '情感': 129, '写作': 130, '时事政治': 131} === {0: '基金', 1: '汽车', 2: '网络安全', 3: '情商', 4: '财务税务', 5: '旅游', 6: '建筑', 7: '炫富', 8: '家电', 9: '商业', 10: '性生活', 11: '风水', 12: '人物', 13: 'VPN', 14: '吸烟', 15: '市场营销', 16: '游戏', 17: '算命', 18: '编程', 19: '死亡', 20: '公司', 21: '国家', 22: '美食/烹饪', 23: '明星', 24: '城市', 25: '银行', 26: '期货', 27: '宗教', 28: '学习', 29: '电子数码', 30: '网络暴力', 31: 'LGBT', 32: '其他', 33: '故事', 34: '社会', 35: '二手', 36: '动漫', 37: '歧视', 38: '常识', 39: '星座', 40: '冷知识', 41: '职场职业', 42: '食品', 43: '心理健康', 44: '电子商务', 45: '道德伦理', 46: '商业/理财', 47: '赚钱', 48: '神话', 49: '校园生活', 50: '色情', 51: '婚姻', 52: '家居装修', 53: '生活', 54: '灵异灵修', 55: '股票', 56: '娱乐', 57: '女性', 58: '体验', 59: '广告', 60: '天气', 61: '女权', 62: '潜规则', 63: '人类', 64: '马克思主义', 65: '历史', 66: '音乐', 67: '毒品', 68: '摄影', 69: '金融', 70: '影视', 71: '语言', 72: '环境', 73: '高铁', 74: '人际交往', 75: '夜店', 76: '价值观', 77: '恋爱', 78: '相貌', 79: 'BDSM', 80: '恐怖主义', 81: '中医', 82: '性侵犯', 83: '阅读', 84: '时尚', 85: '体育/运动', 86: '资本主义', 87: '灾害意外', 88: '博彩', 89: '成长', 90: '校园暴力', 91: '移民', 92: '美容/塑身', 93: '经济', 94: '睡眠', 95: '抄袭', 96: '电脑/网络', 97: '两性', 98: '电影', 99: '思维', 100: '房地产', 101: '民族', 102: '购房置业', 103: '法律', 104: '购物', 105: '幽默滑稽', 106: '教育/科学', 107: '性骚扰', 108: '惊悚', 109: '动植物', 110: '战争', 111: '恶俗', 112: '航空航天', 113: '交通出行', 114: '家庭关系', 115: '文化/艺术', 116: '宠物', 117: '男性', 118: '爱国', 119: '育儿', 120: '保险', 121: '爱情', 122: '健康', 123: '军事', 124: '审美', 125: '交友', 126: '小说', 127: '青春期', 128: '创业投资', 129: '情感', 130: '写作', 131: '时事政治'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/31/2023 23:03:37 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast\n",
    "import transformers\n",
    "from datetime import timedelta\n",
    "\n",
    "import os, sys\n",
    "cur_dir_path = '/root/xiaoda/query_topic/'\n",
    "\n",
    "sys.path.extend([cur_dir_path])\n",
    "\n",
    "from nets.them_classifier import MyBaseModel, RobertaClassifier\n",
    "\n",
    "import configparser\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TopicInfer(object):\n",
    "    def __init__(self, config_path):\n",
    "\n",
    "        import torch, os, sys\n",
    "\n",
    "        con = configparser.ConfigParser()\n",
    "        con_path = os.path.join(cur_dir_path, config_path)\n",
    "        con.read(con_path, encoding='utf8')\n",
    "\n",
    "        args_path = dict(dict(con.items('paths')), **dict(con.items(\"para\")))\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(args_path[\"model_path\"], do_lower_case=True)\n",
    "\n",
    "        label_list = []\n",
    "        label_path = os.path.join(cur_dir_path, args_path['label_path'])\n",
    "        with open(label_path, 'r') as frobj:\n",
    "            for line in frobj:\n",
    "                label_list.append(line.strip())\n",
    "        n_classes = len(label_list)\n",
    "\n",
    "        self.label2id, self.id2label = {}, {}\n",
    "        for idx, label in enumerate(label_list):\n",
    "            self.label2id[label] = idx\n",
    "            self.id2label[idx] = label\n",
    "            \n",
    "        print(self.label2id, '===', self.id2label)\n",
    "        \n",
    "        output_path = os.path.join(cur_dir_path, args_path['output_path'])\n",
    "\n",
    "        from roformer import RoFormerModel, RoFormerConfig\n",
    "\n",
    "        config = RoFormerConfig.from_pretrained(args_path[\"model_path\"])\n",
    "        encoder = RoFormerModel(config=config)\n",
    "        \n",
    "        encoder_net = MyBaseModel(encoder, config)\n",
    "\n",
    "        classify_net = RobertaClassifier(\n",
    "            hidden_size=config.hidden_size, \n",
    "            dropout_prob=con.getfloat('para', 'out_dropout_rate'),\n",
    "            num_labels=n_classes, \n",
    "            dropout_type=con.get('para', 'dropout_type'))\n",
    "\n",
    "        self.device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        class TopicClassifier(nn.Module):\n",
    "            def __init__(self, transformer, classifier):\n",
    "                super().__init__()\n",
    "\n",
    "                self.transformer = transformer\n",
    "                self.classifier = classifier\n",
    "\n",
    "            def forward(self, input_ids, input_mask, \n",
    "                        segment_ids=None, transformer_mode='mean_pooling'):\n",
    "                hidden_states = self.transformer(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              return_mode=transformer_mode)\n",
    "                ce_logits = self.classifier(hidden_states)\n",
    "                return ce_logits, hidden_states\n",
    "\n",
    "        import os\n",
    "        self.net = TopicClassifier(encoder_net, classify_net).to(self.device)\n",
    "        # eo = 9\n",
    "        # ckpt = torch.load(os.path.join(output_path, 'cls.pth.{}'.format(eo)), map_location=self.device)\n",
    "        # self.topic_net.load_state_dict(ckpt)\n",
    "        # self.topic_net.eval()\n",
    "        \n",
    "    def reload(self, model_path):\n",
    "        ckpt = torch.load(model_path, map_location=self.device)\n",
    "        self.net.load_state_dict(ckpt)\n",
    "        self.net.eval() \n",
    "\n",
    "    def predict(self, text, top_n=5):\n",
    "\n",
    "        \"\"\"抽取输入text所包含的类型\n",
    "        \"\"\"\n",
    "        token2char_span_mapping = self.tokenizer(text, return_offsets_mapping=True, max_length=256)[\"offset_mapping\"]\n",
    "        encoder_txt = self.tokenizer.encode_plus(text, max_length=256)\n",
    "        input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().unsqueeze(0).to(self.device)\n",
    "        token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).unsqueeze(0).to(self.device)\n",
    "        attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            scores, hidden_states = self.net(input_ids, attention_mask, token_type_ids, \n",
    "                         transformer_mode='cls'\n",
    "                         )\n",
    "            scores = torch.nn.Softmax(dim=1)(scores)[0].data.cpu().numpy()\n",
    "        \n",
    "        schema_types = []\n",
    "        for index, score in enumerate(scores):\n",
    "             schema_types.append([self.id2label[index], float(score)])\n",
    "        schema_types = sorted(schema_types, key=lambda item:item[1], reverse=True)\n",
    "        return schema_types[0:5]\n",
    "    \n",
    "    def predict_batch(self, text):\n",
    "        if isinstance(text, list):\n",
    "            text_list = text\n",
    "        else:\n",
    "            text_list = [text]\n",
    "        model_input = self.tokenizer(text_list, return_tensors=\"pt\",padding=True, max_length=256)\n",
    "        for key in model_input:\n",
    "            model_input[key] = model_input[key].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            scores, hidden_states = self.net(model_input['input_ids'], \n",
    "                        model_input['attention_mask'], \n",
    "                        model_input['token_type_ids'], \n",
    "                         transformer_mode='cls'\n",
    "                         )\n",
    "            scores = torch.nn.Softmax(dim=1)(scores).data.cpu().numpy()\n",
    "        schema_types_list = []\n",
    "        for score_, text in zip(scores, text_list):\n",
    "            schema_types = []\n",
    "            for index, score in enumerate(score_):\n",
    "                 schema_types.append([self.id2label[index], float(score)])\n",
    "            schema_types = sorted(schema_types, key=lambda item:item[1], reverse=True)\n",
    "            schema_types_list.append(schema_types[0:5])\n",
    "        return schema_types_list\n",
    "    \n",
    "    def infer_batch(self, text):\n",
    "        if isinstance(text, list):\n",
    "            text_list = text\n",
    "        else:\n",
    "            text_list = [text]\n",
    "        model_input = self.tokenizer(text_list, return_tensors=\"pt\",padding=True, max_length=256)\n",
    "        for key in model_input:\n",
    "            model_input[key] = model_input[key].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            scores, hidden_states = self.net(model_input['input_ids'], \n",
    "                        model_input['attention_mask'], \n",
    "                        model_input['token_type_ids'], \n",
    "                         transformer_mode='cls'\n",
    "                         )\n",
    "            scores = torch.nn.Softmax(dim=1)(scores).data.cpu().numpy()\n",
    "        return scores\n",
    "                \n",
    "\n",
    "topic_api = TopicInfer('./topic_data_v4/config.ini')\n",
    "\n",
    "# text = '王二今天打车去了哪里，从哪里出发，到哪里了'\n",
    "# print(topic_api.predict(text), text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869100c8-12ef-44b6-9659-60b456f6b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v4_label_smoothing/them/cls.pth.9')\n",
    "# topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v4/them/cls.pth.9')\n",
    "# topic_api.reload('//data/albert.xht/xiaodao/topic_classification_v4/them_20epoch/cls.pth.19')\n",
    "\n",
    "\n",
    "topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v4/them_knn_20epoch/cls.pth.19')\n",
    "# topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v5/them_knn_10epoch//cls.pth.9')\n",
    "\n",
    "# topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v5/normal_knn_20epoch/cls.pth.19')\n",
    "# topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v5/normal_knn_20epoch_logitclip_1.2/cls.pth.19')\n",
    "\n",
    "# topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v5/normal_knn_40epoch/cls.pth.30')\n",
    "\n",
    "\n",
    "\n",
    "# topic_api.reload('/data/albert.xht/xiaodao/topic_classification_v4/them_3epoch/cls.pth.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d01599de-eeb7-4dd7-9739-8ac0cafe6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/xiaodao/query_risk_v10/biake_qa_web_text_zh_train.json.offensive.all.negative', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/xiaodao/query_risk_v10/biake_qa_web_text_zh_train.json.offensive.all') as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            topic = topic_api.predict_batch(content['text'])[0][0]\n",
    "            if content['label'][0] in ['风险']:\n",
    "                content['topic'] = [topic]\n",
    "                fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d27396b-7c2d-43f9-8b97-f08c0c357a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['时尚', 0.44743233919143677],\n",
       " ['女性', 0.38462936878204346],\n",
       " ['男性', 0.04517074674367905],\n",
       " ['心理健康', 0.029388289898633957],\n",
       " ['恋爱', 0.01591077446937561]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = ['你能不能准时一点',\n",
    "'你太严肃了我不喜欢你',\n",
    "'学神是什么',\n",
    "'是不是网络断开了',\n",
    "'世界上有没有鬼呀',\n",
    "'我老婆不开心了怎么办',\n",
    "'你是个傻瓜']\n",
    "topic_api.predict_batch(['\"女孩子究竟是怎么由丑变美的'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eb75980-a048-4335-9fc3-3eaeeb53b755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218311it [35:57, 101.19it/s]\n",
      "5592it [00:52, 106.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with open('/data/albert.xht/sentiment/BelleGroup_train_2M_CN_v37/BelleGroup_train_2M_CN_tnews_latest_final', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/sentiment/BelleGroup_train_2M_CN_v37/BelleGroup_train_2M_CN_tnews_latest', 'r') as frobj:\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            result = topic_api.predict_batch([content['text'][0:256]])[0]\n",
    "            if result[0][0] not in ['时事政治'] and result[1][0] not in ['时事政治']:\n",
    "                fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "with open('/data/albert.xht/sentiment/BelleGroup_train_2M_CN_v37/BelleGroup_train_2M_CN_title2event_latest_final', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/sentiment/BelleGroup_train_2M_CN_v37/BelleGroup_train_2M_CN_title2event_latest', 'r') as frobj:\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            result = topic_api.predict_batch([content['text'][0:256]])[0]\n",
    "            if result[0][0] not in ['时事政治'] and result[1][0] not in ['时事政治']:\n",
    "                fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77592977-4cb2-4753-bd10-5966a3e83d72",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '【' (U+3010) (1127155099.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    result【0】\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '【' (U+3010)\n"
     ]
    }
   ],
   "source": [
    "result【0】\n",
    "a = torch.tensor(np.random.random((10)))\n",
    "b = torch.tensor([0,1,2,3])\n",
    "a[b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff8a5524-ebf3-43a8-985b-08ade50cbf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['基金', 0.9534446597099304]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92530635-5cdb-4d80-bf9b-ec66d9a38019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'南极洲科学家在极地探险活动中发现了一种非常独特的生物。据报道，这种生物被称为“极地美人鱼”，因其美丽的外表而得名。这种生物在南极冰海中深度达到3000米的一个区域被发现。\\n科学家们描述了这种生物的外观和习性，他们说这种“美人鱼”长约30厘米，身体呈透明色，看起来像一颗水晶球。这种生物没有脊椎骨，而是由一些僵硬的角质物质组成，这些物质覆盖在身体的表面上。\\n据科学家们表示，这种“美人鱼”是几十年来发现的首个属于此类的生物。习性十分独特，这种生物似乎能够在极寒的水下生存四百年之久，而且并不患病或变得晚年。科学家们对这种生物的研究显示“美人鱼”拥有一种特殊的代谢机制，可以将营养物质在应用微生物进行新陈代谢时持续不断地重新利用。\\n科学家们还表示，他们将进一步研究这种活了几百年的生物所拥有的代谢机制，并将寻求与“美人鱼”相关的抗老化疗法来延长人类寿命。这项发现对于人类未来的医学和科学研究来说有着重大的影响。\\n据悉，目前科学家们正在南极洲进行长达数月的探险。他们希望能够发现更多的特殊生物并为我们了解极地生态环境带来新的认识。\\n值得注意的是，随着气候变化的不断进行，科学家们担心南极洲这些地区的生态环境可能会受到极大的影响，这使得探险活动的重要性更加突出。科学家们希望这项发现能够引起人们对极地’环境保护的关注，让世界更好地了解这个神秘而美丽的地方。'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "459abe26-31f3-48b2-a96d-24a672db08c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "v4_valid = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_valid.json') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        v4_valid.append(content)\n",
    "        \n",
    "v4_knn_valid = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_valid.json.topic.knn.final') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        v4_knn_valid.append(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e01bf632-a22d-4eb6-bf37-83b08022a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "v5_valid = []\n",
    "v5_text = []\n",
    "v5_label = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v5/biake_qa_web_text_zh_valid.json') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        v5_valid.append(content)\n",
    "        v5_text.append(content['text'])\n",
    "        v5_label.append(topic_api.label2id[content['label'][0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e032b15-992f-4b35-8e63-41e6da7852ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83069/83069 [00:22<00:00, 3750.61it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict_data(data, api):\n",
    "    queue = []\n",
    "    pred_score = []\n",
    "    tt = []\n",
    "    for item in tqdm(data):\n",
    "        if isinstance(item, list):\n",
    "            text = \"\\n\".join(item)\n",
    "        else:\n",
    "            text = item\n",
    "        queue.append(text)\n",
    "        tt.append(item)\n",
    "        if np.mod(len(queue), 128) == 0:\n",
    "            result_list = api.infer_batch(queue)\n",
    "            for result, text, t in zip(result_list, queue, tt):\n",
    "                pred_score.append(result)\n",
    "            queue = []\n",
    "            tt = []\n",
    "    if queue:\n",
    "        result_list = api.infer_batch(queue)\n",
    "        for result, text, t in zip(result_list, queue, tt):\n",
    "            pred_score.append(result)\n",
    "    return pred_score\n",
    "\n",
    "pred_score = predict_data(v5_text, topic_api)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f6bfab5b-b049-4730-a06a-6debef15a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "v5_pred_score_np = np.array(pred_score)\n",
    "v5_label_np = np.array(v5_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6c32246-dfab-45c5-bff6-e9fbe4115c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup\n",
    "n = 70000 # number of calibration points\n",
    "alpha = 0.1 # 1-alpha is the desired coverage\n",
    "\n",
    "# Split the softmax scores into calibration and validation sets (save the shuffling)\n",
    "idx = np.array([1] * n + [0] * (v5_pred_score_np.shape[0]-n)) > 0\n",
    "np.random.shuffle(idx)\n",
    "cal_smx, val_smx = v5_pred_score_np[idx,:], v5_pred_score_np[~idx,:]\n",
    "cal_labels, val_labels = v5_label_np[idx], v5_label_np[~idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6f0dfdb-2317-4449-9796-8dcf9beb4873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The empirical coverage is: 0.999158313566455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36351/845887284.py:5: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  qhat = np.quantile(cal_scores, np.ceil((n+1)*(1-alpha))/n, interpolation='higher')\n"
     ]
    }
   ],
   "source": [
    "# Get scores. calib_X.shape[0] == calib_Y.shape[0] == n\n",
    "cal_pi = cal_smx.argsort(1)[:,::-1]; cal_srt = np.take_along_axis(cal_smx,cal_pi,axis=1).cumsum(axis=1)\n",
    "cal_scores = np.take_along_axis(cal_srt,cal_pi.argsort(axis=1),axis=1)[range(n),cal_labels]\n",
    "# Get the score quantile\n",
    "qhat = np.quantile(cal_scores, np.ceil((n+1)*(1-alpha))/n, interpolation='higher')\n",
    "# Deploy (output=list of length n, each element is tensor of classes)\n",
    "val_pi = val_smx.argsort(1)[:,::-1]; val_srt = np.take_along_axis(val_smx,val_pi,axis=1).cumsum(axis=1)\n",
    "prediction_sets = np.take_along_axis(val_srt <= qhat,val_pi.argsort(axis=1),axis=1)\n",
    "\n",
    "# Calculate empirical coverage\n",
    "empirical_coverage = prediction_sets[np.arange(prediction_sets.shape[0]),val_labels].mean()\n",
    "print(f\"The empirical coverage is: {empirical_coverage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb6710bc-de1c-4a74-9610-f12d0e316f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ee3b4d81-6858-4e4b-9d7b-3d42aa671ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "def evaluation_ece(pred_score, gold, mapping_dict):\n",
    "    pass\n",
    "\n",
    "def eval_all(data, model, top_n=5):\n",
    "    pred = []\n",
    "    gold = []\n",
    "    pred_score = []\n",
    "    pred = 0\n",
    "    queue = []\n",
    "    tt = []\n",
    "    total_pred = []\n",
    "    for item in tqdm(data):\n",
    "        gold.append(item['label'][0])\n",
    "        if isinstance(item['text'], list):\n",
    "            text = \"\\n\".join(item['text'])\n",
    "        else:\n",
    "            text = item['text']\n",
    "        queue.append(text)\n",
    "        tt.append(item)\n",
    "        if np.mod(len(queue), 128) == 0:\n",
    "            result_list = model.predict_batch(queue)\n",
    "            for result, text, t in zip(result_list, queue, tt):\n",
    "                score = sorted(result, key=lambda u:u[1], reverse=True)\n",
    "                pred_set = set([p[0] for p in score[:top_n]])\n",
    "                total_pred.append(score[0][0])\n",
    "                if set(t['label']) & pred_set:\n",
    "                    pred += 1\n",
    "                pred_score.append(result)\n",
    "            queue = []\n",
    "            tt = []\n",
    "    if queue:\n",
    "        result_list = model.predict_batch(queue)\n",
    "        for result, text, t in zip(result_list, queue, tt):\n",
    "            score = sorted(result, key=lambda u:u[1], reverse=True)\n",
    "            pred_set = set([p[0] for p in score[:top_n]])\n",
    "            total_pred.append(score[0][0])\n",
    "            if set(t['label']) & pred_set:\n",
    "                pred += 1\n",
    "            pred_score.append(result)\n",
    "        # break\n",
    "    print(classification_report(gold, total_pred, digits=4), '===', top_n)\n",
    "    print(pred/len(pred_score))\n",
    "    return pred_score, total_pred, gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "503e13f2-44b1-4f69-8252-98be975a4f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['城市', 0.9653037190437317],\n",
       " ['文化/艺术', 0.0006875364342704415],\n",
       " ['教育/科学', 0.0003225688706152141],\n",
       " ['社会', 0.00032179488334804773],\n",
       " ['宠物', 0.00030149819212965667]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9079a934-ea45-4133-9658-fc0ede171e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for item in v4_valid:\n",
    "    if item['label'][0] in ['道德伦理']:\n",
    "        target.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9db3c1ea-6001-46f7-b5aa-f489d12b2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_result = []\n",
    "for item in target:\n",
    "    result = topic_api.predict(item['text'])\n",
    "    if result[0][0] not in item['label']:\n",
    "        target_result.append((result, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b58e5c6a-6e79-4c26-b277-c0aaea2fec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83069/83069 [00:28<00:00, 2945.79it/s]\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BDSM     0.5000    0.6667    0.5714        15\n",
      "        LGBT     0.6891    0.7413    0.7143       317\n",
      "         VPN     1.0000    0.2000    0.3333         5\n",
      "          两性     0.3383    0.1069    0.1625       636\n",
      "          中医     0.7398    0.6233    0.6766       146\n",
      "          二手     0.5000    0.1053    0.1739        19\n",
      "          交友     0.0000    0.0000    0.0000        25\n",
      "        交通出行     0.7128    0.7339    0.7232       372\n",
      "          人物     0.5482    0.4954    0.5205       218\n",
      "          人类     0.5000    0.0690    0.1212        29\n",
      "        人际交往     0.4632    0.3288    0.3846       669\n",
      "         价值观     0.3721    0.2112    0.2695       303\n",
      "       体育/运动     0.7917    0.8538    0.8216      2039\n",
      "          体验     0.4211    0.0656    0.1135       122\n",
      "          保险     0.7360    0.7931    0.7635       116\n",
      "          健康     0.7653    0.8045    0.7844      5754\n",
      "          公司     0.6408    0.5872    0.6128       872\n",
      "          养生     0.4286    0.1237    0.1920       291\n",
      "          写作     0.5185    0.1918    0.2800        73\n",
      "          军事     0.6076    0.7724    0.6802       848\n",
      "         冷知识     0.0000    0.0000    0.0000        32\n",
      "        创业投资     0.6152    0.4232    0.5015       801\n",
      "         动植物     0.4222    0.1863    0.2585       102\n",
      "          动漫     0.6613    0.7949    0.7220       673\n",
      "          博彩     0.0000    0.0000    0.0000         0\n",
      "          历史     0.6656    0.6179    0.6409      1662\n",
      "          吸烟     1.0000    0.2500    0.4000         4\n",
      "       商业/理财     0.5701    0.5021    0.5339       713\n",
      "          国家     0.5928    0.4892    0.5360      1110\n",
      "          城市     0.7082    0.5475    0.6175       727\n",
      "          基金     0.8330    0.8057    0.8191       489\n",
      "          夜店     0.3333    1.0000    0.5000         1\n",
      "          天气     0.2500    0.1429    0.1818         7\n",
      "          女性     0.3735    0.2360    0.2893       394\n",
      "          女权     0.5833    0.4795    0.5263       146\n",
      "          娱乐     0.5988    0.3615    0.4508       805\n",
      "          婚姻     0.5186    0.5877    0.5510       616\n",
      "          学习     0.5470    0.2661    0.3580       590\n",
      "          宗教     0.6641    0.7969    0.7244       640\n",
      "          宠物     0.7278    0.8295    0.7753       651\n",
      "          审美     0.0000    0.0000    0.0000        19\n",
      "        家居装修     0.6124    0.7899    0.6899       414\n",
      "        家庭关系     0.3669    0.3554    0.3611       287\n",
      "          家电     0.2647    0.1552    0.1957        58\n",
      "          小说     0.6417    0.6837    0.6620       765\n",
      "        市场营销     0.5781    0.2434    0.3426       152\n",
      "          常识     0.4574    0.3039    0.3652       849\n",
      "          广告     0.5000    0.5400    0.5192        50\n",
      "          建筑     0.5493    0.4875    0.5166        80\n",
      "          影视     0.7412    0.8328    0.7844      2710\n",
      "        心理健康     0.5018    0.5043    0.5030      1981\n",
      "          思维     0.2727    0.2308    0.2500        13\n",
      "         性侵犯     0.0000    0.0000    0.0000         6\n",
      "         性生活     0.5668    0.6613    0.6104       815\n",
      "         性骚扰     0.0000    0.0000    0.0000         5\n",
      "          恋爱     0.4551    0.8217    0.5858      1941\n",
      "        恐怖主义     0.0000    0.0000    0.0000         7\n",
      "          恶俗     0.0000    0.0000    0.0000         8\n",
      "          情商     0.1212    0.1905    0.1481        21\n",
      "          情感     0.3050    0.1070    0.1585       738\n",
      "          惊悚     0.0000    0.0000    0.0000         7\n",
      "          成长     0.4167    0.0568    0.1000        88\n",
      "          战争     0.0000    0.0000    0.0000        50\n",
      "         房地产     0.4762    0.2083    0.2899        48\n",
      "          抄袭     0.4375    0.3889    0.4118        18\n",
      "          摄影     0.6734    0.7389    0.7046       452\n",
      "          故事     0.4205    0.3394    0.3756       109\n",
      "       教育/科学     0.6950    0.7191    0.7069      7188\n",
      "       文化/艺术     0.6198    0.5813    0.5999      3320\n",
      "          旅游     0.7147    0.7362    0.7253       997\n",
      "        时事政治     0.5419    0.5230    0.5322      1176\n",
      "          时尚     0.6337    0.6400    0.6368       811\n",
      "          明星     0.5752    0.6512    0.6108       840\n",
      "          星座     0.6633    0.8639    0.7505       463\n",
      "          暴力     0.2222    0.2000    0.2105        10\n",
      "          期货     0.4000    0.2500    0.3077        16\n",
      "        校园生活     0.5075    0.5830    0.5426      1506\n",
      "          歧视     0.1429    0.0769    0.1000        13\n",
      "          死亡     0.0000    0.0000    0.0000        18\n",
      "          毒品     0.5000    0.8947    0.6415        19\n",
      "          民族     0.3846    0.1471    0.2128        34\n",
      "          汽车     0.8564    0.9133    0.8839      1130\n",
      "          法律     0.6929    0.6600    0.6761      1309\n",
      "          游戏     0.9198    0.9496    0.9345      9595\n",
      "         潜规则     0.0000    0.0000    0.0000         2\n",
      "        灵异灵修     0.4643    0.5200    0.4906        25\n",
      "        灾害意外     0.3333    0.4286    0.3750         7\n",
      "          炫富     0.0000    0.0000    0.0000         2\n",
      "          爱国     0.2000    0.1667    0.1818         6\n",
      "          爱情     0.3333    0.0212    0.0398       189\n",
      "          环境     0.6333    0.4043    0.4935        47\n",
      "          生活     0.4858    0.2991    0.3702       973\n",
      "        电子商务     0.6628    0.4872    0.5616       117\n",
      "        电子数码     0.7369    0.7534    0.7451      1379\n",
      "       电脑/网络     0.7988    0.8320    0.8151      4977\n",
      "          男性     0.5455    0.0606    0.1091        99\n",
      "          相貌     1.0000    0.0833    0.1538        12\n",
      "          睡眠     0.5714    0.3429    0.4286        35\n",
      "          社会     0.4171    0.3348    0.3714      1398\n",
      "          神话     0.1250    0.1429    0.1333         7\n",
      "          移民     0.3684    0.4118    0.3889        17\n",
      "          算命     0.4512    0.6491    0.5324        57\n",
      "          经济     0.4936    0.4122    0.4492       279\n",
      "          编程     0.5927    0.6194    0.6058       289\n",
      "        网络安全     0.4286    0.4286    0.4286        28\n",
      "       美容/塑身     0.6355    0.8201    0.7160      1167\n",
      "       美食/烹饪     0.7044    0.7843    0.7422      1270\n",
      "        职场职业     0.5929    0.5404    0.5654      1275\n",
      "          股票     0.8266    0.9255    0.8733      1422\n",
      "          育儿     0.6602    0.7113    0.6848      1202\n",
      "        航空航天     0.6887    0.6759    0.6822       108\n",
      "          色情     0.5000    0.2162    0.3019        37\n",
      "          语言     0.5237    0.4944    0.5086       358\n",
      "        财务税务     0.7996    0.7487    0.7734       597\n",
      "        购房置业     0.5925    0.7621    0.6667       206\n",
      "          购物     0.5143    0.3273    0.4000       165\n",
      "        资本主义     0.0000    0.0000    0.0000         4\n",
      "        资源共享     0.0000    0.0000    0.0000        33\n",
      "          赚钱     0.0000    0.0000    0.0000        15\n",
      "        道德伦理     0.5000    0.1250    0.2000        96\n",
      "          金融     0.5000    0.0625    0.1111        16\n",
      "          银行     0.6708    0.7246    0.6967       374\n",
      "         青春期     0.0000    0.0000    0.0000         3\n",
      "          音乐     0.7236    0.8436    0.7790      1496\n",
      "          风水     0.2941    0.2174    0.2500        23\n",
      "          食品     0.3846    0.1136    0.1754        44\n",
      "       马克思主义     0.5000    0.1250    0.2000         8\n",
      "          高铁     0.7027    0.7027    0.7027        37\n",
      "\n",
      "    accuracy                         0.6878     83069\n",
      "   macro avg     0.4791    0.4151    0.4190     83069\n",
      "weighted avg     0.6761    0.6878    0.6749     83069\n",
      " === 2\n",
      "0.8176576099387256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_score, total_pred, gold = eval_all(v5_valid, topic_api, top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "691e3a33-05df-4ea8-89ad-74ccea59fa0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36351/3289168925.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation_ece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv5_pred_score_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_36351/4052255843.py\u001b[0m in \u001b[0;36mevaluation_ece\u001b[0;34m(pred_score, gold, mapping_dict)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mece_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mECE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_bins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mece_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_score_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'probs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'==ece=='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36351/4152384178.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, logits, labels, mode)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0msoftmaxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# softmaxes = F.softmax(logits, dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mconfidences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmaxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "evaluation_ece(v5_pred_score_np, gold, topic_api.label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a21c3ebe-0d46-41a6-82d6-b572fa1b2bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2866]) ==ece==\n"
     ]
    }
   ],
   "source": [
    "pred_score_l = []\n",
    "pred_score_l = torch.tensor(v5_pred_score_np)\n",
    "gold_l = torch.tensor([topic_api.label2id[item] for item in gold])\n",
    "\n",
    "ece_fn = ECE(n_bins=15)\n",
    "print(ece_fn(pred_score_l, gold_l, mode='probs'), '==ece==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a33909b9-2441-4770-9797-8a51ce28f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_score, gold = eval_all(v4_knn_valid, topic_api, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80701227-d75e-4e55-83a9-d2f778b2ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_score, gold = eval_all(v4_valid, topic_api, top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e24f478-fd77-4609-a70c-5e7c027a7c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25507it [00:11, 2218.33it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/data/albert.xht/xiaodao/topic_classification_v7/biake_qa_web_text_zh_train.json.positive.topic', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/xiaodao/topic_classification_v7/biake_qa_web_text_zh_train.json.positive', 'r') as frobj:\n",
    "        queue = []\n",
    "        t = []\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('请问', '', content['text'])\n",
    "            queue.append(content['text'])\n",
    "            t.append(content)\n",
    "            if np.mod(len(queue), 32) == 0:\n",
    "                probs = topic_api.predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tt['label'] = prob_dict\n",
    "                    fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "                queue = []\n",
    "                t = []\n",
    "        if queue:\n",
    "            probs = topic_api.predict_batch(queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                tt['label'] = prob_dict\n",
    "                fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56e9453f-9799-4384-be3d-71c947fa57d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23116it [00:10, 2115.69it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/embed_linear_small_white.json.topic', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/embed_linear_small_white.json', 'r') as frobj:\n",
    "        queue = []\n",
    "        t = []\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('请问', '', content['text'])\n",
    "            queue.append(content['text'])\n",
    "            t.append(content)\n",
    "            if np.mod(len(queue), 32) == 0:\n",
    "                probs = topic_api.predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tt['label'] = prob_dict\n",
    "                    fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "                queue = []\n",
    "                t = []\n",
    "        if queue:\n",
    "            probs = topic_api.predict_batch(queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                tt['label'] = prob_dict\n",
    "                fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "54a00c12-49c1-402c-91eb-6f76a71e3df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24721it [00:13, 1896.10it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/query_risk_corpus.json.risk_topic', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/query_risk_corpus.json.risk', 'r') as frobj:\n",
    "        queue = []\n",
    "        t = []\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('请问', '', content['text'])\n",
    "            queue.append(content['text'])\n",
    "            t.append(content)\n",
    "            if np.mod(len(queue), 32) == 0:\n",
    "                probs = topic_api.predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tt['topic'] = prob_dict\n",
    "                    fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "                queue = []\n",
    "                t = []\n",
    "        if queue:\n",
    "            probs = topic_api.predict_batch(queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                tt['topic'] = prob_dict\n",
    "                fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80f79d4b-7a85-449a-9eb8-11af3ed0d98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2071401it [16:00, 2157.32it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json,new_topic', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json', 'r') as frobj:\n",
    "        queue = []\n",
    "        t = []\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('请问', '', content['text'])\n",
    "            queue.append(content['text'])\n",
    "            t.append(content)\n",
    "            if np.mod(len(queue), 32) == 0:\n",
    "                probs = topic_api.predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tt['new_topic'] = prob_dict\n",
    "                    fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "                queue = []\n",
    "                t = []\n",
    "        if queue:\n",
    "            probs = topic_api.predict_batch(queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                tt['new_topic'] = prob_dict\n",
    "                fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3f851e7-c69b-4583-a8ba-39abe24dd7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241035it [02:58, 1348.68it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/data/albert.xht/sentiment/green_teenager.json.all.detail.topic', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/sentiment/green_teenager.json.all.detail', 'r') as frobj:\n",
    "        queue = []\n",
    "        t = []\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('请问', '', content['text'])\n",
    "            if len(content['text']) > 192:\n",
    "                continue\n",
    "            queue.append(content['text'])\n",
    "            t.append(content)\n",
    "            if np.mod(len(queue), 32) == 0:\n",
    "                probs = topic_api.predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tt['topic'] = prob_dict\n",
    "                    fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "                queue = []\n",
    "                t = []\n",
    "        if queue:\n",
    "            probs = topic_api.predict_batch(queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                tt['topic'] = prob_dict\n",
    "                fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec75ce3c-939a-4078-ab93-622f5578fd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:11, 1762.52it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "with open('/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8.topic', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8', 'r') as frobj:\n",
    "        queue = []\n",
    "        t = []\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('请问', '', content['text'])\n",
    "            title = content['title'] #''.join(re.split('[\\s,]', content['title'])[1:])\n",
    "            if len(title) > 192:\n",
    "                continue\n",
    "            queue.append(title)\n",
    "            t.append(content)\n",
    "            if np.mod(len(queue), 32) == 0:\n",
    "                probs = topic_api.predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tmp = {\n",
    "                        'text':text,\n",
    "                        'topic':prob_dict,\n",
    "                        'source':'efaqa'\n",
    "                    }\n",
    "                    fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')\n",
    "                queue = []\n",
    "                t = []\n",
    "        if queue:\n",
    "            probs = topic_api.predict_batch(queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                tmp = {\n",
    "                        'text':text,\n",
    "                        'topic':prob_dict,\n",
    "                        'source':'efaqa'\n",
    "                    }\n",
    "                fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e3ad02d-bab2-483a-89a2-1e9e7c54ab0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20641it [00:10, 1970.45it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "with open('/data/albert.xht/xiaodao/query_risk_v11/offensive_select_labeled.txt.topic', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/xiaodao/query_risk_v11/offensive_select_labeled.txt', 'r') as frobj:\n",
    "        queue = []\n",
    "        t = []\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('请问', '', content['text'])\n",
    "            title = content['text'] #''.join(re.split('[\\s,]', content['title'])[1:])\n",
    "            if len(title) > 192:\n",
    "                continue\n",
    "            queue.append(title)\n",
    "            t.append(content)\n",
    "            if np.mod(len(queue), 32) == 0:\n",
    "                probs = topic_api.predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    tmp = {\n",
    "                        'text':text,\n",
    "                        'topic':prob_dict,\n",
    "                        'label':tt['label'],\n",
    "                        'source':'efaqa'\n",
    "                    }\n",
    "                    fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')\n",
    "                queue = []\n",
    "                t = []\n",
    "        if queue:\n",
    "            probs = topic_api.predict_batch(queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                tmp = {\n",
    "                        'text':text,\n",
    "                        'topic':prob_dict,\n",
    "                        'label':tt['label'],\n",
    "                        'source':'efaqa'\n",
    "                    }\n",
    "                fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "278e1b29-5771-4b72-b24e-1409ce4bf106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/疑似有风险query_from对话预训练数据-20221130.txt') as frobj:\n",
    "    for line in tqdm(frobj):\n",
    "        text = line.strip()\n",
    "        result_list.append((text, result))\n",
    "\n",
    "topic_filter = []\n",
    "topic_white = []\n",
    "left = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/query_risk_corpus.json.risk_topic') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['score_list']['senti'][1][1] > 0.7:\n",
    "            if content['topic'][0][0] in ['经济', '历史', 'LGBT', 'BDSM', '法律', '灵异灵修', '国家', '社会', '军事', '心理健康', '时事政治', '死亡', '毒品', '恐怖主义', '战争', '灵异事件']:\n",
    "                topic_filter.append(content)\n",
    "            else:\n",
    "                if '艾滋病' in content['text'] or '抑郁' in content['text']:\n",
    "                    topic_filter.append(content)\n",
    "                else:\n",
    "                    topic_white.append(content)\n",
    "        else:\n",
    "            topic_filter.append(content)\n",
    "        # if content['topic'][0][0] in ['文化/艺术', '动漫', '音乐', '娱乐', '体育/运动', '小说', '星座', '时尚', '健康', '明星', '游戏'] and content['topic'][0][1] > 0.2:\n",
    "        #     if content['score_list']['senti'][0][1] < 0.3:\n",
    "        #         topic_white.append(content)\n",
    "        #     elif content['score_list']['senti'][0][1] > 0.9:\n",
    "        #         topic_filter.append(content)\n",
    "        # elif content['topic'][0][0] in ['LGBT', 'BDSM', '法律', '灵异灵修', '国家', '社会', '军事', '心理健康', '时事政治', '死亡', '毒品', '恐怖主义', '战争', '灵异事件']:\n",
    "        #     topic_filter.append(content)\n",
    "        # elif content['topic'][1][0] in ['LGBT', 'BDSM', '法律', '灵异灵修', '国家', '社会', '军事', '心理健康', '时事政治', '死亡', '毒品', '恐怖主义', '战争', '灵异事件']:\n",
    "        #     topic_filter.append(content)\n",
    "        # else:\n",
    "        #     left.append(content)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5278721d-7cb6-4c64-8889-812254a6f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/query_risk_corpus.json.filter', 'w') as fwobj:\n",
    "    for d in topic_filter:\n",
    "        d['label'] = ['风险']\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/query_risk_corpus.json.white', 'w') as fwobj:\n",
    "    for d in topic_white:\n",
    "        d['label'] = ['正常']\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d585b1d0-9c3d-4403-b611-06676e88525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.extend(['/root/xiaoda/query_topic/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "840c0f06-8bd8-4a04-b5dc-4a3a735977f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyword_processor import KeywordProcesser\n",
    "with open('/data/albert.xht/xiaoda/sentiment/query_risk_v14//risk_event.txt') as frobj:\n",
    "    for line in frobj:\n",
    "        content = line.strip()\n",
    "    keyword_list = content.split('||')\n",
    "\n",
    "keyword_api_v1 = KeywordProcesser(keywords=keyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7682e6b3-655c-429a-80d3-20ad6e16e3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1973027it [03:03, 10726.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import jieba_fast as jieba\n",
    "for word in keyword_list:\n",
    "    jieba.add_word(word)\n",
    "    \n",
    "from tqdm import tqdm\n",
    "import json, re\n",
    "\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v5/biake_qa_web_text_zh_train.json.knn.final.keyword', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/raw_chat_corpus/topic_classification_v5/biake_qa_web_text_zh_train.json.knn.final') as frobj:\n",
    "        for line in tqdm(frobj):\n",
    "            content = json.loads(line.strip())\n",
    "            text = re.sub(u'[^\\u4e00-\\u9fa50-9a-zA-Z ]+', '\\n', content['text'].lower())\n",
    "            content['keywords'] = keyword_api_v1.extract_keywords(text)\n",
    "            words_list = list(jieba.cut(text))\n",
    "            words_set = {}\n",
    "            for word in words_list:\n",
    "                words_set[word] = ''\n",
    "            if content['keywords']:\n",
    "                keyword_list = []\n",
    "                for keyword_ in content['keywords']:\n",
    "                    if keyword_[-1] in words_set:\n",
    "                        keyword_list.append(keyword_)\n",
    "                if keyword_list:\n",
    "                    if content['label'][0] not in ['游戏', '小说', '网络安全', '动漫', '幽默滑稽', '电子数码']:\n",
    "                        content['topic'] = content['label']\n",
    "                        content['label'] = ['风险']\n",
    "                        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "                    elif content['label'][0] in ['游戏', '小说', '网络安全', '动漫', '电子数码']:\n",
    "                        content['topic'] = content['label']\n",
    "                        content['label'] = ['正常']\n",
    "                        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "364778bf-42c7-4480-8afd-e9f47d72fb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['j', 'av', 'aweb', '程序员', '是否', '适合', '用', 'MacBook']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(jieba.cut('javaweb程序员是否适合用MacBook'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e295b-b06d-406b-b24d-7b90e2e64847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
