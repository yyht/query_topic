{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a5c82e8-bb94-41b5-a367-498ba4af5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys,os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "399b4feb-bad5-4b66-b52f-8c556cbaafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.extend(['/root/xiaoda/query_topic/'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da59d2cc-b5a2-46da-ac4e-33e75409552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/ondrejbohdal/meta-calibration/blob/main/Metrics/metrics.py\n",
    "\"\"\"\n",
    "\n",
    "class ECE(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(ECE, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, logits, labels, mode='logits'):\n",
    "        if mode == 'logits':\n",
    "            softmaxes = F.softmax(logits, dim=1)\n",
    "        else:\n",
    "            softmaxes = logits\n",
    "        # softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "        \n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1e079e4e-bb09-4aa2-b982-5560f7ad75bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast\n",
    "import transformers\n",
    "from datetime import timedelta\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from nets.them_classifier import MyBaseModel, RobertaClassifier\n",
    "from nets.simcse import MLPLayer, Similarity\n",
    "\n",
    "import configparser\n",
    "from tqdm import tqdm\n",
    "\n",
    "cur_dir_path = '/root/xiaoda/query_topic/'\n",
    "\n",
    "def load_label(filepath):\n",
    "    label_list = []\n",
    "    with open(filepath, 'r') as frobj:\n",
    "        for line in frobj:\n",
    "            label_list.append(line.strip())\n",
    "        n_classes = len(label_list)\n",
    "\n",
    "        label2id = {}\n",
    "        id2label = {}\n",
    "        for idx, label in enumerate(label_list):\n",
    "            label2id[label] = idx\n",
    "            id2label[idx] = label\n",
    "        return label2id, id2label\n",
    "\n",
    "class RiskInfer(object):\n",
    "    def __init__(self, config_path):\n",
    "\n",
    "        import torch, os, sys\n",
    "\n",
    "        con = configparser.ConfigParser()\n",
    "        con_path = os.path.join(cur_dir_path, config_path)\n",
    "        con.read(con_path, encoding='utf8')\n",
    "\n",
    "        args_path = dict(dict(con.items('paths')), **dict(con.items(\"para\")))\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(args_path[\"model_path\"], do_lower_case=True)\n",
    "\n",
    "        from collections import OrderedDict\n",
    "        self.schema_dict = OrderedDict({})\n",
    "        self.schema2schema_id = {}\n",
    "        self.schema_id2schema = {}\n",
    "\n",
    "        for label_index, schema_info in enumerate(args_path[\"label_path\"].split(',')):\n",
    "            schema_type, schema_path = schema_info.split(':')\n",
    "            schema_path = os.path.join(cur_dir_path, schema_path)\n",
    "            print(schema_type, schema_path, '===schema-path===')\n",
    "            label2id, id2label = load_label(schema_path)\n",
    "            self.schema_dict[schema_type] = {\n",
    "                'label2id':label2id,\n",
    "                'id2label':id2label,\n",
    "                'label_index':label_index\n",
    "            }\n",
    "            # print(self.schema_dict[schema_type], '==schema_type==', schema_type)\n",
    "            self.schema2schema_id[schema_type] = label_index\n",
    "            self.schema_id2schema[label_index] = schema_type\n",
    "        \n",
    "        output_path = os.path.join(cur_dir_path, args_path['output_path'])\n",
    "\n",
    "        # from roformer import RoFormerModel, RoFormerConfig\n",
    "        if args_path.get('model_type', 'bert') == 'bert':\n",
    "            from transformers import BertModel, BertConfig\n",
    "            config = BertConfig.from_pretrained(args_path[\"model_path\"])\n",
    "            encoder = BertModel(config=config)\n",
    "        elif args_path.get('model_type', 'bert') == 'roformer':\n",
    "            from roformer import RoFormerModel, RoFormerConfig\n",
    "            config = RoFormerConfig.from_pretrained(args_path[\"model_path\"])\n",
    "            encoder = RoFormerModel(config=config)\n",
    "        elif args_path.get('model_type', 'bert') == 'erine':\n",
    "            from nets.erine import ErnieConfig, ErnieModel\n",
    "            config = ErnieConfig.from_pretrained(args_path[\"model_path\"])\n",
    "            encoder = ErnieModel(config=config)\n",
    "            \n",
    "        print(args_path.get('model_type', 'bert'))\n",
    "        \n",
    "        encoder_net = MyBaseModel(encoder, config)\n",
    "\n",
    "        self.device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        classifier_list = []\n",
    "\n",
    "        schema_list = list(self.schema_dict.keys())\n",
    "\n",
    "        for schema_key in schema_list:\n",
    "            classifier = RobertaClassifier(\n",
    "                hidden_size=config.hidden_size, \n",
    "                dropout_prob=con.getfloat('para', 'out_dropout_rate'),\n",
    "                num_labels=len(self.schema_dict[schema_key]['label2id']), \n",
    "                dropout_type=con.get('para', 'dropout_type'))\n",
    "            classifier_list.append(classifier)\n",
    "\n",
    "        classifier_list = nn.ModuleList(classifier_list)\n",
    "\n",
    "        class MultitaskClassifier(nn.Module):\n",
    "            def __init__(self, transformer, classifier_list):\n",
    "                super().__init__()\n",
    "\n",
    "                self.transformer = transformer\n",
    "                self.classifier_list = classifier_list\n",
    "                \n",
    "                self.pooler_mlp = MLPLayer(config.hidden_size, 256)\n",
    "\n",
    "            def forward(self, input_ids, input_mask, \n",
    "                        segment_ids=None, \n",
    "                        transformer_mode='mean_pooling', \n",
    "                        dt_idx=None, mode='predict'):\n",
    "                hidden_states = self.transformer(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              return_mode=transformer_mode)\n",
    "                outputs_list = []\n",
    "                \n",
    "                for idx, classifier in enumerate(self.classifier_list):\n",
    "                    \n",
    "                    if dt_idx:\n",
    "                        if idx not in dt_idx:\n",
    "                            outputs_list.append([])\n",
    "                            continue\n",
    "                    \n",
    "                    scores = classifier(hidden_states)\n",
    "                    if mode == 'predict':\n",
    "                        scores = torch.nn.Softmax(dim=1)(scores)\n",
    "                    outputs_list.append(scores)\n",
    "                pooler_output = self.pooler_mlp(hidden_states)\n",
    "                embeddings = hidden_states / hidden_states.norm(dim=1, keepdim=True)\n",
    "                pooler_output_embeddings = pooler_output / pooler_output.norm(dim=1, keepdim=True)\n",
    "                return outputs_list, hidden_states, embeddings, pooler_output_embeddings\n",
    "\n",
    "        self.net = MultitaskClassifier(encoder_net, classifier_list).to(self.device)\n",
    "\n",
    "        # eo = 9\n",
    "        # ckpt = torch.load(os.path.join(output_path, 'multitask_cls.pth.{}.raw'.format(eo)), map_location=self.device)\n",
    "        # # ckpt = torch.load(os.path.join(output_path, 'multitask_cls.pth.{}.raw.focal'.format(eo)), map_location=self.device)\n",
    "        # # ckpt = torch.load(os.path.join(output_path, 'multitask_contrast_cls.pth.{}'.format(eo)), map_location=self.device)\n",
    "        # self.net.load_state_dict(ckpt)\n",
    "        # self.net.eval()\n",
    "        \n",
    "    def reload(self, model_path):\n",
    "        ckpt = torch.load(model_path, map_location=self.device)\n",
    "        self.net.load_state_dict(ckpt)\n",
    "        self.net.eval()\n",
    "        self.net = self.net.half()\n",
    "\n",
    "    def predict(self, text, allowed_schema_type={}):\n",
    "\n",
    "        \"\"\"抽取输入text所包含的类型\n",
    "        \"\"\"\n",
    "        # start = time.time()\n",
    "        # encoder_txt = self.tokenizer.encode_plus(text, max_length=256)\n",
    "        # input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().unsqueeze(0).to(self.device)\n",
    "        # token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).unsqueeze(0).to(self.device)\n",
    "        # attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).unsqueeze(0).to(self.device)\n",
    "        # print(time.time() - start, '====tokenization====')\n",
    "        \n",
    "        start = time.time()\n",
    "        encoder_txt = self.tokenizer([text], max_length=512)\n",
    "        input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().to(self.device)\n",
    "        token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).to(self.device)\n",
    "        attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).to(self.device)\n",
    "        # print(time.time() - start, '====tokenization====')\n",
    "        \n",
    "        allowed_schema_type_ids = {}\n",
    "        for schema_type in allowed_schema_type:\n",
    "            allowed_schema_type_ids[self.schema2schema_id[schema_type]] = schema_type\n",
    "        \n",
    "        scores_dict = {}\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states, \n",
    "             embeddings, \n",
    "             pooler_output_embeddings] = self.net(input_ids, \n",
    "                attention_mask, token_type_ids, transformer_mode='cls', dt_idx=allowed_schema_type_ids)\n",
    "        # print(time.time() - start, '====inference====')\n",
    "        \n",
    "        old_start = time.time()\n",
    "        \n",
    "        for schema_idx, (schema_type, scores) in enumerate(zip(list(self.schema_dict.keys()), logits_list)):\n",
    "            if allowed_schema_type:\n",
    "                if schema_type not in allowed_schema_type:\n",
    "                    continue\n",
    "            scores = scores[0].data.cpu().numpy()\n",
    "            scores_dict[schema_type] = []\n",
    "            for index, score in enumerate(scores):\n",
    "                scores_dict[schema_type].append([self.schema_dict[schema_type]['id2label'][index], \n",
    "                                        float(score)])\n",
    "            if len(scores_dict[schema_type]) >= 5:\n",
    "                schema_type_scores = sorted(scores_dict[schema_type], key=lambda item:item[1], reverse=True)\n",
    "                scores_dict[schema_type] = schema_type_scores[0:5]\n",
    "        return scores_dict, embeddings.data.cpu().numpy(), pooler_output_embeddings.data.cpu().numpy()\n",
    "    \n",
    "    def get_logitnorm(self, text):\n",
    "        \"\"\"抽取输入text所包含的类型\n",
    "        \"\"\"\n",
    "        encoder_txt = self.tokenizer.encode_plus(text, max_length=512)\n",
    "        input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().unsqueeze(0).to(self.device)\n",
    "        token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).unsqueeze(0).to(self.device)\n",
    "        attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        scores_dict = {}\n",
    "        logits_norm_list = []\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states] = self.net(input_ids, \n",
    "                attention_mask, token_type_ids, transformer_mode='cls')\n",
    "            for logits in logits_list:\n",
    "                logits_norm_list.append(logits/torch.norm(logits, p=2, dim=-1, keepdim=True) + 1e-7)\n",
    "        for schema_type, logit_norm in zip(list(self.schema_dict.keys()), logits_norm_list):\n",
    "            scores_dict[schema_type] = logit_norm[0].data.cpu().numpy()\n",
    "        return scores_dict\n",
    "            \n",
    "    \n",
    "    def predict_batch(self, text, allowed_schema_type={}):\n",
    "        if isinstance(text, list):\n",
    "            text_list = text\n",
    "        else:\n",
    "            text_list = [text]\n",
    "        model_input = self.tokenizer(text_list, max_length=512, truncation=True, return_tensors=\"pt\",padding=True)\n",
    "        for key in model_input:\n",
    "            model_input[key] = model_input[key].to(self.device)\n",
    "                    \n",
    "        allowed_schema_type_ids = {}\n",
    "        for schema_type in allowed_schema_type:\n",
    "            allowed_schema_type_ids[self.schema2schema_id[schema_type]] = schema_type\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states,\n",
    "            embeddings, \n",
    "             pooler_output_embeddings] = self.net(model_input['input_ids'], \n",
    "                                                model_input['attention_mask'], \n",
    "                                                model_input['token_type_ids'], \n",
    "                                                transformer_mode='cls', \n",
    "                                                dt_idx=allowed_schema_type_ids)\n",
    "        score_dict_list = []\n",
    "        embedding_array = []\n",
    "        pooler_output_embeddings_array = []\n",
    "        for idx, text in enumerate(text_list):\n",
    "            scores_dict = {}\n",
    "            for schema_idx, (schema_type, scores) in enumerate(zip(list(self.schema_dict.keys()), logits_list)):\n",
    "                if allowed_schema_type:\n",
    "                    if schema_type not in allowed_schema_type:\n",
    "                        continue\n",
    "                # scores = torch.nn.Softmax(dim=1)(logits)[idx].data.cpu().numpy()\n",
    "                scores = scores[idx].data.cpu().numpy()\n",
    "                scores_dict[schema_type] = []\n",
    "                for index, score in enumerate(scores):\n",
    "                    scores_dict[schema_type].append([self.schema_dict[schema_type]['id2label'][index], \n",
    "                                            float(score)])\n",
    "                if len(scores_dict[schema_type]) >= 5:\n",
    "                    schema_type_scores = sorted(scores_dict[schema_type], key=lambda item:item[1], reverse=True)\n",
    "                    scores_dict[schema_type] = schema_type_scores[0:5]\n",
    "            score_dict_list.append(scores_dict)\n",
    "        embedding_array = embeddings.data.cpu().numpy()\n",
    "        pooler_output_embeddings_array = pooler_output_embeddings.data.cpu().numpy()\n",
    "        return score_dict_list, embedding_array, pooler_output_embeddings_array\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "51aab72b-fc2b-4e14-a642-0091d4a1a27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ErnieTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ErnieTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity /data/albert.xht/xiaoda/sentiment/jigsaw-unintended-bias-in-toxicity-classification/toxicity_multiclass_label_list.txt ===schema-path===\n",
      "stackoverflow_topic /data/albert.xht/xiaoda/sentiment/stackoverflow_topic/stackoverflow_topic_label_list.txt ===schema-path===\n",
      "nli /data/albert.xht/xiaoda/sentiment/classification/cmnli/cmnli_label_list.txt ===schema-path===\n",
      "lcqmc /data/albert.xht/xiaoda/sentiment/classification/paws-x-zh/paws_label_list.txt ===schema-path===\n",
      "yewu /data/albert.xht/xiaoda/sentiment/yewu_v1/yewu_label_list.txt ===schema-path===\n",
      "risk_news /data/albert.xht/xiaoda/sentiment/risk_news/risk_news_label_list.txt ===schema-path===\n",
      "tnews /data/albert.xht/xiaoda/sentiment/classification/tnews_v1/tnews_label_list.txt ===schema-path===\n",
      "title2event /data/albert.xht/xiaoda/sentiment/classification/title2event_v1/title2event_label_list.txt ===schema-path===\n",
      "fewfc_2022 /data/albert.xht/xiaoda/sentiment/classification/fewfc_2022/fewfc_2022_label_list.txt ===schema-path===\n",
      "duee /data/albert.xht/xiaoda/sentiment/classification/DuEE1.0/duee_label_list.txt ===schema-path===\n",
      "ethics_common /root/xiaoda/query_topic/risk_data_tiny_query_reseponse_open/topic_query_risk/ethics_common_label_list.txt ===schema-path===\n",
      "insult /data/albert.xht/xiaoda/measuring_hate_speech/hate_speech_label_list.txt.insult ===schema-path===\n",
      "humiliate /data/albert.xht/xiaoda/measuring_hate_speech/hate_speech_label_list.txt.humiliate ===schema-path===\n",
      "dehumanize /data/albert.xht/xiaoda/measuring_hate_speech/hate_speech_label_list.txt.dehumanize ===schema-path===\n",
      "violence /data/albert.xht/xiaoda/measuring_hate_speech/hate_speech_label_list.txt.violence ===schema-path===\n",
      "genocide /data/albert.xht/xiaoda/measuring_hate_speech/hate_speech_label_list.txt.genocide ===schema-path===\n",
      "hatespeech /data/albert.xht/xiaoda/measuring_hate_speech/hate_speech_label_list.txt.hatespeech ===schema-path===\n",
      "attack_defend /data/albert.xht/xiaoda/measuring_hate_speech/hate_speech_label_list.txt.attack_defend ===schema-path===\n",
      "query_resposne_risk /data/albert.xht/xiaoda/query_response/red_team/query_response/query_response_label.txt ===schema-path===\n",
      "cmid /data/albert.xht/xiaoda/sentiment/CMID-main/cmid_label_list.txt ===schema-path===\n",
      "topic /data/albert.xht/raw_chat_corpus/topic_classification_v4/label_list.txt ===schema-path===\n",
      "senti_query /data/albert.xht/xiaoda/sentiment/senti/senti_query_label.txt ===schema-path===\n",
      "senti /data/albert.xht/xiaoda/sentiment/senti/senti_label.txt ===schema-path===\n",
      "bias /data/albert.xht/xiaoda/sentiment/bias/bias_label.txt ===schema-path===\n",
      "ciron /data/albert.xht/xiaoda/sentiment/ciron/ciron_label.txt ===schema-path===\n",
      "intent /data/albert.xht/xiaoda/sentiment/intention_data_v2-1/label.txt ===schema-path===\n",
      "offensive /data/albert.xht/xiaoda/sentiment/offensive/offensive_label.txt ===schema-path===\n",
      "query_risk /data/albert.xht/xiaoda/sentiment/query_risk_v12/query_risk_label.txt ===schema-path===\n",
      "teenager /data/albert.xht/xiaoda/sentiment/teenager//teenager_label.txt ===schema-path===\n",
      "politics /data/albert.xht/xiaoda/sentiment/green_politics/green_politics_label.txt.detail ===schema-path===\n",
      "porn /data/albert.xht/xiaoda/sentiment/green_porn_v1/green_porn_label.txt ===schema-path===\n",
      "abusive /data/albert.xht/xiaoda/sentiment/green_abusive_v1/green_abusive_label.txt ===schema-path===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "05/28/2023 01:02:58 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erine\n"
     ]
    }
   ],
   "source": [
    "erine_green_green_topic_risk_open_all_politics_detail_api = RiskInfer('/root/xiaoda/query_topic/resources_open_all_politics_detail_yewu_en_erine_update_simcse/topic_query_risk/config.ini')\n",
    "# model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcse/opic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_all_add_nli_instruct_politics_detail_mtdnn_v39_simcse_zh_en_1e-4_odps//multitask_cls.pth.9'\n",
    "# model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcse/topic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_all_add_nli_instruct_politics_detail_mtdnn_v40_simcse_zh_en_1e-4_v3/multitask_cls.pth.9'\n",
    "# model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcse/topic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_all_add_nli_instruct_politics_detail_mtdnn_v41_simcse_zh_en_1e-4_odps_v1/multitask_cls.pth.9'\n",
    "# model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcsetopic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_mtdnn_v41_simcse_zh_en_1e-4_odps_v3_nmt_rdrop/multitask_cls.pth.9'\n",
    "\n",
    "# model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcse/topic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_mtdnn_v41_simcse_zh_en_1e-4_odps_v4_nmt_rdrop/multitask_cls.pth.9'\n",
    "\n",
    "# model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcse/topic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_mtdnn_v41_simcse_zh_en_1e-4_odps_v5_nmt_rdrop_self/multitask_cls.pth.6'\n",
    "model_path = '/data/albert.xht/xiaoda/risk_classification/erine_simcse/topic_v4_update_green_v1_teenager_v1_porn_multitask_raw_filter_senti_query_risk_v13_intent_v2-1_10_no_symbol_senti_query_senta_green_mtdnn_add_hate_speech_detail_ethics_open_mtdnn_v41_simcse_zh_en_1e-4_odps_v5_nmt_rdrop_self_sampler/multitask_cls.pth.5'\n",
    "\n",
    "erine_green_green_topic_risk_open_all_politics_detail_api.reload(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "45b2178a-9219-44bf-b7da-c7eedb8aa797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018183231353759766\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pprint import pprint\n",
    "start = time.time()\n",
    "allowed_schema_type = {\n",
    "    # 'topic':'',\n",
    "    # 'query_risk':'',\n",
    "    # 'porn':'',\n",
    "    # 'abusive':'',\n",
    "    # # 'offensive':'',\n",
    "    # 'intent':'',\n",
    "    # 'teenager':'',\n",
    "    # 'politics':''\n",
    "}\n",
    "allowed_schema_type = {}\n",
    "import re\n",
    "text = \"\"\"\n",
    "“马拉松”一词的来源与古希腊的传说有关。据传说，公元490年，波斯帝国入侵古希腊，然后在马拉松平原上展开一场激烈的战斗。古希腊的阿克里斯和亚典斯两个城邦派出的联军最终打败了波斯军队。为了向亚典斯传递这个胜利的消息，古希腊的士兵派出一位名叫菲迪皮德斯 (Pheidippides) 的快递员，让他跑到亚典斯报告胜利的消息。\\n快递员起源于马拉松城，跑了约40公里的距离才到达亚典斯，把喜讯传达给了人民。据传说，菲迪皮德斯到达亚典斯后，兴奋得喊出了“我们赢了”(Nenikékamen)的话，然后跌倒在地，不幸去世。\\n为了纪念这位勇敢之士，古希腊人开始比赛，跑步者需要跑42.195公里，以纪念菲迪皮德斯为了将胜利的消息带给雅典所完成的壮举。现在在世界范围内，马拉松比赛已经成为最具影响力和最受欢迎的长距离赛事之一。\n",
    "\"\"\"\n",
    "text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", str(text).lower())\n",
    "\n",
    "resp = erine_green_green_topic_risk_open_all_politics_detail_api.predict_batch(\n",
    "    [\n",
    "        text,\n",
    "        \"\"\"\n",
    "        2009年济南\n",
    "        \"\"\"\n",
    "    ], allowed_schema_type=allowed_schema_type)\n",
    "\n",
    "print(time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3c6e014c-729d-40bb-bded-9fee1503fe8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxicity': [['no_toxicity', 0.99853515625],\n",
       "  ['insult', 0.0007600784301757812],\n",
       "  ['toxicity', 0.00043702125549316406],\n",
       "  ['obscene', 9.97781753540039e-05],\n",
       "  ['threat', 3.49879264831543e-05]],\n",
       " 'stackoverflow_topic': [['or', 0.190185546875],\n",
       "  ['monero', 0.187255859375],\n",
       "  ['stellar', 0.1627197265625],\n",
       "  ['cardano', 0.156494140625],\n",
       "  ['drones', 0.154052734375]],\n",
       " 'nli': [['neutral', 0.970703125],\n",
       "  ['entailment', 0.01776123046875],\n",
       "  ['contradiction', 0.01177215576171875]],\n",
       " 'lcqmc': [['不相似', 0.99560546875], ['相似', 0.0044097900390625]],\n",
       " 'yewu': [['其他', 0.84716796875],\n",
       "  ['导购', 0.1512451171875],\n",
       "  ['售后', 0.001491546630859375],\n",
       "  ['事实类', 9.053945541381836e-05],\n",
       "  ['权益公平性', 1.996755599975586e-05]],\n",
       " 'risk_news': [['无', 1.0],\n",
       "  ['破产重整', 2.9206275939941406e-06],\n",
       "  ['主板/创业板/中小板/债券退市', 2.682209014892578e-06],\n",
       "  ['实控人变更', 1.7285346984863281e-06],\n",
       "  ['重大诉讼仲裁', 9.5367431640625e-07]],\n",
       " 'tnews': [['股票', 0.9931640625],\n",
       "  ['家居', 0.0030879974365234375],\n",
       "  ['科技', 0.001216888427734375],\n",
       "  ['财经', 0.001125335693359375],\n",
       "  ['房产', 0.0009260177612304688]],\n",
       " 'title2event': [['健康', 0.08544921875],\n",
       "  ['历史', 0.0831298828125],\n",
       "  ['科学', 0.0787353515625],\n",
       "  ['美食', 0.07086181640625],\n",
       "  ['音乐', 0.07025146484375]],\n",
       " 'fewfc_2022': [['经营激进', 0.026458740234375],\n",
       "  ['挤兑', 0.0254364013671875],\n",
       "  ['客户管理不善', 0.025238037109375],\n",
       "  ['市场份额减少', 0.0250396728515625],\n",
       "  ['诉讼仲裁-败诉', 0.023895263671875]],\n",
       " 'duee': [['灾害/意外-起火', 0.040802001953125],\n",
       "  ['交往-点赞', 0.032440185546875],\n",
       "  ['交往-道歉', 0.032318115234375],\n",
       "  ['组织行为-罢工', 0.031707763671875],\n",
       "  ['司法行为-举报', 0.031402587890625]],\n",
       " 'ethics_common': [['ethics_risk', 0.1676025390625],\n",
       "  ['ethics_safe', 0.83251953125]],\n",
       " 'insult': [['insult', 0.03662109375], ['no_insult', 0.96337890625]],\n",
       " 'humiliate': [['humiliate', 0.07470703125], ['no_humiliate', 0.92529296875]],\n",
       " 'dehumanize': [['dehumanize', 0.05633544921875],\n",
       "  ['no_dehumanize', 0.94384765625]],\n",
       " 'violence': [['violence', 0.00469970703125], ['no_violence', 0.9951171875]],\n",
       " 'genocide': [['genocide', 0.0012788772583007812],\n",
       "  ['no_genocide', 0.99853515625]],\n",
       " 'hatespeech': [['hatespeech', 0.005435943603515625],\n",
       "  ['no_hatespeech', 0.99462890625]],\n",
       " 'attack_defend': [['attack_defend', 0.06719970703125],\n",
       "  ['no_attack_defend', 0.9326171875]],\n",
       " 'query_resposne_risk': [['rejected', 0.0001392364501953125], ['chosen', 1.0]],\n",
       " 'cmid': [['预防', 0.0633544921875],\n",
       "  ['养生', 0.05792236328125],\n",
       "  ['病症禁忌', 0.054412841796875],\n",
       "  ['价钱', 0.051116943359375],\n",
       "  ['费用', 0.051116943359375]],\n",
       " 'topic': [['教育/科学', 0.73486328125],\n",
       "  ['电脑/网络', 0.028900146484375],\n",
       "  ['社会', 0.0283966064453125],\n",
       "  ['股票', 0.02667236328125],\n",
       "  ['星座', 0.025848388671875]],\n",
       " 'senti_query': [['负向', 0.07025146484375],\n",
       "  ['中性', 0.8017578125],\n",
       "  ['正向', 0.128173828125]],\n",
       " 'senti': [['负向', 0.319580078125], ['正向', 0.6806640625]],\n",
       " 'bias': [['偏见', 0.0013589859008789062], ['正常', 0.99853515625]],\n",
       " 'ciron': [['讽刺', 0.0008444786071777344], ['正常', 0.9990234375]],\n",
       " 'intent': [['主观评价/比较/判断', 0.0007328987121582031],\n",
       "  ['寻求建议/帮助', 9.715557098388672e-05],\n",
       "  ['其它', 0.9990234375]],\n",
       " 'offensive': [['冒犯', 0.005168914794921875], ['正常', 0.99462890625]],\n",
       " 'query_risk': [['风险', 0.0011844635009765625],\n",
       "  ['个人信息', 1.430511474609375e-06],\n",
       "  ['正常', 0.9990234375]],\n",
       " 'teenager': [['不良', 0.00843048095703125], ['正常', 0.99169921875]],\n",
       " 'politics': [['正常', 0.99853515625],\n",
       "  ['black_党和国家领导人负面', 0.00040984153747558594],\n",
       "  ['black_独立', 0.00027108192443847656],\n",
       "  ['black_政府负面', 0.0002040863037109375],\n",
       "  ['black_种族、民族歧视', 0.0001748800277709961]],\n",
       " 'porn': [['色情', 3.784894943237305e-05],\n",
       "  ['低俗', 0.0021820068359375],\n",
       "  ['色情违禁', 3.4809112548828125e-05],\n",
       "  ['正常', 0.99755859375]],\n",
       " 'abusive': [['辱骂', 9.679794311523438e-05],\n",
       "  ['口头语', 0.0007429122924804688],\n",
       "  ['正常', 0.9990234375]]}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccc9e732-01f9-40da-b20c-42dd3df327c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0175"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7000/400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "c734a339-6f23-4ca3-9e83-1e83807a18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = '/data/albert.xht/T2Ranking/data/queries.dev.tsv'\n",
    "\n",
    "def load_qid(file_name):\n",
    "    qid_list = []\n",
    "    with open(file_name) as inp:\n",
    "        for line in inp:\n",
    "            line = line.strip()\n",
    "            qid = line.split('\\t')[0]\n",
    "            qid_list.append(qid)\n",
    "    return qid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c2ebafd9-458b-4623-9c88-86cde07c0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_list = load_qid(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d89ff4fe-d173-422a-b407-55e05385e17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qid', '0', '1', '2', '3', '4', '5', '6', '7', '8']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qid_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8db5cdd7-bd85-4bd3-898a-82d40ec34016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/data/albert.xht/T2Ranking/data/queries.dev.tsv', sep=\"\\t\",header=0, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f39616bd-ed85-46c0-b725-6c2cd772c540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid                 0\n",
       "text    蜂巢取快递验证码摁错怎么办\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2285f530-254c-4812-a8ad-ffb71f2fd952",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['qid','qry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3d625aaa-0ac3-4009-ac6d-821e0213ef28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid           100\n",
       "qry    看肝癌北京哪个医院好\n",
       "Name: 100, dtype: object"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "30e78ea5-e363-47ac-963e-b877b5fbbf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "5b26b1a4-102b-4971-8713-feb1e06f7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((2, 3))\n",
    "b = np.random.random((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "9f6d3f1f-45cb-41c6-9ad9-571ab555becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "a = torch.tensor(a)\n",
    "b = torch.tensor(b)\n",
    "\n",
    "p_loss = F.kl_div(F.log_softmax(a, dim=-1), F.softmax(b, dim=-1), reduction='none')\n",
    "q_loss = F.kl_div(F.log_softmax(b, dim=-1), F.softmax(a, dim=-1), reduction='none')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "53af4b0c-cd79-4255-9c21-4aae3a0491db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0159, 0.0734], dtype=torch.float64)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_loss.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "353f4b28-8152-4f00-aba0-7cba3e54bf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0447, dtype=torch.float64)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_loss.sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9197421-1e1f-403b-b6d1-cda1b96f3d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
