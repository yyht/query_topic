{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b31a76cc-ab29-4cff-8fd7-28462713f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys,os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee39b68-4001-424c-b465-d476406f11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.extend(['/root/xiaoda/query_topic/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d521956-c19d-473b-8002-dae911e2e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/ondrejbohdal/meta-calibration/blob/main/Metrics/metrics.py\n",
    "\"\"\"\n",
    "\n",
    "class ECE(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(ECE, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, logits, labels, mode='logits'):\n",
    "        if mode == 'logits':\n",
    "            softmaxes = F.softmax(logits, dim=1)\n",
    "        else:\n",
    "            softmaxes = logits\n",
    "        # softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(labels)\n",
    "        \n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            # Calculated |confidence - accuracy| in each bin\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b71457b0-22bb-4106-bb29-af3c4fa1d4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offensive /data/albert.xht/xiaoda/sentiment/offensive/offensive_label.txt ===schema-path===\n",
      "{'label2id': {'冒犯': 0, '正常': 1}, 'id2label': {0: '冒犯', 1: '正常'}, 'label_index': 0} ==schema_type== offensive\n",
      "query_risk /data/albert.xht/xiaoda/sentiment/query_risk_v11/query_risk_label.txt ===schema-path===\n",
      "{'label2id': {'风险': 0, '个人信息': 1, '正常': 2}, 'id2label': {0: '风险', 1: '个人信息', 2: '正常'}, 'label_index': 1} ==schema_type== query_risk\n",
      "bias /data/albert.xht/xiaoda/sentiment/bias/bias_label.txt ===schema-path===\n",
      "{'label2id': {'偏见': 0, '正常': 1}, 'id2label': {0: '偏见', 1: '正常'}, 'label_index': 2} ==schema_type== bias\n",
      "ciron /data/albert.xht/xiaoda/sentiment/ciron/ciron_label.txt ===schema-path===\n",
      "{'label2id': {'讽刺': 0, '正常': 1}, 'id2label': {0: '讽刺', 1: '正常'}, 'label_index': 3} ==schema_type== ciron\n",
      "senti /data/albert.xht/xiaoda/sentiment/senti/senti_label.txt ===schema-path===\n",
      "{'label2id': {'负向': 0, '正向': 1}, 'id2label': {0: '负向', 1: '正向'}, 'label_index': 4} ==schema_type== senti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/06/2023 15:10:27 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "01/06/2023 15:10:27 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "01/06/2023 15:10:27 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "01/06/2023 15:10:27 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n",
      "01/06/2023 15:10:27 - INFO - nets.them_classifier - ++RobertaClassifier++ apply stable dropout++\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizerFast\n",
    "import transformers\n",
    "from datetime import timedelta\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from nets.them_classifier import MyBaseModel, RobertaClassifier\n",
    "\n",
    "import configparser\n",
    "from tqdm import tqdm\n",
    "\n",
    "cur_dir_path = '/root/xiaoda/query_topic/'\n",
    "\n",
    "def load_label(filepath):\n",
    "    label_list = []\n",
    "    with open(filepath, 'r') as frobj:\n",
    "        for line in frobj:\n",
    "            label_list.append(line.strip())\n",
    "        n_classes = len(label_list)\n",
    "\n",
    "        label2id = {}\n",
    "        id2label = {}\n",
    "        for idx, label in enumerate(label_list):\n",
    "            label2id[label] = idx\n",
    "            id2label[idx] = label\n",
    "        return label2id, id2label\n",
    "\n",
    "class RiskInfer(object):\n",
    "    def __init__(self, config_path):\n",
    "\n",
    "        import torch, os, sys\n",
    "\n",
    "        con = configparser.ConfigParser()\n",
    "        con_path = os.path.join(cur_dir_path, config_path)\n",
    "        con.read(con_path, encoding='utf8')\n",
    "\n",
    "        args_path = dict(dict(con.items('paths')), **dict(con.items(\"para\")))\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(args_path[\"model_path\"], do_lower_case=True)\n",
    "\n",
    "        from collections import OrderedDict\n",
    "        self.schema_dict = OrderedDict({})\n",
    "\n",
    "        for label_index, schema_info in enumerate(args_path[\"label_path\"].split(',')):\n",
    "            schema_type, schema_path = schema_info.split(':')\n",
    "            schema_path = os.path.join(cur_dir_path, schema_path)\n",
    "            print(schema_type, schema_path, '===schema-path===')\n",
    "            label2id, id2label = load_label(schema_path)\n",
    "            self.schema_dict[schema_type] = {\n",
    "                'label2id':label2id,\n",
    "                'id2label':id2label,\n",
    "                'label_index':label_index\n",
    "            }\n",
    "            print(self.schema_dict[schema_type], '==schema_type==', schema_type)\n",
    "        \n",
    "        output_path = os.path.join(cur_dir_path, args_path['output_path'])\n",
    "\n",
    "        from roformer import RoFormerModel, RoFormerConfig\n",
    "\n",
    "        config = RoFormerConfig.from_pretrained(args_path[\"model_path\"])\n",
    "        encoder = RoFormerModel(config=config)\n",
    "        \n",
    "        encoder_net = MyBaseModel(encoder, config)\n",
    "\n",
    "        self.device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        classifier_list = []\n",
    "\n",
    "        schema_list = list(self.schema_dict.keys())\n",
    "\n",
    "        for schema_key in schema_list:\n",
    "            classifier = RobertaClassifier(\n",
    "                hidden_size=config.hidden_size, \n",
    "                dropout_prob=con.getfloat('para', 'out_dropout_rate'),\n",
    "                num_labels=len(self.schema_dict[schema_key]['label2id']), \n",
    "                dropout_type=con.get('para', 'dropout_type'))\n",
    "            classifier_list.append(classifier)\n",
    "\n",
    "        classifier_list = nn.ModuleList(classifier_list)\n",
    "\n",
    "        class MultitaskClassifier(nn.Module):\n",
    "            def __init__(self, transformer, classifier_list):\n",
    "                super().__init__()\n",
    "\n",
    "                self.transformer = transformer\n",
    "                self.classifier_list = classifier_list\n",
    "\n",
    "            def forward(self, input_ids, input_mask, \n",
    "                        segment_ids=None, \n",
    "                        transformer_mode='mean_pooling', \n",
    "                        dt_idx=None):\n",
    "                hidden_states = self.transformer(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              return_mode=transformer_mode)\n",
    "                outputs_list = []\n",
    "                \n",
    "                for idx, classifier in enumerate(self.classifier_list):\n",
    "                    \n",
    "                    if dt_idx is not None and idx != dt_idx:\n",
    "                        continue\n",
    "                    \n",
    "                    ce_logits = classifier(hidden_states)\n",
    "                    outputs_list.append(ce_logits)\n",
    "                return outputs_list, hidden_states\n",
    "\n",
    "        self.net = MultitaskClassifier(encoder_net, classifier_list).to(self.device)\n",
    "\n",
    "        # eo = 9\n",
    "        # ckpt = torch.load(os.path.join(output_path, 'multitask_cls.pth.{}.raw'.format(eo)), map_location=self.device)\n",
    "        # # ckpt = torch.load(os.path.join(output_path, 'multitask_cls.pth.{}.raw.focal'.format(eo)), map_location=self.device)\n",
    "        # # ckpt = torch.load(os.path.join(output_path, 'multitask_contrast_cls.pth.{}'.format(eo)), map_location=self.device)\n",
    "        # self.net.load_state_dict(ckpt)\n",
    "        # self.net.eval()\n",
    "        \n",
    "    def reload(self, model_path):\n",
    "        ckpt = torch.load(model_path, map_location=self.device)\n",
    "        self.net.load_state_dict(ckpt)\n",
    "        self.net.eval()\n",
    "\n",
    "    def predict(self, text):\n",
    "\n",
    "        \"\"\"抽取输入text所包含的类型\n",
    "        \"\"\"\n",
    "        encoder_txt = self.tokenizer.encode_plus(text, max_length=256)\n",
    "        input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().unsqueeze(0).to(self.device)\n",
    "        token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).unsqueeze(0).to(self.device)\n",
    "        attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        scores_dict = {}\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states] = self.net(input_ids, \n",
    "                attention_mask, token_type_ids, transformer_mode='cls')\n",
    "        for schema_type, logits in zip(list(self.schema_dict.keys()), logits_list):\n",
    "            scores = torch.nn.Softmax(dim=1)(logits)[0].data.cpu().numpy()\n",
    "            scores_dict[schema_type] = []\n",
    "            for index, score in enumerate(scores):\n",
    "                scores_dict[schema_type].append([self.schema_dict[schema_type]['id2label'][index], \n",
    "                                        float(score)])\n",
    "        return scores_dict\n",
    "    \n",
    "    def get_logitnorm(self, text):\n",
    "        \"\"\"抽取输入text所包含的类型\n",
    "        \"\"\"\n",
    "        encoder_txt = self.tokenizer.encode_plus(text, max_length=256)\n",
    "        input_ids = torch.tensor(encoder_txt[\"input_ids\"]).long().unsqueeze(0).to(self.device)\n",
    "        token_type_ids = torch.tensor(encoder_txt[\"token_type_ids\"]).unsqueeze(0).to(self.device)\n",
    "        attention_mask = torch.tensor(encoder_txt[\"attention_mask\"]).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        scores_dict = {}\n",
    "        logits_norm_list = []\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states] = self.net(input_ids, \n",
    "                attention_mask, token_type_ids, transformer_mode='cls')\n",
    "            for logits in logits_list:\n",
    "                logits_norm_list.append(logits/torch.norm(logits, p=2, dim=-1, keepdim=True) + 1e-7)\n",
    "        for schema_type, logit_norm in zip(list(self.schema_dict.keys()), logits_norm_list):\n",
    "            scores_dict[schema_type] = logit_norm[0].data.cpu().numpy()\n",
    "        return scores_dict\n",
    "            \n",
    "    \n",
    "    def predict_batch(self, text):\n",
    "        if isinstance(text, list):\n",
    "            text_list = text\n",
    "        else:\n",
    "            text_list = [text]\n",
    "        model_input = self.tokenizer(text_list, return_tensors=\"pt\",padding=True)\n",
    "        for key in model_input:\n",
    "            model_input[key] = model_input[key].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            [logits_list, \n",
    "            hidden_states] = self.net(model_input['input_ids'], \n",
    "                model_input['attention_mask'], \n",
    "                model_input['token_type_ids'], transformer_mode='cls')\n",
    "        score_dict_list = []\n",
    "        for idx, text in enumerate(text_list):\n",
    "            scores_dict = {}\n",
    "            for schema_type, logits in zip(list(self.schema_dict.keys()), logits_list):\n",
    "                scores = torch.nn.Softmax(dim=1)(logits)[idx].data.cpu().numpy()\n",
    "                scores_dict[schema_type] = []\n",
    "                for index, score in enumerate(scores):\n",
    "                    scores_dict[schema_type].append([self.schema_dict[schema_type]['id2label'][index], \n",
    "                                            float(score)])\n",
    "            score_dict_list.append(scores_dict)\n",
    "        return score_dict_list\n",
    "\n",
    "# risk_api = RiskInfer('./risk_data/config.ini')\n",
    "risk_api = RiskInfer('./risk_data_v5/config_offensive_risk_senti.ini')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29259692-51c1-460e-acdf-b71b934d9611",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_filter_senti_query_risk_v11_offensive_v4/multitask_cls.pth.9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a52e262a-86e3-4a9f-9b7a-b8e69f253881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def eval_all(data, model, key):\n",
    "    pred = []\n",
    "    gold = []\n",
    "    pred_score = []\n",
    "    for item in tqdm(data):\n",
    "        gold.append(item['label'][0])\n",
    "        if isinstance(item['text'], list):\n",
    "            text = \"\\n\".join(item['text'])\n",
    "        else:\n",
    "            text = item['text']\n",
    "        text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", text)   # 合并正文中过多的空格\n",
    "\n",
    "        result = model.predict(text)\n",
    "        score = sorted(result[key], key=lambda u:u[1], reverse=True)\n",
    "        pred.append(score[0][0])\n",
    "        pred_score.append(result[key])\n",
    "    print(classification_report(gold, pred, digits=4))\n",
    "    return pred, gold, pred_score\n",
    "    \n",
    "def evaluation_ece(pred_score, gold):\n",
    "    pred_score_l = []\n",
    "    mapping_dict = {}\n",
    "    for item in pred_score:\n",
    "        pred_score_l.append([])\n",
    "        for idx, p in enumerate(item):\n",
    "            if p[0] not in mapping_dict:\n",
    "                mapping_dict[p[0]] = idx\n",
    "            pred_score_l[-1].append(p[1])\n",
    "    pred_score_l = torch.tensor(pred_score_l)\n",
    "    gold_l = torch.tensor([mapping_dict[item] for item in gold])\n",
    "\n",
    "    ece_fn = ECE(n_bins=15)\n",
    "    print(ece_fn(pred_score_l, gold_l, mode='probs'), '==ece==')\n",
    "# pred, gold, pred_score = eval_all(offensive_test, risk_api, 'offensive')\n",
    "# evaluation_ece(pred_score, gold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ae9c9dc-037d-472f-bd01-bcfa8755d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "risk_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_filter_senti_query_risk_v11_offensive_v3/multitask_cls.pth.9')\n",
    "\n",
    "# risk_query = []\n",
    "# with open('/data/albert.xht/xiaodao/query_risk_v11/offensive_select_labeled.txt') as frobj:\n",
    "#     for line in frobj:\n",
    "#         risk_query.append(json.loads(line.strip()))\n",
    "# pred, gold, pred_score = eval_all(risk_query, risk_api, 'query_risk')\n",
    "# evaluation_ece(pred_score, gold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42597fcb-4889-4704-ad30-7ca8b94cdf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5304/5304 [00:44<00:00, 119.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          冒犯     0.7261    0.8609    0.7877      2106\n",
      "          正常     0.8956    0.7861    0.8373      3198\n",
      "\n",
      "    accuracy                         0.8158      5304\n",
      "   macro avg     0.8108    0.8235    0.8125      5304\n",
      "weighted avg     0.8283    0.8158    0.8176      5304\n",
      "\n",
      "tensor([0.1220]) ==ece==\n"
     ]
    }
   ],
   "source": [
    "offensive = []\n",
    "with open('/data/albert.xht/sentiment/test/offensive_cold.json') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        offensive.append(content)\n",
    "        \n",
    "pred, gold, pred_score = eval_all(offensive, risk_api, 'offensive')\n",
    "evaluation_ece(pred_score, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1cc7d3a2-fc00-40f2-bfbb-e4d19808c817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83982/83982 [10:44<00:00, 130.31it/s]\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        个人信息     0.0000    0.0000    0.0000         0\n",
      "          正常     0.7531    0.7944    0.7732     23186\n",
      "          风险     0.9199    0.9007    0.9102     60796\n",
      "\n",
      "    accuracy                         0.8713     83982\n",
      "   macro avg     0.5577    0.5650    0.5611     83982\n",
      "weighted avg     0.8739    0.8713    0.8724     83982\n",
      "\n",
      "tensor([0.0406]) ==ece==\n"
     ]
    }
   ],
   "source": [
    "\n",
    "risk_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_filter_senti_query_risk_v11_offensive_v2/multitask_cls.pth.9')\n",
    "\n",
    "risk_query = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/offensive_select_labeled.txt.paraphrase') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        for d in content['paraphrase']:\n",
    "            p = {\n",
    "                'text':d[0],\n",
    "                'label':content['label']\n",
    "            }\n",
    "            if d:\n",
    "                risk_query.append(p)\n",
    "        # risk_query.append(json.loads(line.strip()))\n",
    "        \n",
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/offensive_select_labeled.evaluation', 'w') as fwobj:\n",
    "    for d in risk_query:\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "pred, gold, pred_score = eval_all(risk_query, risk_api, 'query_risk')\n",
    "evaluation_ece(pred_score, gold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a6f6f4d7-0954-4f5f-8099-4a8b725e7019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '\"要饭的不嫌饭馊\"?日前家里打扫卫生.沙发陈旧,想送给破烂王.谁\"要饭的不嫌饭馊\"?日前家里打扫卫生.沙发陈旧,想送给破烂王.谁能用过的,我会看不到,不会收拾东西',\n",
       "  'label': ['风险']}]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = []\n",
    "for l, p in zip(pred, risk_query):\n",
    "    if l == '个人信息':\n",
    "        t.append(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6dcd4da2-7e39-4d27-b3b2-3dc5d40abe1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'offensive': [['冒犯', 0.46404173970222473], ['正常', 0.5359582901000977]],\n",
       " 'query_risk': [['风险', 0.3581663966178894],\n",
       "  ['个人信息', 5.1983366574859247e-05],\n",
       "  ['正常', 0.6417816281318665]],\n",
       " 'bias': [['偏见', 0.838456392288208], ['正常', 0.161543607711792]],\n",
       " 'ciron': [['讽刺', 1.2640751265280414e-05], ['正常', 0.9999873638153076]],\n",
       " 'senti': [['负向', 0.221103236079216], ['正向', 0.7788968086242676]]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_api.predict('睾丸')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c79c9a1-0505-4c69-8257-b1300321201f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20641/20641 [02:51<00:00, 120.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          正常     0.8761    0.7833    0.8271      5514\n",
      "          风险     0.9239    0.9596    0.9414     15127\n",
      "\n",
      "    accuracy                         0.9125     20641\n",
      "   macro avg     0.9000    0.8714    0.8843     20641\n",
      "weighted avg     0.9111    0.9125    0.9109     20641\n",
      "\n",
      "tensor([0.0069]) ==ece==\n"
     ]
    }
   ],
   "source": [
    "risk_query = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/model_risk_xiaoda/offensive_select_labeled.txt.paraphrase') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        # for d in content['paraphrase']:\n",
    "        #     p = {\n",
    "        #         'text':d[0],\n",
    "        #         'label':content['label']\n",
    "        #     }\n",
    "        #     if d:\n",
    "        #         risk_query.append(p)\n",
    "        risk_query.append(json.loads(line.strip()))\n",
    "pred, gold, pred_score = eval_all(risk_query, risk_api, 'query_risk')\n",
    "evaluation_ece(pred_score, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ef7cf3-2f93-4009-b3a5-2414465f17d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json, re\n",
    "\n",
    "def risk_predict_batch(risk_api, text):\n",
    "    if isinstance(text, list):\n",
    "        text_list = text\n",
    "    else:\n",
    "        text_list = [text]\n",
    "    result_list = risk_api.predict_batch(text_list)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18127338-4795-4eac-af9b-aa7a688e8448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25507it [00:10, 2540.99it/s]\n"
     ]
    }
   ],
   "source": [
    "offensive = []\n",
    "with open('/data/albert.xht/xiaodao/topic_classification_v7/biake_qa_web_text_zh_train.json.positive', 'r') as frobj:\n",
    "    queue = []\n",
    "    t = []\n",
    "    for line in tqdm(frobj):\n",
    "        content = json.loads(line.strip())\n",
    "        content['text'] = re.sub('如何评价', '', content['text'])\n",
    "        text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", content['text'])   # 合并正文中过多的空格\n",
    "        queue.append(text)\n",
    "        t.append(content)\n",
    "        if np.mod(len(queue), 512) == 0 and queue:\n",
    "            probs = risk_predict_batch(risk_api, queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                content = {\n",
    "                    'text':text,\n",
    "                    'topic':tt['topic'],\n",
    "                    'score_list':prob_dict\n",
    "                }\n",
    "                offensive.append(content)\n",
    "            queue = []\n",
    "            t = []\n",
    "    if queue:\n",
    "        probs = risk_predict_batch(risk_api, queue)\n",
    "        for prob_dict, text, tt in zip(probs, queue, t):\n",
    "            content = {\n",
    "                'text':text,\n",
    "                'topic':tt['topic'],\n",
    "                'score_list':prob_dict\n",
    "            }\n",
    "            offensive.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c22a4e0-33ad-4e68-970a-57d2eda9fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import html\n",
    "import urllib\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import w3lib.html\n",
    "import logging\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from pypinyin import lazy_pinyin, pinyin\n",
    "from opencc import OpenCC\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def harvest_clean_text(text, remove_url=True, email=True, weibo_at=True, stop_terms=(\"转发微博\",),\n",
    "                       emoji=True, weibo_topic=False, deduplicate_space=True,\n",
    "                       norm_url=False, norm_html=False, to_url=False,\n",
    "                       remove_puncts=False, remove_tags=True, t2s=False,\n",
    "                       expression_len=(1, 6), linesep2space=False):\n",
    "    '''\n",
    "    进行各种文本清洗操作，微博中的特殊格式，网址，email，html代码，等等\n",
    "    :param text: 输入文本\n",
    "    :param remove_url: （默认使用）是否去除网址\n",
    "    :param email: （默认使用）是否去除email\n",
    "    :param weibo_at: （默认使用）是否去除微博的\\@相关文本\n",
    "    :param stop_terms: 去除文本中的一些特定词语，默认参数为(\"转发微博\",)\n",
    "    :param emoji: （默认使用）去除\\[\\]包围的文本，一般是表情符号\n",
    "    :param weibo_topic: （默认不使用）去除##包围的文本，一般是微博话题\n",
    "    :param deduplicate_space: （默认使用）合并文本中间的多个空格为一个\n",
    "    :param norm_url: （默认不使用）还原URL中的特殊字符为普通格式，如(%20转为空格)\n",
    "    :param norm_html: （默认不使用）还原HTML中的特殊字符为普通格式，如(\\&nbsp;转为空格)\n",
    "    :param to_url: （默认不使用）将普通格式的字符转为还原URL中的特殊字符，用于请求，如(空格转为%20)\n",
    "    :param remove_puncts: （默认不使用）移除所有标点符号\n",
    "    :param remove_tags: （默认使用）移除所有html块\n",
    "    :param t2s: （默认不使用）繁体字转中文\n",
    "    :param expression_len: 假设表情的表情长度范围，不在范围内的文本认为不是表情，不加以清洗，如[加上特别番外荞麦花开时共五册]。设置为None则没有限制\n",
    "    :param linesep2space: （默认不使用）把换行符转换成空格\n",
    "    :return: 清洗后的文本\n",
    "    '''\n",
    "    # unicode不可见字符\n",
    "    # 未转义\n",
    "    text = re.sub(r\"[\\u200b-\\u200d]\", \"\", text)\n",
    "    # 已转义\n",
    "    text = re.sub(r\"(\\\\u200b|\\\\u200c|\\\\u200d)\", \"\", text)\n",
    "    # 反向的矛盾设置\n",
    "    if norm_url and to_url:\n",
    "        raise Exception(\"norm_url和to_url是矛盾的设置\")\n",
    "    if norm_html:\n",
    "        text = html.unescape(text)\n",
    "    if to_url:\n",
    "        text = urllib.parse.quote(text)\n",
    "    if remove_tags:\n",
    "        text = w3lib.html.remove_tags(text)\n",
    "    if remove_url:\n",
    "        try:\n",
    "            URL_REGEX = re.compile(\n",
    "                r'(?i)http[s]?://(?:[a-zA-Z]|[0-9]|[#$%*-;=?&@~.&+]|[!*,])+',\n",
    "                re.IGNORECASE)\n",
    "            text = re.sub(URL_REGEX, \"\", text)\n",
    "        except:\n",
    "            # sometimes lead to \"catastrophic backtracking\"\n",
    "            zh_puncts1 = \"，；、。！？（）《》【】\"\n",
    "            URL_REGEX = re.compile(\n",
    "                r'(?i)((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>' + zh_puncts1 +\n",
    "                ']+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’' + zh_puncts1 + ']))',\n",
    "                re.IGNORECASE)\n",
    "            text = re.sub(URL_REGEX, \"\", text)\n",
    "    if norm_url:\n",
    "        text = urllib.parse.unquote(text)\n",
    "    if email:\n",
    "        EMAIL_REGEX = re.compile(\n",
    "            r\"[-a-z0-9_.]+@(?:[-a-z0-9]+\\.)+[a-z]{2,6}\", re.IGNORECASE)\n",
    "        text = re.sub(EMAIL_REGEX, \"\", text)\n",
    "    if weibo_at:\n",
    "        text = re.sub(r\"(回复)?(//)?(\\\\\\\\)?\\s*@\\S*?\\s*(:|：| |$)\",\n",
    "                      \" \", text)  # 去除正文中的@和回复/转发中的用户名\n",
    "    if emoji:\n",
    "        # 去除括号包围的表情符号\n",
    "        # ? lazy match避免把两个表情中间的部分去除掉\n",
    "        if type(expression_len) in {tuple, list} and len(expression_len) == 2:\n",
    "            # 设置长度范围避免误伤人用的中括号内容，如[加上特别番外荞麦花开时共五册]\n",
    "            lb, rb = expression_len\n",
    "            text = re.sub(r\"\\[\\S{\"+str(lb)+r\",\"+str(rb)+r\"}?\\]\", \"\", text)\n",
    "        else:\n",
    "            text = re.sub(r\"\\[\\S+?\\]\", \"\", text)\n",
    "        # text = re.sub(r\"\\[\\S+\\]\", \"\", text)\n",
    "        # 去除真,图标式emoji\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "    if weibo_topic:\n",
    "        text = re.sub(r\"#\\S+#\", \"\", text)  # 去除话题内容\n",
    "    if linesep2space:\n",
    "        text = text.replace(\"\\n\", \" \")   # 不需要换行的时候变成1行\n",
    "    if deduplicate_space:\n",
    "        text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", r\"\\1\", text)   # 合并正文中过多的空格\n",
    "        text = re.sub(r\"(\\s)+\", r\"\\1\", text)   # 合并正文中过多的空格\n",
    "        # text = re.sub(r\"(\\t)+\", r\"\\1\", text)   # 合并正文中过多的空格\n",
    "    if t2s:\n",
    "        cc = OpenCC('t2s')\n",
    "        text = cc.convert(text)\n",
    "    assert hasattr(stop_terms, \"__iter__\"), Exception(\"去除的词语必须是一个可迭代对象\")\n",
    "    if type(stop_terms) == str:\n",
    "        text = text.replace(stop_terms, \"\")\n",
    "    else:\n",
    "        for x in stop_terms:\n",
    "            text = text.replace(x, \"\")\n",
    "    if remove_puncts:\n",
    "        allpuncs = re.compile(\n",
    "            r\"[，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+]\")\n",
    "        text = re.sub(allpuncs, \"\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def clean_data(text):\n",
    "    text = harvest_clean_text(text, remove_url=True, email=True, weibo_at=False, stop_terms=(\"转发微博\",),\n",
    "                       emoji=True, weibo_topic=True, deduplicate_space=True,\n",
    "                       norm_url=False, norm_html=False, to_url=False,\n",
    "                       remove_puncts=False, remove_tags=False, t2s=False,\n",
    "                       expression_len=(1, 6), linesep2space=False)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9af0eda9-7bf5-4722-b32e-f2976e85944e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "478540it [07:46, 1026.71it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import namedtuple\n",
    "_DocSpan = namedtuple(  # pylint: disable=invalid-name\n",
    "        \"DocSpan\", [\"start\", \"length\"])\n",
    "\n",
    "def slide_window(all_doc_tokens, max_length, doc_stride, offset=32):\n",
    "    doc_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < len(all_doc_tokens):\n",
    "        length = len(all_doc_tokens) - start_offset\n",
    "        if length > max_length - offset:\n",
    "            length = max_length - offset\n",
    "        doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "        if start_offset + length == len(all_doc_tokens):\n",
    "            break\n",
    "        start_offset += min(length, doc_stride)\n",
    "    return doc_spans\n",
    "\n",
    "offensive = []\n",
    "risk_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_filter_senti_query_risk_v11_offensive_v2/multitask_cls.pth.9')\n",
    "\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_politics/short_text_benchmark_for_cro_albert_politics.json.txt', 'r') as frobj:\n",
    "    queue = []\n",
    "    t = []\n",
    "    for idx, line in tqdm(enumerate(frobj)):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        content = json.loads(line.strip())\n",
    "        text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", content['text'])   # 合并正文中过多的空格\n",
    "        # if content['label'] not in ['black']:\n",
    "        #     continue\n",
    "        # if len(text) >= 164:\n",
    "        #     text = text[:164]\n",
    "        if content['label'] in ['white']:\n",
    "            spans = slide_window(text[91:], 91, 91)\n",
    "            for span in spans:\n",
    "                span_text = text[:91]+text[span.start+91:span.start+span.length+91]\n",
    "                queue.append(span_text)\n",
    "                t.append(content)\n",
    "        elif content['label'] in ['black']:\n",
    "            queue.append(text[:192])\n",
    "            t.append(content)\n",
    "        else:\n",
    "            continue\n",
    "        if np.mod(len(queue), 64) == 0 and queue:\n",
    "            probs = risk_predict_batch(risk_api, queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                content = {\n",
    "                    'text':text,\n",
    "                    'topic':tt['label'],\n",
    "                    'score_list':prob_dict\n",
    "                }\n",
    "                offensive.append(content)\n",
    "            queue = []\n",
    "            t = []\n",
    "    if queue:\n",
    "        probs = risk_predict_batch(risk_api, queue)\n",
    "        for prob_dict, text, tt in zip(probs, queue, t):\n",
    "            content = {\n",
    "                'text':text,\n",
    "                'topic':tt['label'],\n",
    "                'score_list':prob_dict\n",
    "            }\n",
    "            offensive.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66d006bb-8709-4db2-9e21-3fed4c0fa50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "_DocSpan = namedtuple(  # pylint: disable=invalid-name\n",
    "        \"DocSpan\", [\"start\", \"length\"])\n",
    "\n",
    "def slide_window(all_doc_tokens, max_length, doc_stride, offset=32):\n",
    "    doc_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < len(all_doc_tokens):\n",
    "        length = len(all_doc_tokens) - start_offset\n",
    "        if length > max_length - offset:\n",
    "            length = max_length - offset\n",
    "        doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "        if start_offset + length == len(all_doc_tokens):\n",
    "            break\n",
    "        start_offset += min(length, doc_stride)\n",
    "    return doc_spans\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2da3d54b-f7e9-44bc-aad5-a240c14aa64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2805959it [41:01, 1139.99it/s]\n"
     ]
    }
   ],
   "source": [
    "porn = []\n",
    "risk_api.reload('/data/albert.xht/xiaoda/risk_classification/multitask_raw_filter_senti_query_risk_v11_offensive_v2/multitask_cls.pth.9')\n",
    "\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_porn/short_text_benchmark_for_cro_albert_seqing.json.txt', 'r') as frobj:\n",
    "    queue = []\n",
    "    t = []\n",
    "    from collections import Counter\n",
    "    pppp = Counter()\n",
    "    for idx, line in tqdm(enumerate(frobj)):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        content = json.loads(line.strip())\n",
    "        text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", content['text'])   # 合并正文中过多的空格\n",
    "        # if content['label'] not in ['black']:\n",
    "        #     continue\n",
    "        if len(text) >= 164:\n",
    "            text = text[:164]\n",
    "        pppp[content['label']] += 1\n",
    "        queue.append(text)\n",
    "        t.append(content)\n",
    "        if np.mod(len(queue), 128) == 0 and queue:\n",
    "            probs = risk_predict_batch(risk_api, queue)\n",
    "            for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                content = {\n",
    "                    'text':text,\n",
    "                    'topic':tt['label'],\n",
    "                    'score_list':prob_dict\n",
    "                }\n",
    "                porn.append(content)\n",
    "            queue = []\n",
    "            t = []\n",
    "    if queue:\n",
    "        probs = risk_predict_batch(risk_api, queue)\n",
    "        for prob_dict, text, tt in zip(probs, queue, t):\n",
    "            content = {\n",
    "                'text':text,\n",
    "                'topic':tt['label'],\n",
    "                'score_list':prob_dict\n",
    "            }\n",
    "            porn.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa1d017d-4aef-4ea1-90ae-0bdcc28e0e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2805958/2805958 [00:43<00:00, 63985.46it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/data/albert.xht/xiaoda/sentiment/green_porn/short_text_benchmark_for_cro_albert_seqing.json.txt.offensive', 'w') as fwobj:\n",
    "    for d in tqdm(porn):\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4325c029-366d-450a-9437-3f26ad211e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 456219/456219 [00:08<00:00, 55401.56it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/data/albert.xht/xiaoda/sentiment/green_politics/short_text_benchmark_for_cro_albert_politics.json.txt.offensive', 'w') as fwobj:\n",
    "    for d in tqdm(offensive):\n",
    "        fwobj.write(json.dumps(d, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "502687e8-715b-459b-a0d9-8128f4c842f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 456219/456219 [00:01<00:00, 441244.29it/s]\n",
      "100%|██████████| 451726/451726 [00:03<00:00, 131818.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mapping = {\n",
    "    'black':'涉政',\n",
    "    'white':'正常'\n",
    "}\n",
    "\n",
    "import random\n",
    "random.shuffle(offensive)\n",
    "\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_politics/green_politics.json', 'w') as fwobj:\n",
    "    black = []\n",
    "    left = []\n",
    "    count1 = 0\n",
    "    data_dict = {}\n",
    "    for d in tqdm(offensive):\n",
    "        p = {\n",
    "                'text':d['text'],\n",
    "                'label':[mapping[d['topic']]]\n",
    "            }\n",
    "        if d['text'] not in data_dict:\n",
    "            data_dict[d['text']] = set()\n",
    "        data_dict[d['text']].add(p['label'][0])\n",
    "    for key in tqdm(data_dict):\n",
    "        label = data_dict[key]\n",
    "        if len(label) == 1:\n",
    "            p = {\n",
    "                'text':key,\n",
    "                'label':list(label)\n",
    "            }\n",
    "        fwobj.write(json.dumps(p, ensure_ascii=False)+'\\n')\n",
    "        \n",
    "    \n",
    "    #     if d['score_list']['offensive'][0][1] > 0.9 and d['score_list']['query_risk'][0][1] > 0.9:\n",
    "    #         p = {\n",
    "    #             'text':d['text'],\n",
    "    #             'label':[mapping[d['topic']]],\n",
    "    #             'source':'risk-offensive'\n",
    "    #         }\n",
    "    #         fwobj.write(json.dumps(p, ensure_ascii=False)+'\\n')\n",
    "    #         count1 += 1\n",
    "    #         if count1 >= 50000:\n",
    "    #             break\n",
    "    # count2 = 0\n",
    "    # for d in offensive:\n",
    "    #     if d['topic'] not in mapping:\n",
    "    #         continue\n",
    "    #     if d['score_list']['offensive'][0][1] < 0.1 and d['score_list']['query_risk'][0][1] < 0.1:\n",
    "    #         p = {\n",
    "    #             'text':d['text'],\n",
    "    #             'label':[mapping[d['topic']]],\n",
    "    #             'source':'politics'\n",
    "    #         }\n",
    "    #         fwobj.write(json.dumps(p, ensure_ascii=False)+'\\n')\n",
    "    #         count2 += 1\n",
    "    #         if count2 >= 50000:\n",
    "    #             break\n",
    "print(count1, count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b443c6a7-8382-4668-b552-d566311e556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "porn = []\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_porn/short_text_benchmark_for_cro_albert_seqing.json.txt.offensive') as frobj:\n",
    "    for line in frobj:\n",
    "        porn.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49fc9345-7acb-45e6-85cc-cc0e47825e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2805958/2805958 [00:07<00:00, 363279.69it/s]\n",
      "100%|██████████| 2758518/2758518 [00:19<00:00, 140611.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mapping = {\n",
    "    '色情':'色情',\n",
    "    '色情-白样本':'正常',\n",
    "    '色情-正常':'正常'\n",
    "}\n",
    "\n",
    "import random\n",
    "random.shuffle(porn)\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "with open('/data/albert.xht/xiaoda/sentiment/green_porn/green_porn.json', 'w') as fwobj:\n",
    "    black = []\n",
    "    left = []\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    count4 = 0\n",
    "    for d in tqdm(porn):\n",
    "        p = {\n",
    "                'text':d['text'],\n",
    "                'label':[mapping[d['topic']]]\n",
    "            }\n",
    "        if d['text'] not in data_dict:\n",
    "            data_dict[d['text']] = set()\n",
    "        data_dict[d['text']].add(p['label'][0])\n",
    "    for key in tqdm(data_dict):\n",
    "        label = data_dict[key]\n",
    "        if len(label) == 1:\n",
    "            p = {\n",
    "                'text':key,\n",
    "                'label':list(label)\n",
    "            }\n",
    "        fwobj.write(json.dumps(p, ensure_ascii=False)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f6f54e1-682a-4ba2-93dd-6d603575770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "low = []\n",
    "for d in porn:\n",
    "    if d['score_list']['offensive'][0][1] < 0.5 and d['score_list']['query_risk'][0][1] < 0.5 and d['score_list']['offensive'][0][1] > 0.4:\n",
    "        if d['topic'] in ['色情-白样本']:\n",
    "            low.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "291139b2-cae1-46be-9165-90a2622728fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '刹住个屁一个村一年都能吃好几万他这一个县呢',\n",
       " 'topic': '色情-白样本',\n",
       " 'score_list': {'offensive': [['冒犯', 0.45606398582458496],\n",
       "   ['正常', 0.5439360737800598]],\n",
       "  'query_risk': [['风险', 0.4721612334251404],\n",
       "   ['个人信息', 1.2448796042008325e-05],\n",
       "   ['正常', 0.5278263688087463]]}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa69f81c-8438-4567-ba41-d8cebcdb6ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tttt = []\n",
    "for d in offensive:\n",
    "    if d['score_list']['query_risk'][0][1] >= 0.8 and d['topic'][0] in ['军事', '时事政治', '历史', '国家']:\n",
    "        tttt.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aaed5838-47bb-4c98-9335-8f5f8ec1fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = []\n",
    "neg = []\n",
    "with open('/data/albert.xht/xiaodao/topic_classification_v7/biake_qa_web_text_zh_train.json.positive.topic.v10') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['topic'][0][0] in ['军事', '时事政治', '历史', '国家']:\n",
    "            if content['score_list']['intent'][0][1] + content['score_list']['intent'][1][1] > 0.4:\n",
    "                neg.append(content)\n",
    "            else:\n",
    "                positive.append(content)\n",
    "        else:\n",
    "            positive.append(content)\n",
    "            \n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/embed_linear_small_white.json.topic.v10') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['topic'][0][0] in ['军事', '时事政治', '历史', '国家']:\n",
    "            if content['score_list']['intent'][0][1] + content['score_list']['intent'][1][1] > 0.4:\n",
    "                neg.append(content)\n",
    "            else:\n",
    "                positive.append(content)\n",
    "        else:\n",
    "            positive.append(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f31ece8b-3fac-4df0-98c4-ece1297549c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128it [00:00, 1257.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/albert.xht/raw_chat_corpus/topic_classification_v4/smal_white_positive.json.filter ===input-path===\n",
      "/data/albert.xht/raw_chat_corpus/topic_classification_v4/smal_white_positive.json.filter.offensive_query_risk ===output-path===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44674it [00:19, 2305.49it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/smal_white_positive.json.filter'\n",
    "output_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/smal_white_positive.json.filter.offensive_query_risk'\n",
    "\n",
    "\n",
    "batch_inference(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd6a59d5-7132-4ec4-befc-ead9b37e3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "black, white = [], []\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/smal_white_positive.json.filter.offensive_query_risk') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['score_list']['query_risk'][0][1] > 0.3 and content['score_list']['offensive'][0][1] < 0.5:\n",
    "            black.append(content)\n",
    "        elif content['score_list']['query_risk'][0][1] < 0.3 and content['score_list']['offensive'][0][1] < 0.3:\n",
    "            white.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a7cb357-f765-430a-8e07-655091428846",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/smal_white_positive.json.filter.offensive_query_risk', 'w') as fwobj:\n",
    "    for d in white:\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'label':['正常']\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d1945250-4a7f-4a56-8616-9f22170a7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/smal_white_positive.json.filter', 'w') as fwobj:\n",
    "    for d in positive:\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'label':['正常']\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e0c4ed2-fd23-4353-8372-886bbae232ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3176 3177\n"
     ]
    }
   ],
   "source": [
    "\n",
    "offensive = []\n",
    "offensive_dict = {}\n",
    "with open('/data/albert.xht/xiaodao/query_risk_v10/biake_qa_web_text_zh_train.json.offensive.all.v10.1') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['score_list']['offensive'][0][1] < 0.8 and content['score_list']['offensive'][0][1] >= 0.5:\n",
    "            offensive.append(content)\n",
    "            if content['text'] not in offensive_dict:\n",
    "                offensive_dict[content['text']] = []\n",
    "            offensive_dict[content['text']].append('风险')\n",
    "print(len(offensive_dict), len(offensive))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a52e8627-d417-4aef-8d2c-fb95ea68276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "itag_dict = {}\n",
    "with open('/data/albert.xht/xiaodao/query_risk_v11/offensive_select_labeled.txt') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['text'] in offensive_dict:\n",
    "            offensive_dict[content['text']].append(content['label'][0])\n",
    "        else:\n",
    "            itag_dict[content['text']] = [content['label'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e235642a-1b6c-4ee7-b666-2e78af139b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "oooo = []\n",
    "for text in offensive_dict:\n",
    "    if len(set(offensive_dict[text])) > 1:\n",
    "        oooo.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "89bc89b8-f89d-4a4d-af2f-07aacdef9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/xiaodao/query_risk_v10/biake_qa_web_text_zh_train.json.offensive.all.v10.1.offensive', 'w') as fwobj:\n",
    "    for d in offensive:\n",
    "        content = {\n",
    "            'text':d['text'],\n",
    "            'label':['风险']\n",
    "        }\n",
    "        fwobj.write(json.dumps(content, ensure_ascii=False)+'\\n')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcaa2d76-91aa-4aa7-b5a3-19f4aca40d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = []\n",
    "topic_other = []\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.all_risk.v9.1') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['topic'][0] in ['军事', '时事政治', '历史', '国家']:\n",
    "            if content['score_list']['intent'][0][1] + content['score_list']['intent'][1][1] > 0.4:\n",
    "                topic.append(content)\n",
    "            else:\n",
    "                topic_other.append(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "dde780aa-ca26-4be7-b470-4725eec7081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference(input_path, output_path):\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    import json, re\n",
    "\n",
    "    def risk_predict_batch(text):\n",
    "        if isinstance(text, list):\n",
    "            text_list = text\n",
    "        else:\n",
    "            text_list = [text]\n",
    "        result_list = risk_api.predict_batch(text_list)\n",
    "        return result_list\n",
    "    \n",
    "    print(input_path, '===input-path===')\n",
    "    print(output_path, '===output-path===')\n",
    "    \n",
    "    with open(output_path, 'w') as fwobj:\n",
    "        with open(input_path, 'r') as frobj:\n",
    "            queue = []\n",
    "            t = []\n",
    "            for line in tqdm(frobj):\n",
    "                content = json.loads(line.strip())\n",
    "                content['text'] = re.sub('请问', '', content['text'])\n",
    "                text = re.sub(r\"([，\\_《。》、？；：‘’＂“”【「】」·！@￥…（）—\\,\\<\\.\\>\\/\\?\\;\\:\\'\\\"\\[\\]\\{\\}\\~\\`\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\=\\+])+\", \"\", content['text'])   # 合并正文中过多的空格\n",
    "                if len(text) >= 192:\n",
    "                    continue\n",
    "                queue.append(text)\n",
    "                t.append(content)\n",
    "                if np.mod(len(queue), 128) == 0:\n",
    "                    probs = risk_predict_batch(queue)\n",
    "                    for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                        # content = {\n",
    "                        #     'text':tt['text'],\n",
    "                        #     'topic':tt['topic'],\n",
    "                        #     'score_list':prob_dict,\n",
    "                        #     'label'\n",
    "                        #     # 'score_list': tt['score_list']\n",
    "                        # }\n",
    "                        tt['score_list'] = prob_dict\n",
    "                        fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "                    queue = []\n",
    "                    t = []\n",
    "            if queue:\n",
    "                probs = risk_predict_batch(queue)\n",
    "                for prob_dict, text, tt in zip(probs, queue, t):\n",
    "                    # content = {\n",
    "                    #     'text':tt['text'],\n",
    "                    #     'topic':tt['topic'],\n",
    "                    #     'score_list':prob_dict,\n",
    "                    #     # 'score_list': tt['score_list']\n",
    "                    # }\n",
    "                    tt['score_list'] = prob_dict\n",
    "                    fwobj.write(json.dumps(tt, ensure_ascii=False)+'\\n')\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d63180c3-55b4-4665-9bc6-ed87f3cfcc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 1793.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final ===input-path===\n",
      "/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.offensive_query_risk ===output-path===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2071401it [14:07, 2444.75it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final'\n",
    "output_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.offensive_query_risk'\n",
    "\n",
    "\n",
    "batch_inference(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1df78-2e5f-4ecf-a565-bae0bc101873",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.topic.knn.final'\n",
    "output_path = '/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.offensive_query_risk'\n",
    "\n",
    "\n",
    "batch_inference(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2b40cf9a-c3a6-4341-bf0c-055e56d7f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "black, white = [], []\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.offensive_query_risk') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['score_list']['offensive'][0][1] < 0.3 and content['score_list']['query_risk'][0][1] >= 0.7:\n",
    "            black.append(content)\n",
    "        elif content['score_list']['offensive'][0][1] > 0.7 and content['score_list']['query_risk'][0][1] <= 0.3:\n",
    "            white.append(content)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9165931f-c4a2-4884-b532-dbfbd0dfb108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8125"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "134f48c1-422f-4903-9de9-201892d6a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = {}\n",
    "with open('/data/albert.xht/xiaodao/query_risk_v11/offensive_select_labeled.txt.topic') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['label'][0] in ['风险']:\n",
    "            if content['text'] not in text_dict:\n",
    "                text_dict[content['text']] = []\n",
    "            text_dict[content['text']].append(content)\n",
    "\n",
    "with open('/data/albert.xht/raw_chat_corpus/topic_classification_v4/biake_qa_web_text_zh_train.json.offensive_query_risk.itag', 'w') as fwobj:\n",
    "    fwobj.write(\"&&&\".join(['query'])+'\\n')\n",
    "    for d in black:\n",
    "        if d['text'] not in text_dict:\n",
    "            fwobj.write(\"&&&\".join([d['text']])+'\\n')\n",
    "    for key in text_dict:\n",
    "        fwobj.write(\"&&&\".join([key])+'\\n')\n",
    "        \n",
    "    with open('/data/albert.xht/sentiment/offensive_cold.json.offensive_query_risk') as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            if content['score_list']['query_risk'][0][1] > 0.8 and content['topic'][0] in ['正常']:\n",
    "                fwobj.write(\"&&&\".join([content['text']])+'\\n')\n",
    "                \n",
    "    with open('/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8.other.offensive_query_risk') as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            content['text'] = re.sub('[男女]+', ' ', content['text'])\n",
    "            if content['score_list']['query_risk'][0][1] > 0.8 and content['score_list']['offensive'][0][1] < 0.3:\n",
    "                fwobj.write(\"&&&\".join([content['text']])+'\\n')\n",
    "                \n",
    "    for d in sucide:\n",
    "        d['title'] = re.sub('[男女]+', ' ', d['title'])\n",
    "        fwobj.write(\"&&&\".join([d['title']])+'\\n')\n",
    "        \n",
    "    for d in white:\n",
    "        fwobj.write(\"&&&\".join([d['text']])+'\\n')\n",
    "\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5555609b-d148-4e36-9da1-e0c1d6c55272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'offensive': [['冒犯', 0.9479578733444214], ['正常', 0.05204217880964279]],\n",
       " 'query_risk': [['风险', 0.8904106020927429],\n",
       "  ['个人信息', 4.8662836888979655e-06],\n",
       "  ['正常', 0.10958455502986908]]}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_api.predict('垃圾的社会主义征程')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bb8f1a55-3520-45cc-a383-9b7d0eb54279",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = []\n",
    "for d in white:\n",
    "    if d['topic'][0] in ['军事']:\n",
    "        ss.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f8b04ae0-b9b2-47bf-b326-3a591411cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "for d in white:\n",
    "    if d['topic'][0] in ['时事政治']:\n",
    "        s.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0fed98e4-b2ae-4bf4-a251-508e2f9e93f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5652"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b2be7405-9d30-4402-9532-18e920ca65b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "128it [00:00, 755.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/albert.xht/sentiment/offensive_cold.json ===input-path===\n",
      "/data/albert.xht/sentiment/offensive_cold.json.offensive_query_risk ===output-path===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25663it [00:21, 1198.03it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/data/albert.xht/sentiment/offensive_cold.json'\n",
    "output_path = '/data/albert.xht/sentiment/offensive_cold.json.offensive_query_risk'\n",
    "\n",
    "\n",
    "batch_inference(input_path, output_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af09afd4-7098-4e24-a1dc-f5879067b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "black, white = [], []\n",
    "with open('/data/albert.xht/sentiment/offensive_cold.json.offensive_query_risk') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['score_list']['query_risk'][0][1] > 0.8 and content['score_list']['offensive'][0][1] < 0.5:\n",
    "            black.append(content)\n",
    "        elif content['score_list']['query_risk'][0][1] < 0.3 and content['score_list']['offensive'][0][1] < 0.3:\n",
    "            white.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "868ad8d7-8d61-4273-89dc-e29b8dce764f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2071401it [00:19, 104403.54it/s]\n"
     ]
    }
   ],
   "source": [
    "black = []\n",
    "with open(output_path) as frobj:\n",
    "    for line in tqdm(frobj):\n",
    "        content = json.loads(line.strip())\n",
    "        if content['score_list']['query_risk'][0][1] > 0.3 and content['score_list']['query_risk'][0][1] < 0.5:\n",
    "            black.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe722d75-e7c1-4fdc-95ce-8682e7df4530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/albert.xht/sentiment/green_teenager.json.all.detail ===input-path===\n",
      "/data/albert.xht/sentiment/green_teenager.json.all.detail.offensive_query_risk ===output-path===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "241035it [03:22, 1188.86it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/data/albert.xht/sentiment/green_teenager.json.all.detail'\n",
    "output_path = '/data/albert.xht/sentiment/green_teenager.json.all.detail.offensive_query_risk'\n",
    "\n",
    "\n",
    "batch_inference(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a1943f-76ae-415b-91a5-edcd9a0ec7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_path = '/data/albert.xht/sentiment/green_teenager.json.all.detail'\n",
    "output_path = '/data/albert.xht/sentiment/green_teenager.json.all.detail.offensive_query_risk'\n",
    "\n",
    "\n",
    "batch_inference(input_path, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "42b6013f-af7c-4381-a945-3eaf50e050f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/sentiment/green_teenager.json.all.detail.offensive_query_risk.filter', 'w') as fwobj:\n",
    "    with open('/data/albert.xht/sentiment/green_teenager.json.all.detail.offensive_query_risk') as frobj:\n",
    "        white = []\n",
    "        black = []\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            # content['text'] = re.sub('[男女]+', ' ', content['text'])\n",
    "            if content['score_list']['query_risk'][0][1]< 0.2 and content['score_list']['offensive'][0][1] < 0.3:\n",
    "                white.append(content)\n",
    "            elif content['score_list']['query_risk'][0][1]> 0.9 and content['score_list']['offensive'][0][1] > 0.9:\n",
    "                black.append(content)\n",
    "    for d in white:\n",
    "        tmp = {\n",
    "            'text':d['text'],\n",
    "            'label':['正常'],\n",
    "            'source':'green_teenager_filter'\n",
    "        }\n",
    "        fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')\n",
    "    for d in black:\n",
    "        tmp = {\n",
    "            'text':d['text'],\n",
    "            'label':['风险'],\n",
    "            'source':'green_teenager_filter'\n",
    "        }\n",
    "        fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3a30d5e2-ce0a-4046-bf39-a8b035600ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_mapping = {\n",
    "    '1.1':'学业烦恼、对未来规划的迷茫',\n",
    "    '1.2':'事业和工作烦恼',\n",
    "    '1.3':'家庭问题和矛盾',\n",
    "    '1.4':'物质滥用',\n",
    "    '1.5':'悲恸',\n",
    "    '1.6':'失眠',\n",
    "    '1.7':'压力',\n",
    "    '1.8':'人际关系',\n",
    "    '1.9':'情感关系问题',\n",
    "    '1.10':'离婚',\n",
    "    '1.11':'分手',\n",
    "    '1.12':'自我探索',\n",
    "    '1.13':'低自尊',\n",
    "    '1.14':'青春期问题',\n",
    "    '1.15':'强迫症',\n",
    "    '1.16':'其它',\n",
    "    '1.17':'男同性恋、女同性恋、双性恋与跨性别',\n",
    "    '1.18':'性问题',\n",
    "    '1.19':'亲子关系'\n",
    "}\n",
    "\n",
    "s2_mapping = {\n",
    "    '2.1':'忧郁症',\n",
    "    '2.2':'焦虑症',\n",
    "    '2.3':'躁郁症',\n",
    "    '2.4':'创伤后应激反应',\n",
    "    '2.5':'恐慌症',\n",
    "    '2.6':'厌食症和暴食症',\n",
    "    '2.7':'非疾病',\n",
    "    '2.8':'其它疾病'\n",
    "}\n",
    "\n",
    "s3_mapping = {\n",
    "    '3.1':'正在进行的自杀行为',\n",
    "    '3.2':'策划进行的自杀行为',\n",
    "    '3.3':'自残',\n",
    "    '3.4':'进行的人身伤害',\n",
    "    '3.5':'计划的人身伤害',\n",
    "    '3.6':'无伤害身体倾向'\n",
    "}\n",
    "\n",
    "sucide = []\n",
    "\n",
    "with open(\"/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8.other\", \"w\") as fwobj:\n",
    "    with open(\"/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8\", \"r\") as frobj:\n",
    "        for line in frobj:\n",
    "            content = json.loads(line.strip())\n",
    "            title = re.sub('[男女]+', ' ', content['title']) #''.join(re.split('[\\s,]', content['title'])[1:])\n",
    "            content['title'] = title\n",
    "            if len(title) >= 5:\n",
    "                if  s3_mapping[content['label']['s3']] in ['正在进行的自杀行为', '策划进行的自杀行为', '自残']:\n",
    "                    sucide.append(content)\n",
    "                    continue\n",
    "                else:\n",
    "                    tmp = {\n",
    "                        'text':title,\n",
    "                        'label':[s3_mapping[content['label']['s3']],\n",
    "                                s1_mapping[content['label']['s1']],\n",
    "                                s2_mapping[content['label']['s2']]\n",
    "                                ],\n",
    "                        'source':'efaqa'\n",
    "                    }\n",
    "                    fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f94037a-8fef-4d34-9346-332d304305ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256it [00:00, 2285.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8.other ===input-path===\n",
      "/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8.other.offensive_query_risk ===output-path===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19655it [00:10, 1874.85it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8.other'\n",
    "output_path = '/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8.other.offensive_query_risk'\n",
    "\n",
    "\n",
    "batch_inference(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "02031aeb-597d-46bd-a048-d6e65301a3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'offensive': [['冒犯', 0.1621621698141098], ['正常', 0.837837815284729]],\n",
       " 'query_risk': [['风险', 0.8829036355018616],\n",
       "  ['个人信息', 9.993584535550326e-05],\n",
       "  ['正常', 0.11699634790420532]]}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_api.predict('最后一集ymd出来耍帅耍了个够…不过我真是佩服小变态脸都毁了还能借到三百万[拜拜][拜拜][拜拜]我以为要教我看清这看脸的世界…不过这季居然有两人上岸…不过大家都还背着债啊')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49731867-ab67-4e8f-880b-24fbfc6f18a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s= \"\"\"{\"chats\": [{\"label\": {\"knowledge\": false, \"negative\": false, \"question\": false}, \"sender\": \"audience\", \"time\": \"16:01:41\", \"type\": \"textMessage\", \"value\": \"公交车要承担赔偿责任\"}, {\"label\": {\"knowledge\": false, \"negative\": false, \"question\": false}, \"sender\": \"audience\", \"time\": \"16:02:17\", \"type\": \"textMessage\", \"value\": \"双方均有责任\"}, {\"label\": {\"attitude\": \"\", \"s3\": \"\"}, \"sender\": \"owner\", \"time\": \"16:02:54\", \"type\": \"textMessage\", \"value\": \"大概能陪多少！算合适！\"}, {\"label\": {\"knowledge\": false, \"negative\": false, \"question\": false}, \"sender\": \"audience\", \"time\": \"16:03:41\", \"type\": \"textMessage\", \"value\": \"具体要由交警依据相关证据做出责任认定\"}, {\"label\": {\"knowledge\": false, \"negative\": false, \"question\": false}, \"sender\": \"audience\", \"time\": \"16:04:19\", \"type\": \"textMessage\", \"value\": \"具体数额要看伤情。这里涉及保险赔付问题\"}, {\"label\": {\"knowledge\": false, \"negative\": false, \"question\": false}, \"sender\": \"audience\", \"time\": \"16:04:20\", \"type\": \"textMessage\", \"value\": \"然后根据损失情况确定赔偿金额\"}, {\"label\": {\"attitude\": \"\", \"s3\": \"\"}, \"sender\": \"owner\", \"time\": \"16:06:43\", \"type\": \"textMessage\", \"value\": \"昏迷三天头部缝三针   交警判定责任是     电动车车主百分之六十  公交车百分之四十   其他没什么了  怎么赔偿？\"}, {\"label\": {\"knowledge\": false, \"negative\": false, \"question\": false}, \"sender\": \"audience\", \"time\": \"16:07:36\", \"type\": \"textMessage\", \"value\": \"伤情，对方是否做了伤情鉴定\"}, {\"label\": {\"attitude\": \"\", \"s3\": \"\"}, \"sender\": \"owner\", \"time\": \"16:08:03\", \"type\": \"textMessage\", \"value\": \"轻伤\"}, {\"label\": {\"knowledge\": false, \"negative\": false, \"question\": false}, \"sender\": \"audience\", \"time\": \"16:08:08\", \"type\": \"textMessage\", \"value\": \"须在治疗终结伤残鉴定后才能确定\"}, {\"label\": {\"knowledge\": false, \"negative\": false, \"question\": false}, \"sender\": \"audience\", \"time\": \"16:08:40\", \"type\": \"textMessage\", \"value\": \"这里有保险，详情，或者具体操作事宜建议电话咨询。\"}, {\"label\": {\"knowledge\": false, \"negative\": false, \"question\": false}, \"sender\": \"audience\", \"time\": \"16:08:41\", \"type\": \"textMessage\", \"value\": \"具体可以电话咨询或关注私聊\"}, {\"label\": {\"attitude\": \"\", \"s3\": \"\"}, \"sender\": \"owner\", \"time\": \"16:08:52\", \"type\": \"textMessage\", \"value\": \"鉴定结果是轻伤   怎么赔偿\"}, {\"label\": {\"knowledge\": false, \"negative\": false, \"question\": false}, \"sender\": \"audience\", \"time\": \"16:10:21\", \"type\": \"textMessage\", \"value\": \"伤残鉴定\"}, {\"label\": {\"knowledge\": false, \"negative\": false, \"question\": false}, \"sender\": \"audience\", \"time\": \"16:13:16\", \"type\": \"textMessage\", \"value\": \"你的轻伤是伤情鉴定\"}, {\"label\": {\"knowledge\": false, \"negative\": false, \"question\": false}, \"sender\": \"audience\", \"time\": \"16:13:32\", \"type\": \"textMessage\", \"value\": \"赔偿需要伤残鉴定\"}, {\"label\": {\"attitude\": \"\", \"s3\": \"\"}, \"sender\": \"owner\", \"time\": \"16:13:40\", \"type\": \"textMessage\", \"value\": \"十级\"}, {\"label\": {\"attitude\": \"\", \"s3\": \"\"}, \"sender\": \"owner\", \"time\": \"16:14:15\", \"type\": \"textMessage\", \"value\": \"基>本上没什么事！\"}, {\"label\": {\"attitude\": \"\", \"s3\": \"\"}, \"sender\": \"owner\", \"time\": \"16:14:18\", \"type\": \"textMessage\", \"value\": \"基本上没什么事！\"}], \"crawldate\": \"2020-03-02 18:10:27.338000\", \"date\": \"2015-06-26 16:00:02\", \"label\": {\"s1\": \"1.16\", \"s2\": \"2.7\", \"s3\": \"3.4\"}, \"owner\": \"匿名\", \"title\": \"你好！闯红灯的公交车和闯红灯的电动车相撞，电动车车主被撞昏迷头部出血  责任怎么处理。\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0e38d9cb-468d-4111-bb13-b6b60df8ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = json.loads(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8cf0d3af-781c-4f72-ba41-d413faa2c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = ''.join(re.split('[\\s,]', content['title'])[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bff51df1-313d-44dc-bbb7-1244cf027471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'offensive': [['冒犯', 0.5700234174728394], ['正常', 0.42997655272483826]],\n",
       " 'query_risk': [['风险', 0.9904796481132507],\n",
       "  ['个人信息', 1.3330283763934858e-05],\n",
       "  ['正常', 0.009506949223577976]]}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_api.predict('中共一大都干了什么')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9d1ce8da-d645-4bc0-bc5b-10850829a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open('/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8.topic.offensive_query_risk') as frobj:\n",
    "    black = []\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        content['text'] = re.sub('[男女]+', ' ', content['text'])\n",
    "        if content['score_list']['query_risk'][0][1]< 0.3 and content['score_list']['offensive'][0][1] < 0.3:\n",
    "            black.append(content)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "95d4e1ac-3fee-4d3e-b8e1-7f22944b5e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'offensive': [['冒犯', 0.0024976327549666166], ['正常', 0.9975023865699768]],\n",
       " 'query_risk': [['风险', 4.455989983398467e-05],\n",
       "  ['个人信息', 0.9625425934791565],\n",
       "  ['正常', 0.03741287812590599]]}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_api.predict('王一博的电话号码')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14693f4f-208c-4e2d-99c7-6cef21f12ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "import re\n",
    "with open('/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8.other.offensive_query_risk') as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        content['text'] = re.sub('[男女]+', ' ', content['text'])\n",
    "        if content['score_list']['query_risk'][0][1]< 0.3 and content['score_list']['offensive'][0][1] < 0.3:\n",
    "            if content['text'] not in data_dict:\n",
    "                data_dict[content['text']] = []\n",
    "            data_dict[content['text']].append(content)\n",
    "\n",
    "with open('/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8.other.offensive_query_risk.white', 'w') as fwobj:\n",
    "    for d in black:\n",
    "        if d['topic'][0][0] in ['法律']:\n",
    "            continue\n",
    "        if d['text'] not in data_dict:\n",
    "            continue\n",
    "        tmp = {\n",
    "            'text':d['text'],\n",
    "            'label':['正常'],\n",
    "            'source':'efaqa'\n",
    "        }\n",
    "        fwobj.write(json.dumps(tmp, ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "139ffbbb-9017-4687-bd77-13e9751c9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_mapping = {\n",
    "    '1.1':'学业烦恼、对未来规划的迷茫',\n",
    "    '1.2':'事业和工作烦恼',\n",
    "    '1.3':'家庭问题和矛盾',\n",
    "    '1.4':'物质滥用',\n",
    "    '1.5':'悲恸',\n",
    "    '1.6':'失眠',\n",
    "    '1.7':'压力',\n",
    "    '1.8':'人际关系',\n",
    "    '1.9':'情感关系问题',\n",
    "    '1.10':'离婚',\n",
    "    '1.11':'分手',\n",
    "    '1.12':'自我探索',\n",
    "    '1.13':'低自尊',\n",
    "    '1.14':'青春期问题',\n",
    "    '1.15':'强迫症',\n",
    "    '1.16':'其它',\n",
    "    '1.17':'男同性恋、女同性恋、双性恋与跨性别',\n",
    "    '1.18':'性问题',\n",
    "    '1.19':'亲子关系'\n",
    "}\n",
    "\n",
    "s2_mapping = {\n",
    "    '2.1':'忧郁症',\n",
    "    '2.2':'焦虑症',\n",
    "    '2.3':'躁郁症',\n",
    "    '2.4':'创伤后应激反应',\n",
    "    '2.5':'恐慌症',\n",
    "    '2.6':'厌食症和暴食症',\n",
    "    '2.7':'非疾病',\n",
    "    '2.8':'其它疾病'\n",
    "}\n",
    "\n",
    "s3_mapping = {\n",
    "    '3.1':'正在进行的自杀行为',\n",
    "    '3.2':'策划进行的自杀行为',\n",
    "    '3.3':'自残',\n",
    "    '3.4':'进行的人身伤害',\n",
    "    '3.5':'计划的人身伤害',\n",
    "    '3.6':'无伤害身体倾向'\n",
    "}\n",
    "\n",
    "efaqa = []\n",
    "with open(\"/data/albert.xht/pretrained_model_risk/corpus/efaqa-corpus-zh/efaqa-corpus-zh.utf8\", \"r\") as frobj:\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        title = content['title'] #''.join(re.split('[\\s,]', content['title'])[1:])\n",
    "        if len(title) >= 5:\n",
    "            if  s3_mapping[content['label']['s3']] in ['正在进行的自杀行为', '策划进行的自杀行为', '自残']:\n",
    "                tmp = {\n",
    "                    'text':title,\n",
    "                    'label':['风险'],\n",
    "                    'source':'efaqa'\n",
    "                }\n",
    "                continue\n",
    "            elif s3_mapping[content['label']['s3']] in ['无伤害身体倾向', '计划的人身伤害'] and \\\n",
    "            s1_mapping[content['label']['s1']] not in ['其他', '男同性恋、女同性恋、双性恋与跨性别']:\n",
    "                tmp = {\n",
    "                    'text':title,\n",
    "                    'label':['正常'],\n",
    "                    'source':'efaqa'\n",
    "                }\n",
    "                efaqa.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8a0fb380-0375-4fc2-854e-4585ab41256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/albert.xht/sentiment/offensive_cold.json.offensive_query_risk') as frobj:\n",
    "    black = []\n",
    "    for line in frobj:\n",
    "        content = json.loads(line.strip())\n",
    "        if content['score_list']['query_risk'][0][1] < 0.3 and content['score_list']['offensive'][0][1] < 0.3 and content['topic'][0] not in ['冒犯']:\n",
    "            black.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1d0578df-4dbf-45e9-95f9-ff3234cd8da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'offensive': [['冒犯', 0.21428874135017395], ['正常', 0.7857112288475037]],\n",
       " 'query_risk': [['风险', 0.6982224583625793],\n",
       "  ['个人信息', 6.725983439537231e-06],\n",
       "  ['正常', 0.30177077651023865]]}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_api.predict('和不同的女人做爱感觉差异真的很大吗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2694ee-e1de-44b8-be5f-2739db91e158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
